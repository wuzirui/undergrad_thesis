
@misc{lin_componerf_2023,
	title = {{CompoNeRF}: {Text}-guided {Multi}-object {Compositional} {NeRF} with {Editable} {3D} {Scene} {Layout}},
	shorttitle = {{CompoNeRF}},
	url = {http://arxiv.org/abs/2303.13843},
	doi = {10.48550/arXiv.2303.13843},
	abstract = {Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.However, a hurdle is that they often encounter guidance collapse when rendering complex scenes from multi-object texts. Because the text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with specific 3D structures. To address this issue, we propose a novel framework, dubbed CompoNeRF, that explicitly incorporates an editable 3D scene layout to provide effective guidance at the single object (i.e., local) and whole scene (i.e., global) levels. Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D box coordinates and text prompt, which can be easily collected from users. Then, we introduce a global MLP to calibrate the compositional latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. Lastly, we apply the text guidance on global and local levels through their corresponding views to avoid guidance ambiguity. This way, our CompoNeRF allows for flexible scene editing and re-composition of trained local NeRFs into a new scene by manipulating the 3D layout or text prompt. Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate faithful and editable text-to-3D results while opening a potential direction for text-guided multi-object composition via the editable 3D scene layout.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Lin, Yiqi and Bai, Haotian and Li, Sijia and Lu, Haonan and Lin, Xiaodong and Xiong, Hui and Wang, Lin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13843 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_being_nodate,
	title = {Being {Friends} with {Floaters}: {Learning} {Radiance} {Fields} from {Hazy} {Images}},
	keywords = {/unread},
}

@misc{bian_nope-nerf_2023,
	title = {{NoPe}-{NeRF}: {Optimising} {Neural} {Radiance} {Field} with {No} {Pose} {Prior}},
	shorttitle = {{NoPe}-{NeRF}},
	url = {http://arxiv.org/abs/2212.07388},
	doi = {10.48550/arXiv.2212.07388},
	abstract = {Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Bian, Wenjing and Wang, Zirui and Li, Kejie and Bian, Jia-Wang and Prisacariu, Victor Adrian},
	month = apr,
	year = {2023},
	note = {arXiv:2212.07388 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_nerf--_2022,
	title = {{NeRF}--: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	shorttitle = {{NeRF}--},
	url = {http://arxiv.org/abs/2102.07064},
	doi = {10.48550/arXiv.2102.07064},
	abstract = {Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (NeRF) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose NeRF\$--\$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with NeRF training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	month = apr,
	year = {2022},
	note = {arXiv:2102.07064 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_geosim_2021,
	title = {{GeoSim}: {Realistic} {Video} {Simulation} via {Geometry}-{Aware} {Composition} for {Self}-{Driving}},
	shorttitle = {{GeoSim}},
	url = {http://arxiv.org/abs/2101.06543},
	doi = {10.48550/arXiv.2101.06543},
	abstract = {Scalable sensor simulation is an important yet challenging open problem for safety-critical domains such as self-driving. Current works in image simulation either fail to be photorealistic or do not model the 3D environment and the dynamic objects within, losing high-level control and physical realism. In this paper, we present GeoSim, a geometry-aware image composition process which synthesizes novel urban driving scenarios by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses. Towards this goal, we first build a diverse bank of 3D objects with both realistic geometry and appearance from sensor data. During simulation, we perform a novel geometry-aware simulation-by-composition procedure which 1) proposes plausible and realistic object placements into a given scene, 2) render novel views of dynamic objects from the asset bank, and 3) composes and blends the rendered image segments. The resulting synthetic images are realistic, traffic-aware, and geometrically consistent, allowing our approach to scale to complex use cases. We demonstrate two such important applications: long-range realistic video simulation across multiple camera sensors, and synthetic data generation for data augmentation on downstream segmentation tasks. Please check https://tmux.top/publication/geosim/ for high-resolution video results.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Chen, Yun and Rong, Frieda and Duggal, Shivam and Wang, Shenlong and Yan, Xinchen and Manivasagam, Sivabalan and Xue, Shangjie and Yumer, Ersin and Urtasun, Raquel},
	month = may,
	year = {2021},
	note = {arXiv:2101.06543 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@inproceedings{williams_pyramidal_1983,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '83},
	title = {Pyramidal parametrics},
	isbn = {978-0-89791-109-2},
	url = {https://dl.acm.org/doi/10.1145/800059.801126},
	doi = {10.1145/800059.801126},
	abstract = {The mapping of images onto surfaces may substantially increase the realism and information content of computer-generated imagery. The projection of a flat source image onto a curved surface may involve sampling difficulties, however, which are compounded as the view of the surface changes. As the projected scale of the surface increases, interpolation between the original samples of the source image is necessary; as the scale is reduced, approximation of multiple samples in the source is required. Thus a constantly changing sampling window of view-dependent shape must traverse the source image. To reduce the computation implied by these requirements, a set of prefiltered source images may be created. This approach can be applied to particular advantage in animation, where a large number of frames using the same source image must be generated. This paper advances a “pyramidal parametric” prefiltering and sampling geometry which minimizes aliasing effects and assures continuity within and between target images. Although the mapping of texture onto surfaces is an excellent example of the process and provided the original motivation for its development, pyramidal parametric data structures admit of wider application. The aliasing of not only surface texture, but also highlights and even the surface representations themselves, may be minimized by pyramidal parametric means.},
	urldate = {2023-05-03},
	booktitle = {Proceedings of the 10th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Williams, Lance},
	year = {1983},
	keywords = {/unread, Antialiasing, Illumination models, Modeling, Pyramidal data structures, Reflectance mapping, Texture mapping, Visible surface algorithms},
	pages = {1--11},
}

@article{bernhard_3d_2023,
	title = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
	journal = {ACM Transactions on Graphics},
	author = {Bernhard, Kerbl and Georgios, Kopanas and Thomas, Leimkühler and George, Drettakis},
	month = may,
	year = {2023},
	note = {titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:},
	keywords = {/unread},
}

@misc{prokudin_dynamic_2023,
	title = {Dynamic {Point} {Fields}},
	url = {http://arxiv.org/abs/2304.02626},
	abstract = {Recent years have witnessed significant progress in the field of neural surface reconstruction. While the extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives such as point clouds can significantly reduce computational complexity, without sacrificing the reconstructed surface quality. However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dynamic point field model that combines the representational benefits of explicit point-based graphics with implicit deformation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as-isometric-as-possible regularisation. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to additionally leverage semantic information such as keypoint dynamics to guide the deformation learning. We demonstrate our model with an example application of creating an expressive animatable human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expressivity of such models when dealing with complex cloth appearances such as long skirts. We show the advantages of our dynamic point field framework in terms of its representational power, learning efficiency, and robustness to out-of-distribution novel poses.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02626 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zheng_pointavatar_2023,
	title = {{PointAvatar}: {Deformable} {Point}-based {Head} {Avatars} from {Videos}},
	shorttitle = {{PointAvatar}},
	url = {http://arxiv.org/abs/2212.08377},
	doi = {10.48550/arXiv.2212.08377},
	abstract = {The ability to create realistic, animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Zheng, Yufeng and Yifan, Wang and Wetzstein, Gordon and Black, Michael J. and Hilliges, Otmar},
	month = feb,
	year = {2023},
	note = {arXiv:2212.08377 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{hongwen_closet_nodate,
	title = {{CloSET}: {Modeling} {Clothed} {Humans} on {Continuous} {Surface} with {Explicit} {Template} {Decomposition}},
	abstract = {Creating animatable avatars from static scans requires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limitations in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses. Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a continuous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the research in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in unseen poses.},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hongwen, Zhang and Siyou, Lin and Ruizhi, Shao and Yuxiang, Zhang and Zerong, Zheng and Han, Huang and Yandong, Guo and Yebin, Liu},
	note = {titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:},
	keywords = {/unread},
}

@article{xie_neural_2022,
	title = {Neural {Fields} in {Visual} {Computing} and {Beyond}},
	volume = {41},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14505},
	doi = {10.1111/cgf.14505},
	language = {en},
	number = {2},
	urldate = {2023-05-02},
	journal = {Computer Graphics Forum},
	author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
	month = may,
	year = {2022},
	keywords = {/unread},
	pages = {641--676},
}

@misc{gao_object-centric_2023,
	title = {Object-{Centric} {Voxelization} of {Dynamic} {Scenes} via {Inverse} {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2305.00393},
	doi = {10.48550/arXiv.2305.00393},
	abstract = {Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its effectiveness on various benchmarks with multiple objects, diverse dynamics, and real-world shapes and textures. We present visualization at https://sites.google.com/view/dynavol-visual.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Gao, Siyu and Zhao, Yanpeng and Wang, Yunbo and Yang, Xiaokang},
	month = apr,
	year = {2023},
	note = {arXiv:2305.00393 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wallingford_neural_2023,
	title = {Neural {Radiance} {Field} {Codebooks}},
	url = {http://arxiv.org/abs/2301.04101},
	doi = {10.48550/arXiv.2301.04101},
	abstract = {Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1\% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29\% relative improvement). Finally, we show that NRC improves on the task of depth ordering by 5.5\% accuracy in THOR.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Wallingford, Matthew and Kusupati, Aditya and Fang, Alex and Ramanujan, Vivek and Kembhavi, Aniruddha and Mottaghi, Roozbeh and Farhadi, Ali},
	month = apr,
	year = {2023},
	note = {arXiv:2301.04101 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{debbagh_neural_2023,
	title = {Neural {Radiance} {Fields} ({NeRFs}): {A} {Review} and {Some} {Recent} {Developments}},
	shorttitle = {Neural {Radiance} {Fields} ({NeRFs})},
	url = {http://arxiv.org/abs/2305.00375},
	abstract = {Neural Radiance Field (NeRF) is a framework that represents a 3D scene in the weights of a fully connected neural network, known as the Multi-Layer Perception(MLP). The method was introduced for the task of novel view synthesis and is able to achieve state-of-the-art photorealistic image renderings from a given continuous viewpoint. NeRFs have become a popular field of research as recent developments have been made that expand the performance and capabilities of the base framework. Recent developments include methods that require less images to train the model for view synthesis as well as methods that are able to generate views from unconstrained and dynamic scene representations.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Debbagh, Mohamed},
	month = apr,
	year = {2023},
	note = {arXiv:2305.00375 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{deng_nerf-loam_2023,
	title = {{NeRF}-{LOAM}: {Neural} {Implicit} {Representation} for {Large}-{Scale} {Incremental} {LiDAR} {Odometry} and {Mapping}},
	shorttitle = {{NeRF}-{LOAM}},
	url = {http://arxiv.org/abs/2303.10709},
	doi = {10.48550/arXiv.2303.10709},
	abstract = {Simultaneously odometry and mapping using LiDAR data is an important task for mobile systems to achieve full autonomy in large-scale environments. However, most existing LiDAR-based methods prioritize tracking quality over reconstruction quality. Although the recently developed neural radiance fields (NeRF) have shown promising advances in implicit reconstruction for indoor environments, the problem of simultaneous odometry and mapping for large-scale scenarios using incremental LiDAR data remains unexplored. To bridge this gap, in this paper, we propose a novel NeRF-based LiDAR odometry and mapping approach, NeRF-LOAM, consisting of three modules neural odometry, neural mapping, and mesh reconstruction. All these modules utilize our proposed neural signed distance function, which separates LiDAR points into ground and non-ground points to reduce Z-axis drift, optimizes odometry and voxel embeddings concurrently, and in the end generates dense smooth mesh maps of the environment. Moreover, this joint optimization allows our NeRF-LOAM to be pre-trained free and exhibit strong generalization abilities when applied to different environments. Extensive evaluations on three publicly available datasets demonstrate that our approach achieves state-of-the-art odometry and mapping performance, as well as a strong generalization in large-scale environments utilizing LiDAR data. Furthermore, we perform multiple ablation studies to validate the effectiveness of our network design. The implementation of our approach will be made available at https://github.com/JunyuanDeng/NeRF-LOAM.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Deng, Junyuan and Chen, Xieyuanli and Xia, Songpengcheng and Sun, Zhen and Liu, Guoqing and Yu, Wenxian and Pei, Ling},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10709 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{tavanaei_deep_2019,
	title = {Deep {Learning} in {Spiking} {Neural} {Networks}},
	volume = {111},
	issn = {08936080},
	url = {http://arxiv.org/abs/1804.08150},
	doi = {10.1016/j.neunet.2018.12.002},
	abstract = {In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.},
	urldate = {2023-05-02},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothee and Maida, Anthony S.},
	month = mar,
	year = {2019},
	note = {arXiv:1804.08150 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	pages = {47--63},
}

@inproceedings{zhan_general_2023,
	title = {General {Neural} {Gauge} {Fields}},
	url = {https://openreview.net/forum?id=XWkWK2UagFR},
	abstract = {The recent advance of neural fields, such as neural radiance fields, has significantly pushed the boundary of scene representation learning. Aiming to boost the computation efﬁciency and rendering quality of 3D scenes, a popular line of research maps the 3D coordinate system to another measuring system, e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion of coordinate systems can be typically dubbed as {\textbackslash}emph\{gauge transformation\}, which is usually a pre-defined mapping function, e.g., orthogonal projection or spatial hash function. This begs a question: can we directly learn a desired gauge transformation along with the neural field in an end-to-end manner? In this work, we extend this problem to a general paradigm with a taxonomy of discrete and continuous cases, and develop an end-to-end learning framework to jointly optimize the gauge transformation and neural fields. To counter the problem that the learning of gauge transformations can collapse easily, we derive a general regularization mechanism from the principle of information conservation during the gauge transformation. To circumvent the high computation cost in gauge learning with regularization, we directly derive an information-invariant gauge transformation which allows to preserve scene information inherently and yield superior performance.},
	language = {en},
	urldate = {2023-05-02},
	author = {Zhan, Fangneng and Liu, Lingjie and Kortylewski, Adam and Theobalt, Christian},
	month = feb,
	year = {2023},
	keywords = {/unread},
}

@article{dadon_ddnerf_2022,
	title = {{DDNeRF}: {Depth} {Distribution} {Neural} {Radiance} {Fields}},
	shorttitle = {{DDNeRF}},
	url = {https://arxiv.org/abs/2203.16626v1},
	doi = {10.48550/arXiv.2203.16626},
	abstract = {In recent years, the field of implicit neural representation has progressed significantly. Models such as neural radiance fields (NeRF), which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally very expensive. We present depth distribution neural radiance field (DDNeRF), a new method that significantly increases sampling efficiency along rays during training while achieving superior results for a given sampling budget. DDNeRF achieves this by learning a more accurate representation of the density distribution along rays. More specifically, we train a coarse model to predict the internal distribution of the transparency of an input volume in addition to the volume's total density. This finer distribution then guides the sampling procedure of the fine model. This method allows us to use fewer samples during training while reducing computational resources.},
	language = {en},
	urldate = {2023-01-19},
	author = {Dadon, David and Fried, Ohad and Hel-Or, Yacov},
	month = mar,
	year = {2022},
	keywords = {/unread},
}

@misc{chang_pointersect_2023,
	title = {Pointersect: {Neural} {Rendering} with {Cloud}-{Ray} {Intersection}},
	shorttitle = {Pointersect},
	url = {http://arxiv.org/abs/2304.12390},
	doi = {10.48550/arXiv.2304.12390},
	abstract = {We propose a novel method that renders point clouds as if they are surfaces. The proposed method is differentiable and requires no scene-specific optimization. This unique capability enables, out-of-the-box, surface normal estimation, rendering room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike existing work that focuses on converting point clouds to other representations--e.g., surfaces or implicit functions--our key idea is to directly infer the intersection of a light ray with the underlying surface represented by the given point cloud. Specifically, we train a set transformer that, given a small number of local neighbor points along a light ray, provides the intersection point, the surface normal, and the material blending weights, which are used to render the outcome of this light ray. Localizing the problem into small neighborhoods enables us to train a model with only 48 meshes and apply it to unseen point clouds. Our model achieves higher estimation accuracy than state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets. When applied to room-scale point clouds, without any scene-specific optimization, the model achieves competitive quality with the state-of-the-art novel-view rendering methods. Moreover, we demonstrate ability to render and manipulate Lidar-scanned point clouds such as lighting control and object insertion.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Chang, Jen-Hao Rick and Chen, Wei-Yu and Ranjan, Anurag and Yi, Kwang Moo and Tuzel, Oncel},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12390 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{yang_learning_2023,
	title = {Learning a {Diffusion} {Prior} for {NeRFs}},
	url = {http://arxiv.org/abs/2304.14473},
	doi = {10.48550/arXiv.2304.14473},
	abstract = {Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Yang, Guandao and Kundu, Abhijit and Guibas, Leonidas J. and Barron, Jonathan T. and Poole, Ben},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14473 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{hertz_sape_2021,
	title = {{SAPE}: {Spatially}-{Adaptive} {Progressive} {Encoding} for {Neural} {Optimization}},
	shorttitle = {{SAPE}},
	url = {https://openreview.net/forum?id=0wqGMmfqBw},
	abstract = {Multilayer-perceptrons (MLP) are known to struggle learning functions of high-frequencies, and in particular, instances of wide frequency bands. We present a progressive mapping scheme for input signals of MLP networks, enabling them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. We introduce Spatially Adaptive Progressive Encoding (SAPE) layers, which gradually unmask signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of our method on variety of domains and applications: regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.},
	language = {en},
	urldate = {2023-05-01},
	author = {Hertz, Amir and Perel, Or and Giryes, Raja and Sorkine-hornung, Olga and Cohen-or, Daniel},
	month = oct,
	year = {2021},
	keywords = {/unread},
}

@misc{deng_nasa_2022,
	title = {{NASA}: {Neural} {Articulated} {Shape} {Approximation}},
	shorttitle = {{NASA}},
	url = {http://arxiv.org/abs/1912.03207},
	doi = {10.48550/arXiv.1912.03207},
	abstract = {Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Deng, Boyang and Lewis, J. P. and Jeruzalski, Timothy and Pons-Moll, Gerard and Hinton, Geoffrey and Norouzi, Mohammad and Tagliasacchi, Andrea},
	month = jul,
	year = {2022},
	note = {arXiv:1912.03207 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{weng_humannerf_2022,
	title = {{HumanNeRF}: {Free}-viewpoint {Rendering} of {Moving} {People} from {Monocular} {Video}},
	shorttitle = {{HumanNeRF}},
	url = {http://arxiv.org/abs/2201.04127},
	doi = {10.48550/arXiv.2201.04127},
	abstract = {We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is particularly challenging, as it requires synthesizing photorealistic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesizing fine details such as cloth folds and facial appearance. Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, produced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Weng, Chung-Yi and Curless, Brian and Srinivasan, Pratul P. and Barron, Jonathan T. and Kemelmacher-Shlizerman, Ira},
	month = jun,
	year = {2022},
	note = {arXiv:2201.04127 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zhao_humannerf_2022,
	title = {{HumanNeRF}: {Efficiently} {Generated} {Human} {Radiance} {Field} from {Sparse} {Inputs}},
	shorttitle = {{HumanNeRF}},
	url = {http://arxiv.org/abs/2112.02789},
	doi = {10.48550/arXiv.2112.02789},
	abstract = {Recent neural human representations can produce high-quality multi-view rendering but require using dense multi-view inputs and costly training. They are hence largely limited to static models as training each frame is infeasible. We present HumanNeRF - a generalizable neural representation - for high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated pixel-alignment feature across multi-view inputs along with a pose embedded non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To further improve the rendering quality, we augment our solution with an appearance blending module for combining the benefits of both neural volumetric rendering and neural texture blending. Extensive experiments on various multi-view dynamic human datasets demonstrate the generalizability and effectiveness of our approach in synthesizing photo-realistic free-view humans under challenging motions and with very sparse camera view inputs.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Zhao, Fuqiang and Yang, Wei and Zhang, Jiakai and Lin, Pei and Zhang, Yingliang and Yu, Jingyi and Xu, Lan},
	month = mar,
	year = {2022},
	note = {arXiv:2112.02789 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{niemeyer_differentiable_2020,
	title = {Differentiable {Volumetric} {Rendering}: {Learning} {Implicit} {3D} {Representations} without {3D} {Supervision}},
	shorttitle = {Differentiable {Volumetric} {Rendering}},
	url = {http://arxiv.org/abs/1912.07372},
	doi = {10.48550/arXiv.1912.07372},
	abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
	month = mar,
	year = {2020},
	note = {arXiv:1912.07372 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{sun_neural_2022,
	title = {Neural {3D} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2205.12955},
	doi = {10.1145/3528233.3530718},
	abstract = {We are witnessing an explosion of neural implicit representations in computer vision and graphics. Their applicability has recently expanded beyond tasks such as shape generation and image-based rendering to the fundamental problem of image-based 3D reconstruction. However, existing methods typically assume constrained 3D environments with constant illumination captured by a small set of roughly uniformly distributed cameras. We introduce a new method that enables efficient and accurate surface reconstruction from Internet photo collections in the presence of varying illumination. To achieve this, we propose a hybrid voxel- and surface-guided sampling technique that allows for more efficient ray sampling around surfaces and leads to significant improvements in reconstruction quality. Further, we present a new benchmark and protocol for evaluating reconstruction performance on such in-the-wild scenes. We perform extensive experiments, demonstrating that our approach surpasses both classical and neural reconstruction methods on a wide variety of metrics.},
	urldate = {2023-05-01},
	booktitle = {Special {Interest} {Group} on {Computer} {Graphics} and {Interactive} {Techniques} {Conference} {Proceedings}},
	author = {Sun, Jiaming and Chen, Xi and Wang, Qianqian and Li, Zhengqi and Averbuch-Elor, Hadar and Zhou, Xiaowei and Snavely, Noah},
	month = aug,
	year = {2022},
	note = {arXiv:2205.12955 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	pages = {1--9},
}

@misc{yu_sdfstudio_2022,
	title = {{SDFStudio}: {A} {Unified} {Framework} for {Surface} {Reconstruction}},
	url = {https://github.com/autonomousvision/sdfstudio},
	author = {Yu, Zehao and Chen, Anpei and Antic, Bozidar and Peng, Songyou and Bhattacharyya, Apratim and Niemeyer, Michael and Tang, Siyu and Sattler, Torsten and Geiger, Andreas},
	year = {2022},
	keywords = {/unread},
}

@misc{ren_scaling_2023,
	title = {Scaling {Forward} {Gradient} {With} {Local} {Losses}},
	url = {http://arxiv.org/abs/2210.03310},
	doi = {10.48550/arXiv.2210.03310},
	abstract = {Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.},
	urldate = {2023-04-30},
	publisher = {arXiv},
	author = {Ren, Mengye and Kornblith, Simon and Liao, Renjie and Hinton, Geoffrey},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03310 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{verbin_ref-nerf_2021,
	title = {Ref-{NeRF}: {Structured} {View}-{Dependent} {Appearance} for {Neural} {Radiance} {Fields}},
	shorttitle = {Ref-{NeRF}},
	url = {http://arxiv.org/abs/2112.03907},
	doi = {10.48550/arXiv.2112.03907},
	abstract = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Verbin, Dor and Hedman, Peter and Mildenhall, Ben and Zickler, Todd and Barron, Jonathan T. and Srinivasan, Pratul P.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.03907 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{dai_hybrid_2023,
	title = {Hybrid {Neural} {Rendering} for {Large}-{Scale} {Scenes} with {Motion} {Blur}},
	url = {http://arxiv.org/abs/2304.12652},
	doi = {10.48550/arXiv.2304.12652},
	abstract = {Rendering novel view images is highly desirable for many applications. Despite recent progress, it remains challenging to render high-fidelity and view-consistent novel views of large-scale scenes from in-the-wild images with inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid neural rendering model that makes image-based representation and neural 3D representation join forces to render high-quality, view-consistent images. Besides, images captured in the wild inevitably contain artifacts, such as motion blur, which deteriorates the quality of rendered images. Accordingly, we propose strategies to simulate blur effects on the rendered images to mitigate the negative influence of blurriness images and reduce their importance during training based on precomputed quality-aware weights. Extensive experiments on real and synthetic data demonstrate our model surpasses state-of-the-art point-based methods for novel view synthesis. The code is available at https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Dai, Peng and Zhang, Yinda and Yu, Xin and Lyu, Xiaoyang and Qi, Xiaojuan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12652 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_freenerf_2023,
	title = {{FreeNeRF}: {Improving} {Few}-shot {Neural} {Rendering} with {Free} {Frequency} {Regularization}},
	shorttitle = {{FreeNeRF}},
	url = {http://arxiv.org/abs/2303.07418},
	doi = {10.48550/arXiv.2303.07418},
	abstract = {Novel view synthesis with sparse inputs is a challenging problem for neural radiance fields (NeRF). Recent efforts alleviate this challenge by introducing external supervision, such as pre-trained models and extra depth signals, and by non-trivial patch-based rendering. In this paper, we present Frequency regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms previous methods with minimal modifications to the plain NeRF. We analyze the key challenges in few-shot neural rendering and find that frequency plays an important role in NeRF's training. Based on the analysis, we propose two regularization terms. One is to regularize the frequency range of NeRF's inputs, while the other is to penalize the near-camera density fields. Both techniques are ``free lunches'' at no additional computational cost. We demonstrate that even with one line of code change, the original NeRF can achieve similar performance as other complicated methods in the few-shot setting. FreeNeRF achieves state-of-the-art performance across diverse datasets, including Blender, DTU, and LLFF. We hope this simple baseline will motivate a rethinking of the fundamental role of frequency in NeRF's training under the low-data regime and beyond.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Yang, Jiawei and Pavone, Marco and Wang, Yue},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07418 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shen_deep_2021,
	title = {Deep {Marching} {Tetrahedra}: a {Hybrid} {Representation} for {High}-{Resolution} {3D} {Shape} {Synthesis}},
	shorttitle = {Deep {Marching} {Tetrahedra}},
	url = {http://arxiv.org/abs/2111.04276},
	doi = {10.48550/arXiv.2111.04276},
	abstract = {We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Shen, Tianchang and Gao, Jun and Yin, Kangxue and Liu, Ming-Yu and Fidler, Sanja},
	month = nov,
	year = {2021},
	note = {arXiv:2111.04276 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_beyond_2023,
	title = {Beyond {NeRF} {Underwater}: {Learning} {Neural} {Reflectance} {Fields} for {True} {Color} {Correction} of {Marine} {Imagery}},
	shorttitle = {Beyond {NeRF} {Underwater}},
	url = {http://arxiv.org/abs/2304.03384},
	doi = {10.48550/arXiv.2304.03384},
	abstract = {Underwater imagery often exhibits distorted coloration as a result of light-water interactions, which complicates the study of benthic environments in marine biology and geography. In this research, we propose an algorithm to restore the true color (albedo) in underwater imagery by jointly learning the effects of the medium and neural scene representations. Our approach models water effects as a combination of light attenuation with distance and backscattered light. The proposed neural scene representation is based on a neural reflectance field model, which learns albedos, normals, and volume densities of the underwater environment. We introduce a logistic regression model to separate water from the scene and apply distinct light physics during training. Our method avoids the need to estimate complex backscatter effects in water by employing several approximations, enhancing sampling efficiency and numerical stability during training. The proposed technique integrates underwater light effects into a volume rendering framework with end-to-end differentiability. Experimental results on both synthetic and real-world data demonstrate that our method effectively restores true color from underwater imagery, outperforming existing approaches in terms of color consistency.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Johnson-Roberson, Matthew},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03384 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_dehazing-nerf_2023,
	title = {Dehazing-{NeRF}: {Neural} {Radiance} {Fields} from {Hazy} {Images}},
	shorttitle = {Dehazing-{NeRF}},
	url = {http://arxiv.org/abs/2304.11448},
	doi = {10.48550/arXiv.2304.11448},
	abstract = {Neural Radiance Field (NeRF) has received much attention in recent years due to the impressively high quality in 3D scene reconstruction and novel view synthesis. However, image degradation caused by the scattering of atmospheric light and object light by particles in the atmosphere can significantly decrease the reconstruction quality when shooting scenes in hazy conditions. To address this issue, we propose Dehazing-NeRF, a method that can recover clear NeRF from hazy image inputs. Our method simulates the physical imaging process of hazy images using an atmospheric scattering model, and jointly learns the atmospheric scattering model and a clean NeRF model for both image dehazing and novel view synthesis. Different from previous approaches, Dehazing-NeRF is an unsupervised method with only hazy images as the input, and also does not rely on hand-designed dehazing priors. By jointly combining the depth estimated from the NeRF 3D scene with the atmospheric scattering model, our proposed model breaks through the ill-posed problem of single-image dehazing while maintaining geometric consistency. Besides, to alleviate the degradation of image quality caused by information loss, soft margin consistency regularization, as well as atmospheric consistency and contrast discriminative loss, are addressed during the model training process. Extensive experiments demonstrate that our method outperforms the simple combination of single-image dehazing and NeRF on both image dehazing and novel view image synthesis.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Li, Tian and Li, L. U. and Wang, Wei and Feng, Zhangchi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11448 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_nerf-rpn_2023,
	title = {{NeRF}-{RPN}: {A} general framework for object detection in {NeRFs}},
	shorttitle = {{NeRF}-{RPN}},
	url = {http://arxiv.org/abs/2211.11646},
	doi = {10.48550/arXiv.2211.11646},
	abstract = {This paper presents the first significant object detection framework, NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model, NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting a novel voxel representation that incorporates multi-scale 3D neural volumetric features, we demonstrate it is possible to regress the 3D bounding boxes of objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN is a general framework and can be applied to detect objects without class labels. We experimented NeRF-RPN with various backbone architectures, RPN head designs and loss functions. All of them can be trained in an end-to-end manner to estimate high quality 3D bounding boxes. To facilitate future research in object detection for NeRF, we built a new benchmark dataset which consists of both synthetic and real-world data with careful labeling and clean up. Code and dataset are available at https://github.com/lyclyc52/NeRF\_RPN.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Hu, Benran and Huang, Junkai and Liu, Yichen and Tai, Yu-Wing and Tang, Chi-Keung},
	month = mar,
	year = {2023},
	note = {arXiv:2211.11646 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{shen_conditional-flow_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Conditional-{Flow} {NeRF}: {Accurate} {3D} {Modelling} with {Reliable} {Uncertainty} {Quantification}},
	isbn = {978-3-031-20062-5},
	shorttitle = {Conditional-{Flow} {NeRF}},
	doi = {10.1007/978-3-031-20062-5_31},
	abstract = {A critical limitation of current methods based on Neural Radiance Fields (NeRF) is that they are unable to quantify the uncertainty associated with the learned appearance and geometry of the scene. This information is paramount in real applications such as medical diagnosis or autonomous driving where, to reduce potentially catastrophic failures, the confidence on the model outputs must be included into the decision-making process. In this context, we introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to incorporate uncertainty quantification into NeRF-based approaches. For this purpose, our method learns a distribution over all possible radiance fields modelling the scene which is used to quantify the uncertainty associated with the modelled scene. In contrast to previous approaches enforcing strong constraints over the radiance field distribution, CF-NeRF learns it in a flexible and fully data-driven manner by coupling Latent Variable Modelling and Conditional Normalizing Flows. This strategy allows to obtain reliable uncertainty estimation while preserving model expressivity. Compared to previous state-of-the-art methods proposed for uncertainty quantification in NeRF, our experiments show that the proposed method achieves significantly lower prediction errors and more reliable uncertainty values for synthetic novel view and depth-map estimation.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Shen, Jianxiong and Agudo, Antonio and Moreno-Noguer, Francesc and Ruiz, Adria},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, 3D, Scene representation, Uncertainty quantification},
	pages = {540--557},
}

@inproceedings{pan_activenerf_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ActiveNeRF}: {Learning} {Where} to {See} with {Uncertainty} {Estimation}},
	isbn = {978-3-031-19827-4},
	shorttitle = {{ActiveNeRF}},
	doi = {10.1007/978-3-031-19827-4_14},
	abstract = {Recently, Neural Radiance Fields (NeRF) has shown promising performances on reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D images. Albeit effective, the performance of NeRF is highly influenced by the quality of training samples. With limited posed images from the scene, NeRF fails to generalize well to novel views and may collapse to trivial solutions in unobserved regions. This makes NeRF impractical under resource-constrained scenarios. In this paper, we present a novel learning framework, ActiveNeRF, aiming to model a 3D scene with a constrained input budget. Specifically, we first incorporate uncertainty estimation into a NeRF model, which ensures robustness under few observations and provides an interpretation of how NeRF understands the scene. On this basis, we propose to supplement the existing training set with newly captured samples based on an active learning scheme. By evaluating the reduction of uncertainty given new inputs, we select the samples that bring the most information gain. In this way, the quality of novel view synthesis can be improved with minimal additional resources. Extensive experiments validate the performance of our model on both realistic and synthetic scenes, especially with scarcer training data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Xuran and Lai, Zihang and Song, Shiji and Huang, Gao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, Active learning, Neural radiance fields, Uncertainty estimation},
	pages = {230--246},
}

@misc{geng_gapartnet_2023,
	title = {{GAPartNet}: {Cross}-{Category} {Domain}-{Generalizable} {Object} {Perception} and {Manipulation} via {Generalizable} and {Actionable} {Parts}},
	shorttitle = {{GAPartNet}},
	url = {http://arxiv.org/abs/2211.05272},
	doi = {10.48550/arXiv.2211.05272},
	abstract = {For years, researchers have been devoted to generalizable object perception and manipulation, where cross-category generalizability is highly desired yet underexplored. In this work, we propose to learn such cross-category skills via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, we construct a large-scale part-centric interactive dataset, GAPartNet, where we provide rich, part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, we propose a robust 3D segmentation method from the perspective of domain generalization by integrating adversarial learning techniques. Our method outperforms all existing methods by a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both the simulator and the real world. Our dataset, code, and demos are available on our project page.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
	month = mar,
	year = {2023},
	note = {arXiv:2211.05272 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jain_enhanced_2023,
	title = {Enhanced {Stable} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2303.17094},
	abstract = {We introduce an approach to enhance the novel view synthesis from images taken from a freely moving camera. The introduced approach focuses on outdoor scenes where recovering accurate geometric scaffold and camera pose is challenging, leading to inferior results using the state-of-the-art stable view synthesis (SVS) method. SVS and related methods fail for outdoor scenes primarily due to (i) over-relying on the multiview stereo (MVS) for geometric scaffold recovery and (ii) assuming COLMAP computed camera poses as the best possible estimates, despite it being well-studied that MVS 3D reconstruction accuracy is limited to scene disparity and camera-pose accuracy is sensitive to key-point correspondence selection. This work proposes a principled way to enhance novel view synthesis solutions drawing inspiration from the basics of multiple view geometry. By leveraging the complementary behavior of MVS and monocular depth, we arrive at a better scene depth per view for nearby and far points, respectively. Moreover, our approach jointly refines camera poses with image-based rendering via multiple rotation averaging graph optimization. The recovered scene depth and the camera-pose help better view-dependent on-surface feature aggregation of the entire scene. Extensive evaluation of our approach on the popular benchmark dataset, such as Tanks and Temples, shows substantial improvement in view synthesis results compared to the prior art. For instance, our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar statistics are observed when tested on other benchmark datasets such as FVS, Mip-NeRF 360, and DTU.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Jain, Nishant and Kumar, Suryansh and Van Gool, Luc},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17094 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zhu_vdn-nerf_2023,
	title = {{VDN}-{NeRF}: {Resolving} {Shape}-{Radiance} {Ambiguity} via {View}-{Dependence} {Normalization}},
	shorttitle = {{VDN}-{NeRF}},
	url = {http://arxiv.org/abs/2303.17968},
	abstract = {We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for better geometry under non-Lambertian surface and dynamic lighting conditions that cause significant variation in the radiance of a point when viewed from different angles. Instead of explicitly modeling the underlying factors that result in the view-dependent phenomenon, which could be complex yet not inclusive, we develop a simple and effective technique that normalizes the view-dependence by distilling invariant information already encoded in the learned NeRFs. We then jointly train NeRFs for view synthesis with view-dependence normalization to attain quality geometry. Our experiments show that even though shape-radiance ambiguity is inevitable, the proposed normalization can minimize its effect on geometry, which essentially aligns the optimal capacity needed for explaining view-dependent variations. Our method applies to various baselines and significantly improves geometry without changing the volume rendering pipeline, even if the data is captured under a moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Zhu, Bingfan and Yang, Yanchao and Wang, Xulong and Zheng, Youyi and Guibas, Leonidas},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17968 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_qff_2022,
	title = {{QFF}: {Quantized} {Fourier} {Features} for {Neural} {Field} {Representations}},
	shorttitle = {{QFF}},
	url = {https://arxiv.org/abs/2212.00914v1},
	abstract = {Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent approaches encode features in spatial bins to improve speed of learning details, but at the cost of larger model size and loss of continuity. Instead, we propose to encode features in bins of Fourier features that are commonly used for positional encoding. We call these Quantized Fourier Features (QFF). As a naturally multiresolution and periodic representation, our experiments show that using QFF can result in smaller model size, faster training, and better quality outputs for several applications, including Neural Image Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a simple drop-in addition to many neural field representations.},
	language = {en},
	urldate = {2023-04-22},
	author = {Lee, Jae Yong and Wu, Yuqun and Zou, Chuhang and Wang, Shenlong and Hoiem, Derek},
	month = dec,
	year = {2022},
	keywords = {/unread},
}

@misc{hu_multiscale_2023,
	title = {Multiscale {Representation} for {Real}-{Time} {Anti}-{Aliasing} {Neural} {Rendering}},
	url = {https://arxiv.org/abs/2304.10075v1},
	abstract = {The rendering scheme in neural radiance field (NeRF) is effective in rendering a pixel by casting a ray into the scene. However, NeRF yields blurred rendering results when the training images are captured at non-uniform scales, and produces aliasing artifacts if the test images are taken in distant views. To address this issue, Mip-NeRF proposes a multiscale representation as a conical frustum to encode scale information. Nevertheless, this approach is only suitable for offline rendering since it relies on integrated positional encoding (IPE) to query a multilayer perceptron (MLP). To overcome this limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale representation with a deferred architecture for real-time anti-aliasing rendering. Our approach includes a density Mip-VoG for scene geometry and a feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes scene scale using the level of detail (LOD) derived from ray differentials and uses quadrilinear interpolation to map a queried 3D location to its features and density from two neighboring downsampled voxel grids. To our knowledge, our approach is the first to offer multiscale training and real-time anti-aliasing rendering simultaneously. We conducted experiments on multiscale datasets, and the results show that our approach outperforms state-of-the-art real-time rendering baselines.},
	language = {en},
	urldate = {2023-04-22},
	author = {Hu, Dongting and Zhang, Zhenkai and Hou, Tingbo and Liu, Tongliang and Fu, Huan and Gong, Mingming},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{kulhanek_tetra-nerf_2023,
	title = {Tetra-{NeRF}: {Representing} {Neural} {Radiance} {Fields} {Using} {Tetrahedra}},
	shorttitle = {Tetra-{NeRF}},
	url = {https://arxiv.org/abs/2304.09987v1},
	abstract = {Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.},
	language = {en},
	urldate = {2023-04-22},
	author = {Kulhanek, Jonas and Sattler, Torsten},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{mittal_neural_2023,
	title = {Neural {Radiance} {Fields}: {Past}, {Present}, and {Future}},
	shorttitle = {Neural {Radiance} {Fields}},
	url = {https://arxiv.org/abs/2304.10050v1},
	abstract = {The various aspects like modeling and interpreting 3D environments and surroundings have enticed humans to progress their research in 3D Computer Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in Computer Graphics, Robotics, Computer Vision, and the possible scope of High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D models have gained traction from res with more than 500 preprints related to NeRFs published. This paper serves as a bridge for people starting to study these fields by building on the basics of Mathematics, Geometry, Computer Vision, and Computer Graphics to the difficulties encountered in Implicit Representations at the intersection of all these disciplines. This survey provides the history of rendering, Implicit Learning, and NeRFs, the progression of research on NeRFs, and the potential applications and implications of NeRFs in today's world. In doing so, this survey categorizes all the NeRF-related research in terms of the datasets used, objective functions, applications solved, and evaluation criteria for these applications.},
	language = {en},
	urldate = {2023-04-22},
	author = {Mittal, Ansh},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{tao_lidar-nerf_2023,
	title = {{LiDAR}-{NeRF}: {Novel} {LiDAR} {View} {Synthesis} via {Neural} {Radiance} {Fields}},
	shorttitle = {{LiDAR}-{NeRF}},
	url = {https://arxiv.org/abs/2304.10406v1},
	abstract = {We introduce a new task, novel view synthesis for LiDAR sensors. While traditional model-based LiDAR simulators with style-transfer neural networks can be applied to render novel views, they fall short in producing accurate and realistic LiDAR patterns, because the renderers they rely on exploit game engines, which are not differentiable. We address this by formulating, to the best of our knowledge, the first differentiable LiDAR renderer, and propose an end-to-end framework, LiDAR-NeRF, leveraging a neural radiance field (NeRF) to enable jointly learning the geometry and the attributes of 3D points. To evaluate the effectiveness of our approach, we establish an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains observations of objects from 9 categories seen from 360-degree viewpoints captured with multiple LiDAR sensors. Our extensive experiments on the scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our LiDAR- NeRF surpasses the model-based algorithms significantly.},
	language = {en},
	urldate = {2023-04-22},
	author = {Tao, Tang and Gao, Longfei and Wang, Guangrun and Chen, Peng and Hao, Dayang and Liang, Xiaodan and Salzmann, Mathieu and Yu, Kaicheng},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{toschi_relight_2023,
	title = {{ReLight} {My} {NeRF}: {A} {Dataset} for {Novel} {View} {Synthesis} and {Relighting} of {Real} {World} {Objects}},
	shorttitle = {{ReLight} {My} {NeRF}},
	url = {https://arxiv.org/abs/2304.10448v1},
	abstract = {In this paper, we focus on the problem of rendering novel views from a Neural Radiance Field (NeRF) under unobserved light conditions. To this end, we introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world objects under one-light-at-time (OLAT) conditions, annotated with accurate ground-truth camera and light poses. Our acquisition pipeline leverages two robotic arms holding, respectively, a camera and an omni-directional point-wise light source. We release a total of 20 scenes depicting a variety of objects with complex geometry and challenging materials. Each scene includes 2000 images, acquired from 50 different points of views under 40 different OLAT conditions. By leveraging the dataset, we perform an ablation study on the relighting capability of variants of the vanilla NeRF architecture and identify a lightweight architecture that can render novel views of an object under novel light conditions, which we use to establish a non-trivial baseline for the dataset. Dataset and benchmark are available at https://eyecan-ai.github.io/rene.},
	language = {en},
	urldate = {2023-04-22},
	author = {Toschi, Marco and De Matteo, Riccardo and Spezialetti, Riccardo and De Gregorio, Daniele and Di Stefano, Luigi and Salti, Samuele},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{warburg_nerfbusters_2023,
	title = {Nerfbusters: {Removing} {Ghostly} {Artifacts} from {Casually} {Captured} {NeRFs}},
	shorttitle = {Nerfbusters},
	url = {https://arxiv.org/abs/2304.10532v1},
	abstract = {Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.},
	language = {en},
	urldate = {2023-04-22},
	author = {Warburg, Frederik and Weber, Ethan and Tancik, Matthew and Holynski, Aleksander and Kanazawa, Angjoo},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{xu_revisiting_2023,
	title = {Revisiting {Implicit} {Neural} {Representations} in {Low}-{Level} {Vision}},
	url = {https://arxiv.org/abs/2304.10250v1},
	abstract = {Implicit Neural Representation (INR) has been emerging in computer vision in recent years. It has been shown to be effective in parameterising continuous signals such as dense 3D models from discrete image data, e.g. the neural radius field (NeRF). However, INR is under-explored in 2D image processing tasks. Considering the basic definition and the structure of INR, we are interested in its effectiveness in low-level vision problems such as image restoration. In this work, we revisit INR and investigate its application in low-level image restoration tasks including image denoising, super-resolution, inpainting, and deblurring. Extensive experimental evaluations suggest the superior performance of INR in several low-level vision tasks with limited resources, outperforming its counterparts by over 2dB. Code and models are available at https://github.com/WenTXuL/LINR},
	language = {en},
	urldate = {2023-04-22},
	author = {Xu, Wentian and Jiao, Jianbo},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{warburg_nerfbusters_2023-1,
	title = {Nerfbusters: {Removing} {Ghostly} {Artifacts} from {Casually} {Captured} {NeRFs}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Nerfbusters},
	url = {https://arxiv.org/abs/2304.10532},
	doi = {10.48550/ARXIV.2304.10532},
	abstract = {Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.},
	urldate = {2023-04-22},
	author = {Warburg, Frederik and Weber, Ethan and Tancik, Matthew and Holynski, Aleksander and Kanazawa, Angjoo},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {/unread, Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Graphics (cs.GR)},
}

@misc{son_singraf_2022,
	title = {{SinGRAF}: {Learning} a {3D} {Generative} {Radiance} {Field} for a {Single} {Scene}},
	shorttitle = {{SinGRAF}},
	url = {https://arxiv.org/abs/2211.17260v2},
	abstract = {Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.},
	language = {en},
	urldate = {2023-04-21},
	author = {Son, Minjung and Park, Jeong Joon and Guibas, Leonidas and Wetzstein, Gordon},
	month = nov,
	year = {2022},
	keywords = {/unread},
}

@misc{lin_3d_2022,
	title = {{3D} {GAN} {Inversion} for {Controllable} {Portrait} {Image} {Animation}},
	url = {https://arxiv.org/abs/2203.13441v1},
	abstract = {Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.},
	language = {en},
	urldate = {2023-04-21},
	author = {Lin, Connor Z. and Lindell, David B. and Chan, Eric R. and Wetzstein, Gordon},
	month = mar,
	year = {2022},
	keywords = {/unread},
}

@misc{chan_generative_2023,
	title = {Generative {Novel} {View} {Synthesis} with {3D}-{Aware} {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2304.02602v1},
	abstract = {We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.},
	language = {en},
	urldate = {2023-04-21},
	author = {Chan, Eric R. and Nagano, Koki and Chan, Matthew A. and Bergman, Alexander W. and Park, Jeong Joon and Levy, Axel and Aittala, Miika and De Mello, Shalini and Karras, Tero and Wetzstein, Gordon},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{shue_3d_2022,
	title = {{3D} {Neural} {Field} {Generation} using {Triplane} {Diffusion}},
	url = {https://arxiv.org/abs/2211.16677v1},
	abstract = {Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.},
	language = {en},
	urldate = {2023-04-21},
	author = {Shue, J. Ryan and Chan, Eric Ryan and Po, Ryan and Ankner, Zachary and Wu, Jiajun and Wetzstein, Gordon},
	month = nov,
	year = {2022},
	keywords = {/unread},
}

@misc{tewari_advances_2022,
	title = {Advances in {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2111.05849},
	doi = {10.48550/arXiv.2111.05849},
	abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and Martin-Brualla, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
	month = mar,
	year = {2022},
	note = {arXiv:2111.05849 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58452-8},
	shorttitle = {{NeRF}},
	url = {https://arxiv.org/abs/2003.08934},
	doi = {10.1007/978-3-030-58452-8_24},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction \$\$({\textbackslash}theta ,{\textbackslash}phi )\$\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for  convincing comparisons.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread, 3D deep learning, Done Reading, Emphasis, Image-based rendering, Scene representation, View synthesis, Volume rendering},
	pages = {405--421},
}

@misc{mirzaei_reference-guided_2023,
	title = {Reference-guided {Controllable} {Inpainting} of {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2304.09677},
	doi = {10.48550/arXiv.2304.09677},
	abstract = {The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led to a desire for NeRF editing tools. Here, we focus on inpainting regions in a view-consistent and controllable manner. In addition to the typical NeRF inputs and masks delineating the unwanted region in each view, we require only a single inpainted view of the scene, i.e., a reference view. We use monocular depth estimators to back-project the inpainted view to the correct 3D positions. Then, via a novel rendering technique, a bilateral solver can construct view-dependent effects in non-reference views, making the inpainted region appear consistent from any view. For non-reference disoccluded regions, which cannot be supervised by the single reference view, we devise a method based on image inpainters to guide both the geometry and appearance. Our approach shows superior performance to NeRF inpainting baselines, with the additional advantage that a user can control the generated scene via a single inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Mirzaei, Ashkan and Aumentado-Armstrong, Tristan and Brubaker, Marcus A. and Kelly, Jonathan and Levinshtein, Alex and Derpanis, Konstantinos G. and Gilitschenski, Igor},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09677 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{gortler_visual_2019,
	title = {A {Visual} {Exploration} of {Gaussian} {Processes}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
	doi = {10.23915/distill.00017},
	abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
	language = {en},
	number = {4},
	urldate = {2023-04-20},
	journal = {Distill},
	author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
	month = apr,
	year = {2019},
	keywords = {/unread},
	pages = {e17},
}

@misc{kerr_lerf_2023,
	title = {{LERF}: {Language} {Embedded} {Radiance} {Fields}},
	shorttitle = {{LERF}},
	url = {http://arxiv.org/abs/2303.09553},
	doi = {10.48550/arXiv.2303.09553},
	abstract = {Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found at https://lerf.io .},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09553 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{liu_nerf-loc_2023,
	title = {{NeRF}-{Loc}: {Visual} {Localization} with {Conditional} {Neural} {Radiance} {Field}},
	shorttitle = {{NeRF}-{Loc}},
	url = {http://arxiv.org/abs/2304.07979},
	doi = {10.48550/arXiv.2304.07979},
	abstract = {We propose a novel visual re-localization method based on direct matching between the implicit 3D descriptors and the 2D image with transformer. A conditional neural radiance field(NeRF) is chosen as the 3D scene representation in our pipeline, which supports continuous 3D descriptors generation and neural rendering. By unifying the feature matching and the scene coordinate regression to the same framework, our model learns both generalizable knowledge and scene prior respectively during two training stages. Furthermore, to improve the localization robustness when domain gap exists between training and testing phases, we propose an appearance adaptation layer to explicitly align styles between the 3D model and the query image. Experiments show that our method achieves higher localization accuracy than other learning-based approaches on multiple benchmarks. Code is available at {\textbackslash}url\{https://github.com/JenningsL/nerf-loc\}.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Liu, Jianlin and Nie, Qiang and Liu, Yong and Wang, Chengjie},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07979 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{song_moda_2023,
	title = {{MoDA}: {Modeling} {Deformable} {3D} {Objects} from {Casual} {Videos}},
	shorttitle = {{MoDA}},
	url = {http://arxiv.org/abs/2304.08279},
	doi = {10.48550/arXiv.2304.08279},
	abstract = {In this paper, we focus on the challenges of modeling deformable 3D objects from casual videos. With the popularity of neural radiance fields (NeRF), many works extend it to dynamic scenes with a canonical NeRF and a deformation model that achieves 3D point transformation between the observation space and the canonical space. Recent works rely on linear blend skinning (LBS) to achieve the canonical-observation transformation. However, the linearly weighted combination of rigid transformation matrices is not guaranteed to be rigid. As a matter of fact, unexpected scale and shear factors often appear. In practice, using LBS as the deformation model can always lead to skin-collapsing artifacts for bending or twisting motions. To solve this problem, we propose neural dual quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can perform rigid transformation without skin-collapsing artifacts. Besides, we introduce a texture filtering approach for texture rendering that effectively minimizes the impact of noisy colors outside target deformable objects. Extensive experiments on real and synthetic datasets show that our approach can reconstruct 3D models for humans and animals with better qualitative and quantitative performance than state-of-the-art methods.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Song, Chaoyue and Chen, Tianyi and Chen, Yiwen and Wei, Jiacheng and Foo, Chuan Sheng and Liu, Fayao and Lin, Guosheng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08279 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_cat-nerf_2023,
	title = {{CAT}-{NeRF}: {Constancy}-{Aware} {Tx}\${\textasciicircum}2\${Former} for {Dynamic} {Body} {Modeling}},
	shorttitle = {{CAT}-{NeRF}},
	url = {http://arxiv.org/abs/2304.07915},
	doi = {10.48550/arXiv.2304.07915},
	abstract = {This paper addresses the problem of human rendering in the video with temporal appearance constancy. Reconstructing dynamic body shapes with volumetric neural rendering methods, such as NeRF, requires finding the correspondence of the points in the canonical and observation space, which demands understanding human body shape and motion. Some methods use rigid transformation, such as SE(3), which cannot precisely model each frame's unique motion and muscle movements. Others generate the transformation for each frame with a trainable network, such as neural blend weight field or translation vector field, which does not consider the appearance constancy of general body shape. In this paper, we propose CAT-NeRF for self-awareness of appearance constancy with Tx\${\textasciicircum}2\$Former, a novel way to combine two Transformer layers, to separate appearance constancy and uniqueness. Appearance constancy models the general shape across the video, and uniqueness models the unique patterns for each frame. We further introduce a novel Covariance Loss to limit the correlation between each pair of appearance uniquenesses to ensure the frame-unique pattern is maximally captured in appearance uniqueness. We assess our method on H36M and ZJU-MoCap and show state-of-the-art performance.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Zhu, Haidong and Zheng, Zhaoheng and Zheng, Wanrong and Nevatia, Ram},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07915 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.08584},
	doi = {10.48550/arXiv.1901.08584},
	abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	month = may,
	year = {2019},
	note = {arXiv:1901.08584 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1806.08734},
	doi = {10.48550/arXiv.1806.08734},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100{\textbackslash}\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets {\textbackslash}emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {arXiv:1806.08734 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{basri_convergence_2019,
	title = {The {Convergence} {Rate} of {Neural} {Networks} for {Learned} {Functions} of {Different} {Frequencies}},
	url = {http://arxiv.org/abs/1906.00425},
	doi = {10.48550/arXiv.1906.00425},
	abstract = {We study the relationship between the frequency of a function and the speed at which a neural network learns it. We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system. When normalized training data is uniformly distributed on a hypersphere, the eigenfunctions of this linear system are spherical harmonic functions. We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model. This bias term had been omitted from the linear network model without significantly affecting previous theoretical results. However, we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple, low frequency functions with odd frequencies. Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency. These predictions match the empirical behavior of both shallow and deep networks.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
	month = dec,
	year = {2019},
	note = {arXiv:1906.00425 [cs, eess, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{arora_exact_2019,
	title = {On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}},
	url = {http://arxiv.org/abs/1904.11955},
	doi = {10.48550/arXiv.1904.11955},
	abstract = {How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its width --- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers --- is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for the performance of a pure kernel-based method on CIFAR-10, being \$10{\textbackslash}\%\$ higher than the methods reported in [Novak et al., 2019], and only \$6{\textbackslash}\%\$ lower than the performance of the corresponding finite deep net architecture (once batch normalization, etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
	month = nov,
	year = {2019},
	note = {arXiv:1904.11955 [cs, stat]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1811.03804},
	doi = {10.48550/arXiv.1811.03804},
	abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = may,
	year = {2019},
	note = {arXiv:1811.03804 [cs, math, stat]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{jacot_neural_2020,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/1806.07572},
	doi = {10.48550/arXiv.1806.07572},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	month = feb,
	year = {2020},
	note = {arXiv:1806.07572 [cs, math, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{chen_single-stage_2023,
	title = {Single-{Stage} {Diffusion} {NeRF}: {A} {Unified} {Approach} to {3D} {Generation} and {Reconstruction}},
	shorttitle = {Single-{Stage} {Diffusion} {NeRF}},
	url = {http://arxiv.org/abs/2304.06714},
	doi = {10.48550/arXiv.2304.06714},
	abstract = {3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, developing a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generalizable prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models. In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent diffusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for unconditional generation, or combine it with arbitrary observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Chen, Hansheng and Gu, Jiatao and Chen, Anpei and Tian, Wei and Tu, Zhuowen and Liu, Lingjie and Su, Hao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06714 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_swin3d_2023,
	title = {{Swin3D}: {A} {Pretrained} {Transformer} {Backbone} for {3D} {Indoor} {Scene} {Understanding}},
	shorttitle = {{Swin3D}},
	url = {http://arxiv.org/abs/2304.06906},
	abstract = {Pretrained backbones with fine-tuning have been widely adopted in 2D vision and natural language processing tasks and demonstrated significant advantages to task-specific networks. In this paper, we present a pretrained 3D backbone, named \{{\textbackslash}SST\}, which first outperforms all state-of-the-art methods in downstream 3D indoor scene understanding tasks. Our backbone network is based on a 3D Swin transformer and carefully designed to efficiently conduct self-attention on sparse voxels with linear memory complexity and capture the irregularity of point signals via generalized contextual relative positional embedding. Based on this backbone design, we pretrained a large \{{\textbackslash}SST\} model on a synthetic Structed3D dataset that is 10 times larger than the ScanNet dataset and fine-tuned the pretrained model in various downstream real-world indoor scene understanding tasks. The results demonstrate that our model pretrained on the synthetic dataset not only exhibits good generality in both downstream segmentation and detection on real 3D point datasets, but also surpasses the state-of-the-art methods on downstream tasks after fine-tuning with +2.3 mIoU and +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation, +2.1 mIoU on ScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, +8.1 mAP@0.5 on S3DIS detection. Our method demonstrates the great potential of pretrained 3D backbones with fine-tuning for 3D understanding tasks. The code and models are available at https://github.com/microsoft/Swin3D .},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Yang, Yu-Qi and Guo, Yu-Xiao and Xiong, Jian-Yu and Liu, Yang and Pan, Hao and Wang, Peng-Shuai and Tong, Xin and Guo, Baining},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06906 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yu_inpaint_2023,
	title = {Inpaint {Anything}: {Segment} {Anything} {Meets} {Image} {Inpainting}},
	shorttitle = {Inpaint {Anything}},
	url = {http://arxiv.org/abs/2304.06790},
	doi = {10.48550/arXiv.2304.06790},
	abstract = {Modern image inpainting systems, despite the significant progress, often struggle with mask selection and holes filling. Based on Segment-Anything Model (SAM), we make the first attempt to the mask-free image inpainting and propose a new paradigm of ``clicking and filling'', which is named as Inpaint Anything (IA). The core idea behind IA is to combine the strengths of different models in order to build a very powerful and user-friendly pipeline for solving inpainting-related problems. IA supports three main features: (i) Remove Anything: users could click on an object and IA will remove it and smooth the ``hole'' with the context; (ii) Fill Anything: after certain objects removal, users could provide text-based prompts to IA, and then it will fill the hole with the corresponding generative content via driving AIGC models like Stable Diffusion; (iii) Replace Anything: with IA, users have another option to retain the click-selected object and replace the remaining background with the newly generated scenes. We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA). Our codes are available at https://github.com/geekyutao/Inpaint-Anything.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Yu, Tao and Feng, Runseng and Feng, Ruoyu and Liu, Jinming and Jin, Xin and Zeng, Wenjun and Chen, Zhibo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06790 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fan_uva_2023,
	title = {{UVA}: {Towards} {Unified} {Volumetric} {Avatar} for {View} {Synthesis}, {Pose} rendering, {Geometry} and {Texture} {Editing}},
	shorttitle = {{UVA}},
	url = {http://arxiv.org/abs/2304.06969},
	abstract = {Neural radiance field (NeRF) has become a popular 3D representation method for human avatar reconstruction due to its high-quality rendering capabilities, e.g., regarding novel views and poses. However, previous methods for editing the geometry and appearance of the avatar only allow for global editing through body shape parameters and 2D texture maps. In this paper, we propose a new approach named {\textbackslash}textbf\{U\}nified {\textbackslash}textbf\{V\}olumetric {\textbackslash}textbf\{A\}vatar ({\textbackslash}textbf\{UVA\}) that enables local and independent editing of both geometry and texture, while retaining the ability to render novel views and poses. UVA transforms each observation point to a canonical space using a skinning motion field and represents geometry and texture in separate neural fields. Each field is composed of a set of structured latent codes that are attached to anchor nodes on a deformable mesh in canonical space and diffused into the entire space via interpolation, allowing for local editing. To address spatial ambiguity in code interpolation, we use a local signed height indicator. We also replace the view-dependent radiance color with a pose-dependent shading factor to better represent surface illumination in different poses. Experiments on multiple human avatars demonstrate that our UVA achieves competitive results in novel view synthesis and novel pose rendering while enabling local and independent editing of geometry and appearance. The source code will be released.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Fan, Jinlong and Zhang, Jing and Tao, Dacheng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06969 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{barron_zip-nerf_2023,
	title = {Zip-{NeRF}: {Anti}-{Aliased} {Grid}-{Based} {Neural} {Radiance} {Fields}},
	shorttitle = {Zip-{NeRF}},
	url = {http://arxiv.org/abs/2304.06706},
	doi = {10.48550/arXiv.2304.06706},
	abstract = {Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8\% - 76\% lower than either prior technique, and that trains 22x faster than mip-NeRF 360.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06706 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{zhang_nerflets_2023,
	title = {Nerflets: {Local} {Radiance} {Fields} for {Efficient} {Structure}-{Aware} {3D} {Scene} {Representation} from {2D} {Supervision}},
	shorttitle = {Nerflets},
	url = {http://arxiv.org/abs/2303.03361},
	doi = {10.48550/arXiv.2303.03361},
	abstract = {We address efficient and structure-aware 3D scene representation from images. Nerflets are our key contribution -- a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, orientation, and extent, within which it contributes to panoptic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor environments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) allow the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Zhang, Xiaoshuai and Kundu, Abhijit and Funkhouser, Thomas and Guibas, Leonidas and Su, Hao and Genova, Kyle},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03361 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	doi = {10.48550/arXiv.2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sandstrom_point-slam_2023,
	title = {Point-{SLAM}: {Dense} {Neural} {Point} {Cloud}-based {SLAM}},
	shorttitle = {Point-{SLAM}},
	url = {http://arxiv.org/abs/2304.04278},
	doi = {10.48550/arXiv.2304.04278},
	abstract = {We propose a dense neural simultaneous localization and mapping (SLAM) approach for monocular RGBD input which anchors the features of a neural scene representation in a point cloud that is iteratively generated in an input-dependent data-driven manner. We demonstrate that both tracking and mapping can be performed with the same point-based neural scene representation by minimizing an RGBD-based re-rendering loss. In contrast to recent dense neural SLAM methods which anchor the scene features in a sparse grid, our point-based approach allows dynamically adapting the anchor point density to the information density of the input. This strategy reduces runtime and memory usage in regions with fewer details and dedicates higher point density to resolve fine details. Our approach performs either better or competitive to existing dense neural RGBD SLAM methods in tracking, mapping and rendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code is available at https://github.com/tfy14esa/Point-SLAM.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Sandström, Erik and Li, Yue and Van Gool, Luc and Oswald, Martin R.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04278 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{InstantNGP}},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	language = {en},
	number = {4},
	urldate = {2022-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	month = jul,
	year = {2022},
	note = {titleTranslation:},
	keywords = {2022, Emphasis, Fast Rendering, Fast Training, Hash Encoding, NeRF, Reading, SIGGRAPH},
	pages = {1--15},
}

@inproceedings{takikawa_neural_2021,
	title = {Neural {Geometric} {Level} of {Detail}: {Real}-time {Rendering} with {Implicit} {3D} {Shapes}},
	shorttitle = {Neural {Geometric} {Level} of {Detail}},
	doi = {10.1109/CVPR46437.2021.01120},
	abstract = {Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Measurement, Neural networks, Octrees, Rendering (computer graphics), Shape, Surface reconstruction, Three-dimensional displays},
	pages = {11353--11362},
}

@inproceedings{huang_di-fusion_2021,
	title = {{DI}-{Fusion}: {Online} {Implicit} {3D} {Reconstruction} {With} {Deep} {Priors}},
	shorttitle = {{DI}-{Fusion}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Huang_DI-Fusion_Online_Implicit_3D_Reconstruction_With_Deep_Priors_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-04-09},
	author = {Huang, Jiahui and Huang, Shi-Sheng and Song, Haoxuan and Hu, Shi-Min},
	year = {2021},
	keywords = {/unread},
	pages = {8932--8941},
}

@inproceedings{peng_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Occupancy} {Networks}},
	isbn = {978-3-030-58580-8},
	doi = {10.1007/978-3-030-58580-8_31},
	abstract = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread},
	pages = {523--540},
}

@misc{kondo_vaxnerf_2021,
	title = {{VaxNeRF}: {Revisiting} the {Classic} for {Voxel}-{Accelerated} {Neural} {Radiance} {Field}},
	shorttitle = {{VaxNeRF}},
	url = {http://arxiv.org/abs/2111.13112},
	doi = {10.48550/arXiv.2111.13112},
	abstract = {Neural Radiance Field (NeRF) is a popular method in data-driven 3D reconstruction. Given its simplicity and high quality rendering, many NeRF applications are being developed. However, NeRF's big limitation is its slow speed. Many attempts are made to speeding up NeRF training and inference, including intricate code-level optimization and caching, use of sophisticated data structures, and amortization through multi-task and meta learning. In this work, we revisit the basic building blocks of NeRF through the lens of classic techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF), integrating NeRF with visual hull, a classic 3D reconstruction technique only requiring binary foreground-background pixel labels per image. Visual hull, which can be optimized in about 10 seconds, can provide coarse in-out field separation to omit substantial amounts of network evaluations in NeRF. We provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF codebase, consisting of only about 30 lines of code changes and a modular visual hull subroutine, and achieve about 2-8x faster learning on top of the highly-performative JaxNeRF baseline with zero degradation in rendering quality. With sufficient compute, this effectively brings down full NeRF training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of a classic technique with a deep method (that arguably replaced it) -- can empower and accelerate new NeRF extensions and applications, with its simplicity, portability, and reliable performance gains. Codes are available at https://github.com/naruya/VaxNeRF .},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Kondo, Naruya and Ikeda, Yuya and Tagliasacchi, Andrea and Matsuo, Yutaka and Ochiai, Yoichi and Gu, Shixiang Shane},
	month = nov,
	year = {2021},
	note = {arXiv:2111.13112 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jiang_instantavatar_2022,
	title = {{InstantAvatar}: {Learning} {Avatars} from {Monocular} {Video} in 60 {Seconds}},
	shorttitle = {{InstantAvatar}},
	url = {http://arxiv.org/abs/2212.10550},
	doi = {10.48550/arXiv.2212.10550},
	abstract = {In this paper, we take a significant step towards real-world applicability of monocular neural avatar reconstruction by contributing InstantAvatar, a system that can reconstruct human avatars from a monocular video within seconds, and these avatars can be animated and rendered at an interactive rate. To achieve this efficiency we propose a carefully designed and engineered system, that leverages emerging acceleration structures for neural fields, in combination with an efficient empty space-skipping strategy for dynamic scenes. We also contribute an efficient implementation that we will make available for research purposes. Compared to existing methods, InstantAvatar converges 130x faster and can be trained in minutes instead of hours. It achieves comparable or even better reconstruction quality and novel pose synthesis results. When given the same time budget, our method significantly outperforms SoTA methods. InstantAvatar can yield acceptable visual quality in as little as 10 seconds training time.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10550 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhi_-place_2021,
	title = {In-{Place} {Scene} {Labelling} and {Understanding} with {Implicit} {Scene} {Representation}},
	shorttitle = {{SemanticNeRF}},
	url = {http://arxiv.org/abs/2103.15875},
	doi = {10.48550/arXiv.2103.15875},
	abstract = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Zhi, Shuaifeng and Laidlow, Tristan and Leutenegger, Stefan and Davison, Andrew J.},
	month = aug,
	year = {2021},
	note = {arXiv:2103.15875 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mescheder_occupancy_2019,
	title = {Occupancy {Networks}: {Learning} {3D} {Reconstruction} in {Function} {Space}},
	shorttitle = {Occupancy {Networks}},
	url = {http://arxiv.org/abs/1812.03828},
	doi = {10.48550/arXiv.1812.03828},
	abstract = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
	month = apr,
	year = {2019},
	note = {arXiv:1812.03828 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_neural_2021,
	title = {Neural {Sparse} {Voxel} {Fields}},
	url = {http://arxiv.org/abs/2007.11571},
	doi = {10.48550/arXiv.2007.11571},
	abstract = {Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},
	month = jan,
	year = {2021},
	note = {arXiv:2007.11571 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{garbin_fastnerf_2021,
	title = {{FastNeRF}: {High}-{Fidelity} {Neural} {Rendering} at {200FPS}},
	shorttitle = {{FastNeRF}},
	url = {http://arxiv.org/abs/2103.10380},
	doi = {10.48550/arXiv.2103.10380},
	abstract = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Garbin, Stephan J. and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien},
	month = apr,
	year = {2021},
	note = {arXiv:2103.10380 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	language = {en},
	number = {5},
	urldate = {2023-04-07},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {/unread, Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@phdthesis{wojciech_efficient_nodate,
	title = {Efficient {Monte} {Carlo} {Methods} for {Light} {Transport} in {Scattering} {Media}},
	abstract = {In this dissertation we focus on developing accurate and efficient Monte Carlo methods for synthesizing images containing general participating media. Participating media such as clouds, smoke, and fog are ubiquitous in the world and are responsible for many important visual phenomena which are of interest to computer graphics as well as related fields. In these situations, the medium participatesin lighting interactions by scattering or absorbing photons as they travel through the scene. Though these effects add atmosphere and considerable depth to rendered images they are computationally very expensive to simulate. Most practical solutions make significant simplifying assumptions about the medium in order to maintain efficiency. Unfortunately, simulating light transport in general scattering media efficiently and with high accuracy is a challenging undertaking. In this dissertation, we address this problem by presenting two complementary techniques for efficiently computing lighting within arbitrary participating media.

We first turn to the irradiance caching method for computing indirect illumination on surfaces. Irradiance caching gains efficiency by computing an accurate representation of lighting only at a sparse set of locations and reusing these values through interpolation whenever possible. We derive the mathematical concepts that form the foundation of this approach and analyze its strengths and weaknesses. Drawing inspiration from this algorithm, we then introduce a novel volumetric radiance caching method for efficiently simulating global illumination within participating media. In developing the technique we also introduce efficient methods for evaluating the gradient of the lighting within participating media. Our gradient analysis has immediate applicability for improved interpolation quality in both surface and media-based caching methods.

We also develop a novel photon mapping technique for participating media. We present a theoretical reformulation of volumetric photon mapping which provides significant new insights. This reformulation makes it easier to qualify the error introduced by the radiance estimate, but, more importantly, also allows us to develop more efficient rendering techniques. Conventional photon mapping methods compute lighting by first simulating the propagation of photons from light sources and then using this collection of photons to estimate lighting at any pointin the scene. Our reformulation allows us to compute the accumulated lighting along the length of entire rays as they penetrate the medium. This algorithmic improvement provides for significantly reduced render times and even the potential for real-time visualization of light transport in participating media.},
	school = {UCSD},
	author = {Wojciech, Jarosz},
	note = {titleTranslation:
titleTranslation:},
	keywords = {/unread},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	shorttitle = {{MoCo}},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Buildings, Dictionaries, Loss measurement, Task analysis, Training, Unsupervised learning, Visualization},
	pages = {9726--9735},
}

@misc{hu_point2pix_2023,
	title = {{Point2Pix}: {Photo}-{Realistic} {Point} {Cloud} {Rendering} via {Neural} {Radiance} {Fields}},
	shorttitle = {{Point2Pix}},
	url = {http://arxiv.org/abs/2303.16482},
	doi = {10.48550/arXiv.2303.16482},
	abstract = {Synthesizing photo-realistic images from a point cloud is challenging because of the sparsity of point cloud representation. Recent Neural Radiance Fields and extensions are proposed to synthesize realistic images from 2D input. In this paper, we present Point2Pix as a novel point renderer to link the 3D sparse point clouds with 2D dense image pixels. Taking advantage of the point cloud 3D prior and NeRF rendering pipeline, our method can synthesize high-quality images from colored point clouds, generally for novel indoor scenes. To improve the efficiency of ray sampling, we propose point-guided sampling, which focuses on valid samples. Also, we present Point Encoding to build Multi-scale Radiance Fields that provide discriminative 3D point features. Finally, we propose Fusion Encoding to efficiently synthesize high-quality images. Extensive experiments on the ScanNet and ArkitScenes datasets demonstrate the effectiveness and generalization.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Hu, Tao and Xu, Xiaogang and Liu, Shu and Jia, Jiaya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16482 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_trivol_2023,
	title = {{TriVol}: {Point} {Cloud} {Rendering} via {Triple} {Volumes}},
	shorttitle = {{TriVol}},
	url = {http://arxiv.org/abs/2303.16485},
	doi = {10.48550/arXiv.2303.16485},
	abstract = {Existing learning-based methods for point cloud rendering adopt various 3D representations and feature querying mechanisms to alleviate the sparsity problem of point clouds. However, artifacts still appear in rendered images, due to the challenges in extracting continuous and discriminative 3D features from point clouds. In this paper, we present a dense while lightweight 3D representation, named TriVol, that can be combined with NeRF to render photo-realistic images from point clouds. Our TriVol consists of triple slim volumes, each of which is encoded from the point cloud. TriVol has two advantages. First, it fuses respective fields at different scales and thus extracts local and non-local features for discriminative representation. Second, since the volume size is greatly reduced, our 3D decoder can be efficiently inferred, allowing us to increase the resolution of the 3D space to render more point details. Extensive experiments on different benchmarks with varying kinds of scenes/objects demonstrate our framework's effectiveness compared with current approaches. Moreover, our framework has excellent generalization ability to render a category of scenes/objects without fine-tuning.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Hu, Tao and Xu, Xiaogang and Chu, Ruihang and Jia, Jiaya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16485 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_dropout_2023,
	title = {Dropout {Reduces} {Underfitting}},
	url = {http://arxiv.org/abs/2303.01500},
	doi = {10.48550/arXiv.2303.01500},
	abstract = {Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout .},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01500 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{pumarola_d-nerf_2021,
	title = {D-{NeRF}: {Neural} {Radiance} {Fields} for {Dynamic} {Scenes}},
	shorttitle = {D-{NeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-03-30},
	author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
	year = {2021},
	keywords = {/unread},
	pages = {10318--10327},
}

@misc{liao_kitti-360_2022,
	title = {{KITTI}-360: {A} {Novel} {Dataset} and {Benchmarks} for {Urban} {Scene} {Understanding} in {2D} and {3D}},
	shorttitle = {{KITTI}-360},
	url = {http://arxiv.org/abs/2109.13410},
	doi = {10.48550/arXiv.2109.13410},
	abstract = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Liao, Yiyi and Xie, Jun and Geiger, Andreas},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13410 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_f2-nerf_2023,
	title = {F\${\textasciicircum}\{2\}\$-{NeRF}: {Fast} {Neural} {Radiance} {Field} {Training} with {Free} {Camera} {Trajectories}},
	shorttitle = {F\${\textasciicircum}\{2\}\$-{NeRF}},
	url = {https://arxiv.org/abs/2303.15951v1},
	abstract = {This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360-degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us. Project page: https://totoro97.github.io/projects/f2-nerf.},
	language = {en},
	urldate = {2023-03-30},
	author = {Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
	month = mar,
	year = {2023},
	keywords = {/unread},
}

@inproceedings{long_neuraludf_2022,
	title = {{NeuralUDF}: {Learning} {Unsigned} {Distance} {Fields} for {Multi}-view {Reconstruction} of {Surfaces} with {Arbitrary} {Topologies}},
	shorttitle = {{NeuralUDF}},
	url = {https://www.xxlong.site/NeuralUDF/},
	abstract = {We present a novel method, called NeuralUDF, for reconstructing surfaces with arbitrary topologies from 2D images via volume rendering. Recent advances in neural rendering based reconstruction have achieved compelling results. However, these methods are limited to objects with closed surfaces since they adopt Signed Distance Function (SDF) as surface representation which requires the target shape to be divided into inside and outside. In this paper, we propose to represent surfaces as the Unsigned Distance Function (UDF) and develop a new volume rendering scheme to learn the neural UDF representation. Specifically, a new density function that correlates the property of UDF with the volume rendering scheme is introduced for robust optimization of the UDF fields. Experiments on the DTU and DeepFashion3D datasets show that our method not only enables high-quality reconstruction of non-closed shapes with complex typologies, but also achieves comparable performance to the SDF based methods on the reconstruction of closed surfaces.},
	urldate = {2023-03-30},
	author = {Long, Xiaoxiao and Lin, Cheng and Liu, Lingjie and Liu, Yuan and Wang, Peng and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	year = {2022},
	keywords = {/unread, ⛔ No DOI found},
}

@misc{atzmon_sal_2020,
	title = {{SAL}: {Sign} {Agnostic} {Learning} of {Shapes} from {Raw} {Data}},
	shorttitle = {{SAL}},
	url = {http://arxiv.org/abs/1911.10414},
	doi = {10.48550/arXiv.1911.10414},
	abstract = {Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Atzmon, Matan and Lipman, Yaron},
	month = mar,
	year = {2020},
	note = {arXiv:1911.10414 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{jin_reliable_2023,
	title = {Reliable {Image} {Dehazing} by {NeRF}},
	url = {http://arxiv.org/abs/2303.09153},
	doi = {10.48550/arXiv.2303.09153},
	abstract = {We present an image dehazing algorithm with high quality, wide application, and no data training or prior needed. We analyze the defects of the original dehazing model, and propose a new and reliable dehazing reconstruction and dehazing model based on the combination of optical scattering model and computer graphics lighting rendering model. Based on the new haze model and the images obtained by the cameras, we can reconstruct the three-dimensional space, accurately calculate the objects and haze in the space, and use the transparency relationship of haze to perform accurate haze removal. To obtain a 3D simulation dataset we used the Unreal 5 computer graphics rendering engine. In order to obtain real shot data in different scenes, we used fog generators, array cameras, mobile phones, underwater cameras and drones to obtain haze data. We use formula derivation, simulation data set and real shot data set result experimental results to prove the feasibility of the new method. Compared with various other methods, we are far ahead in terms of calculation indicators (4 dB higher quality average scene), color remains more natural, and the algorithm is more robust in different scenarios and best in the subjective perception.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Jin, Zheyan and Chen, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09153 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_dehazenerf_2023,
	title = {{DehazeNeRF}: {Multiple} {Image} {Haze} {Removal} and {3D} {Shape} {Reconstruction} using {Neural} {Radiance} {Fields}},
	shorttitle = {{DehazeNeRF}},
	url = {http://arxiv.org/abs/2303.11364},
	doi = {10.48550/arXiv.2303.11364},
	abstract = {Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance for 3D computer vision tasks, including novel view synthesis and 3D shape reconstruction. However, these methods fail in adverse weather conditions. To address this challenge, we introduce DehazeNeRF as a framework that robustly operates in hazy conditions. DehazeNeRF extends the volume rendering equation by adding physically realistic terms that model atmospheric scattering. By parameterizing these terms using suitable networks that match the physical properties, we introduce effective inductive biases, which, together with the proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view haze removal, novel view synthesis, and 3D shape reconstruction where existing approaches fail.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Chen, Wei-Ting and Yifan, Wang and Kuo, Sy-Yen and Wetzstein, Gordon},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11364 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shao_doublefield_2022,
	title = {{DoubleField}: {Bridging} the {Neural} {Surface} and {Radiance} {Fields} for {High}-fidelity {Human} {Reconstruction} and {Rendering}},
	shorttitle = {{DoubleField}},
	url = {http://arxiv.org/abs/2106.03798},
	doi = {10.48550/arXiv.2106.03798},
	abstract = {We introduce DoubleField, a novel framework combining the merits of both surface field and radiance field for high-fidelity human reconstruction and rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. Moreover, a view-to-view transformer is introduced to fuse multi-view features and learn view-dependent features directly from high-resolution inputs. With the modeling power of DoubleField and the view-to-view transformer, our method significantly improves the reconstruction quality of both geometry and appearance, while supporting direct inference, scene-specific high-resolution finetuning, and fast rendering. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for high-quality human model reconstruction and photo-realistic free-viewpoint human rendering. Data and source code will be made public for the research purpose. Please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Shao, Ruizhi and Zhang, Hongwen and Zhang, He and Chen, Mingjia and Cao, Yanpei and Yu, Tao and Liu, Yebin},
	month = mar,
	year = {2022},
	note = {arXiv:2106.03798 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{turki_suds_2023,
	title = {{SUDS}: {Scalable} {Urban} {Dynamic} {Scenes}},
	shorttitle = {{SUDS}},
	url = {http://arxiv.org/abs/2303.14536},
	doi = {10.48550/arXiv.2303.14536},
	abstract = {We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes. Prior work tends to reconstruct single video clips of short durations (up to 10 seconds). Two reasons are that such methods (a) tend to scale linearly with the number of moving objects and input videos because a separate model is built for each and (b) tend to require supervision via 3D bounding boxes and panoptic labels, obtained manually or via category-specific models. As a step towards truly open-world reconstructions of dynamic cities, we introduce two key innovations: (a) we factorize the scene into three separate hash table data structures to efficiently encode static, dynamic, and far-field radiance fields, and (b) we make use of unlabeled target signals consisting of RGB images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most importantly, 2D optical flow. Operationalizing such inputs via photometric, geometric, and feature-metric reconstruction losses enables SUDS to decompose dynamic scenes into the static background, individual objects, and their motions. When combined with our multi-branch table representation, such reconstructions can be scaled to tens of thousands of objects across 1.2 million frames from 1700 videos spanning geospatial footprints of hundreds of kilometers, (to our knowledge) the largest dynamic NeRF built to date. We present qualitative initial results on a variety of tasks enabled by our representations, including novel-view synthesis of dynamic urban scenes, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To compare to prior work, we also evaluate on KITTI and Virtual KITTI 2, surpassing state-of-the-art methods that rely on ground truth 3D bounding box annotations while being 10x quicker to train.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Turki, Haithem and Zhang, Jason Y. and Ferroni, Francesco and Ramanan, Deva},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14536 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{xu_grid-guided_2023,
	title = {Grid-guided {Neural} {Radiance} {Fields} for {Large} {Urban} {Scenes}},
	url = {http://arxiv.org/abs/2303.14001},
	doi = {10.48550/arXiv.2303.14001},
	abstract = {Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred renderings on large-scale scenes due to limited model capacity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alternative solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches suboptimal solutions, producing noisy artifacts in renderings, especially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being computationally efficient. We propose to use a compact multiresolution ground feature plane representation to coarsely capture the scene, and complement it with positional encoding inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground feature planes, can meanwhile gain further refinements, forming a more accurate and compact feature space and output much more natural rendering results.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Xu, Linning and Xiangli, Yuanbo and Peng, Sida and Pan, Xingang and Zhao, Nanxuan and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14001 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_cerberus_2022,
	title = {Cerberus {Transformer}: {Joint} {Semantic}, {Affordance} and {Attribute} {Parsing}},
	shorttitle = {Cerberus {Transformer}},
	url = {http://arxiv.org/abs/2111.12608},
	doi = {10.48550/arXiv.2111.12608},
	abstract = {Multi-task indoor scene understanding is widely considered as an intriguing formulation, as the affinity of different tasks may lead to improved performance. In this paper, we tackle the new problem of joint semantic, affordance and attribute parsing. However, successfully resolving it requires a model to capture long-range dependency, learn from weakly aligned data and properly balance sub-tasks during training. To this end, we propose an attention-based architecture named Cerberus and a tailored training framework. Our method effectively addresses the aforementioned challenges and achieves state-of-the-art performance on all three tasks. Moreover, an in-depth analysis shows concept affinity consistent with human cognition, which inspires us to explore the possibility of weakly supervised learning. Surprisingly, Cerberus achieves strong results using only 0.1\%-1\% annotation. Visualizations further confirm that this success is credited to common attention maps across tasks. Code and models can be accessed at https://github.com/OPEN-AIR-SUN/Cerberus.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Chen, Xiaoxue and Liu, Tianyu and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},
	month = mar,
	year = {2022},
	note = {arXiv:2111.12608 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_urbangiraffe_2023,
	title = {{UrbanGIRAFFE}: {Representing} {Urban} {Scenes} as {Compositional} {Generative} {Neural} {Feature} {Fields}},
	shorttitle = {{UrbanGIRAFFE}},
	url = {http://arxiv.org/abs/2303.14167},
	doi = {10.48550/arXiv.2303.14167},
	abstract = {Generating photorealistic images with controllable camera pose and scene contents is essential for many applications including AR/VR and simulation. Despite the fact that rapid progress has been made in 3D-aware generative models, most existing methods focus on object-centric images and are not applicable to generating urban scenes for free camera viewpoint control and scene editing. To address this challenging task, we propose UrbanGIRAFFE, which uses a coarse 3D panoptic prior, including the layout distribution of uncountable stuff and countable objects, to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, objects, and sky. Using stuff prior in the form of semantic voxel grids, we build a conditioned stuff generator that effectively incorporates the coarse semantic and geometry information. The object layout prior further allows us to learn an object generator from cluttered scenes. With proper loss functions, our approach facilitates photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation. We validate the effectiveness of our model on both synthetic and real-world datasets, including the challenging KITTI-360 dataset.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Yang, Yuanbo and Yang, Yifei and Guo, Hanlei and Xiong, Rong and Wang, Yue and Liao, Yiyi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14167 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gropp_implicit_2020,
	title = {Implicit {Geometric} {Regularization} for {Learning} {Shapes}},
	shorttitle = {{IGR}},
	url = {http://arxiv.org/abs/2002.10099},
	doi = {10.5555/3524938.3525293},
	abstract = {Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.},
	urldate = {2022-12-04},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},
	month = jul,
	year = {2020},
	note = {titleTranslation:},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Statistics - Machine Learning, ⚠️ Invalid DOI},
}

@misc{yariv_multiview_2020,
	title = {Multiview {Neural} {Surface} {Reconstruction} by {Disentangling} {Geometry} and {Appearance}},
	url = {http://arxiv.org/abs/2003.09852},
	doi = {10.48550/arXiv.2003.09852},
	abstract = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Yariv, Lior and Kasten, Yoni and Moran, Dror and Galun, Meirav and Atzmon, Matan and Basri, Ronen and Lipman, Yaron},
	month = oct,
	year = {2020},
	note = {arXiv:2003.09852 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{maggio_loc-nerf_2022,
	title = {Loc-{NeRF}: {Monte} {Carlo} {Localization} using {Neural} {Radiance} {Fields}},
	shorttitle = {Loc-{NeRF}},
	url = {http://arxiv.org/abs/2209.09050},
	doi = {10.48550/arXiv.2209.09050},
	abstract = {We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time global localization with neural radiance fields. We make our code publicly available at https://github.com/MIT-SPARK/Loc-NeRF.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Maggio, Dominic and Abate, Marcus and Shi, Jingnan and Mario, Courtney and Carlone, Luca},
	month = sep,
	year = {2022},
	note = {arXiv:2209.09050 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{wu_visual_2023,
	title = {Visual {ChatGPT}: {Talking}, {Drawing} and {Editing} with {Visual} {Foundation} {Models}},
	shorttitle = {Visual {ChatGPT}},
	url = {http://arxiv.org/abs/2303.04671},
	doi = {10.48550/arXiv.2303.04671},
	abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04671 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jin_reliable_2023-1,
	title = {Reliable {Image} {Dehazing} by {NeRF}},
	url = {http://arxiv.org/abs/2303.09153},
	doi = {10.48550/arXiv.2303.09153},
	abstract = {We present an image dehazing algorithm with high quality, wide application, and no data training or prior needed. We analyze the defects of the original dehazing model, and propose a new and reliable dehazing reconstruction and dehazing model based on the combination of optical scattering model and computer graphics lighting rendering model. Based on the new haze model and the images obtained by the cameras, we can reconstruct the three-dimensional space, accurately calculate the objects and haze in the space, and use the transparency relationship of haze to perform accurate haze removal. To obtain a 3D simulation dataset we used the Unreal 5 computer graphics rendering engine. In order to obtain real shot data in different scenes, we used fog generators, array cameras, mobile phones, underwater cameras and drones to obtain haze data. We use formula derivation, simulation data set and real shot data set result experimental results to prove the feasibility of the new method. Compared with various other methods, we are far ahead in terms of calculation indicators (4 dB higher quality average scene), color remains more natural, and the algorithm is more robust in different scenarios and best in the subjective perception.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Jin, Zheyan and Chen, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09153 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_nerflix_2023,
	title = {{NeRFLiX}: {High}-{Quality} {Neural} {View} {Synthesis} by {Learning} a {Degradation}-{Driven} {Inter}-viewpoint {MiXer}},
	shorttitle = {{NeRFLiX}},
	url = {http://arxiv.org/abs/2303.06919},
	doi = {10.48550/arXiv.2303.06919},
	abstract = {Neural radiance fields (NeRF) show great success in novel view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Zhou, Kun and Li, Wenbo and Wang, Yi and Hu, Tao and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06919 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chan_efficient_2022,
	title = {Efficient {Geometry}-{Aware} {3D} {Generative} {Adversarial} {Networks}},
	shorttitle = {{EG3D}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-13},
	author = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and De Mello, Shalini and Gallo, Orazio and Guibas, Leonidas J. and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
	year = {2022},
	keywords = {/unread},
	pages = {16123--16133},
}

@misc{wang_hf-neus_2022,
	title = {{HF}-{NeuS}: {Improved} {Surface} {Reconstruction} {Using} {High}-{Frequency} {Details}},
	shorttitle = {{HF}-{NeuS}},
	url = {https://arxiv.org/abs/2206.07850v2},
	abstract = {Neural rendering can be used to reconstruct implicit representations of shapes without 3D supervision. However, current neural surface reconstruction methods have difficulty learning high-frequency geometry details, so the reconstructed shapes are often over-smoothed. We develop HF-NeuS, a novel method to improve the quality of surface reconstruction in neural rendering. We follow recent work to model surfaces as signed distance functions (SDFs). First, we offer a derivation to analyze the relationship between the SDF, the volume density, the transparency function, and the weighting function used in the volume rendering equation and propose to model transparency as transformed SDF. Second, we observe that attempting to jointly encode high-frequency and low-frequency components in a single SDF leads to unstable optimization. We propose to decompose the SDF into a base function and a displacement function with a coarse-to-fine strategy to gradually increase the high-frequency details. Finally, we design an adaptive optimization strategy that makes the training process focus on improving those regions near the surface where the SDFs have artifacts. Our qualitative and quantitative results show that our method can reconstruct fine-grained surface details and obtain better surface reconstruction quality than the current state of the art. Code available at https://github.com/yiqun-wang/HFS.},
	language = {en},
	urldate = {2023-03-13},
	journal = {arXiv.org},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.07850},
	keywords = {/unread},
}

@misc{darmon_improving_2022,
	title = {Improving neural implicit surfaces geometry with patch warping},
	url = {http://arxiv.org/abs/2112.09648},
	doi = {10.48550/arXiv.2112.09648},
	abstract = {Neural implicit surfaces have become an important technique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difficulty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping entire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their similarity with a robust structural similarity (SSIM); (ii) handling visibility and occlusion in such a way that incorrect warps are not given too much importance while encouraging a reconstruction as complete as possible. We evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20\% on both datasets.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Darmon, François and Bascle, Bénédicte and Devaux, Jean-Clément and Monasse, Pascal and Aubry, Mathieu},
	month = may,
	year = {2022},
	note = {arXiv:2112.09648 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mi_switch-nerf_2023,
	title = {Switch-{NeRF}: {Learning} {Scene} {Decomposition} with {Mixture} of {Experts} for {Large}-scale {Neural} {Radiance} {Fields}},
	shorttitle = {Switch-{NeRF}},
	url = {https://openreview.net/forum?id=PQ2zoIZqvm},
	abstract = {The Neural Radiance Fields (NeRF) have been recently applied to reconstruct building-scale and even city-scale scenes. To model a large-scale scene efficiently, a dominant strategy is to employ a divide-and-conquer paradigm via performing scene decomposition, which decomposes a complex scene into parts that are further processed by different sub-networks. Existing large-scale NeRFs mainly use heuristic hand-crafted scene decomposition, with regular 3D-distance-based or physical-street-block-based schemes. Although achieving promising results, the hand-crafted schemes limit the capabilities of NeRF in large-scale scene modeling in several aspects. Manually designing a universal scene decomposition rule for different complex scenes is challenging, leading to adaptation issues for different scenarios. The decomposition procedure is not learnable, hindering the network from jointly optimizing the scene decomposition and the radiance fields in an end-to-end manner. The different sub-networks are typically optimized independently, and thus hand-crafted rules are required to composite them to achieve a better consistency. To tackle these issues, we propose Switch-NeRF, a novel end-to-end large-scale NeRF with learning-based scene decomposition. We design a gating network to dispatch 3D points to different NeRF sub-networks. The gating network can be optimized together with the NeRF sub-networks for different scene partitions, by a design with the Sparsely Gated Mixture of Experts (MoE). The outputs from different sub-networks can also be fused in a learnable way in the unified framework to effectively guarantee the consistency of the whole scene. Furthermore, the proposed MoE-based Switch-NeRF model is carefully implemented and optimized to achieve both high-fidelity scene reconstruction and efficient computation. Our method establishes clear state-of-the-art performances on several large-scale datasets. To the best of our knowledge, we are the first to propose an applicable end-to-end sparse NeRF network with learning-based decomposition for large-scale scenes. Codes are released at https://github.com/MiZhenxing/Switch-NeRF.},
	language = {en},
	urldate = {2023-03-13},
	author = {Mi, Zhenxing and Xu, Dan},
	month = feb,
	year = {2023},
	keywords = {/unread},
}

@misc{zhu_x-nerf_2022,
	title = {X-{NeRF}: {Explicit} {Neural} {Radiance} {Field} for {Multi}-{Scene} 360\${\textasciicircum}\{{\textbackslash}circ\} \$ {Insufficient} {RGB}-{D} {Views}},
	shorttitle = {X-{NeRF}},
	url = {http://arxiv.org/abs/2210.05135},
	doi = {10.48550/arXiv.2210.05135},
	abstract = {Neural Radiance Fields (NeRFs), despite their outstanding performance on novel view synthesis, often need dense input views. Many papers train one model for each scene respectively and few of them explore incorporating multi-modal data into this problem. In this paper, we focus on a rarely discussed but important setting: can we train one model that can represent multiple scenes, with 360\${\textasciicircum}{\textbackslash}circ \$ insufficient views and RGB-D images? We refer insufficient views to few extremely sparse and almost non-overlapping views. To deal with it, X-NeRF, a fully explicit approach which learns a general scene completion process instead of a coordinate-based mapping, is proposed. Given a few insufficient RGB-D input views, X-NeRF first transforms them to a sparse point cloud tensor and then applies a 3D sparse generative Convolutional Neural Network (CNN) to complete it to an explicit radiance field whose volumetric rendering can be conducted fast without running networks during inference. To avoid overfitting, besides common rendering loss, we apply perceptual loss as well as view augmentation through random rotation on point clouds. The proposed methodology significantly out-performs previous implicit methods in our setting, indicating the great potential of proposed problem and approach. Codes and data are available at https://github.com/HaoyiZhu/XNeRF.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Zhu, Haoyi and Fang, Hao-Shu and Lu, Cewu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05135 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@book{__nodate,
	title = {矩阵力量},
	author = {生姜},
	keywords = {/unread},
}

@article{wang_pet-neus_2023,
	title = {{PET}-{NeuS}: {Positional} {Encoding} {Triplanes} for {Neural} {Surfaces}},
	shorttitle = {{PET}-{NeuS}},
	url = {https://openreview.net/forum?id=MHXO5xRCSXh},
	abstract = {The signed distance function (SDF) represented by an MLP network is commonly used for multi-view neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the Tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Discretizing the scene space with Tri-planes leads to a more expressive data structure but involving tri-planes will introduce noise due to discrete discontinuities. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency bands and modulate them with sin and cos functions of different frequency. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 25{\textbackslash}\% on Nerf-synthetic (0.84 compared to 1.12) and by 14{\textbackslash}\% on DTU (0.75 compared to 0.87). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise.},
	language = {en},
	urldate = {2023-03-13},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	month = feb,
	year = {2023},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{jeong_perfception_2022,
	title = {{PeRFception}: {Perception} using {Radiance} {Fields}},
	shorttitle = {{PeRFception}},
	url = {https://openreview.net/forum?id=MzaPEKHv-0J},
	abstract = {The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale radiance fields datasets for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4{\textbackslash}\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the classification and segmentation models that directly take this radiance fields format as input and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in "https://postech-cvlab.github.io/PeRFception/".},
	language = {en},
	urldate = {2023-03-13},
	author = {Jeong, Yoonwoo and Shin, Seungjoo and Lee, Junha and Choy, Chris and Anandkumar, Anima and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2022},
	keywords = {/unread},
}

@article{zhang_efficient_2023,
	title = {Efficient {Large}-scale {Scene} {Representation} with a {Hybrid} of {High}-resolution {Grid} and {Plane} {Features}},
	url = {https://arxiv.org/abs/2303.03003v2},
	doi = {10.48550/arXiv.2303.03003},
	abstract = {Existing neural radiance fields (NeRF) methods for large-scale scene modeling require days of training using multiple GPUs, hindering their applications in scenarios with limited computing resources. Despite fast optimization NeRF variants have been proposed based on the explicit dense or hash grid features, their effectivenesses are mainly demonstrated in object-scale scene representation. In this paper, we point out that the low feature resolution in explicit representation is the bottleneck for large-scale unbounded scene representation. To address this problem, we introduce a new and efficient hybrid feature representation for NeRF that fuses the 3D hash-grids and high-resolution 2D dense plane features. Compared with the dense-grid representation, the resolution of a dense 2D plane can be scaled up more efficiently. Based on this hybrid representation, we propose a fast optimization NeRF variant, called GP-NeRF, that achieves better rendering results while maintaining a compact model size. Extensive experiments on multiple large-scale unbounded scene datasets show that our model can converge in 1.5 hours using a single GPU while achieving results comparable to or even better than the existing method that requires about one day's training with 8 GPUs.},
	language = {en},
	urldate = {2023-03-12},
	author = {Zhang, Yuqi and Chen, Guanying and Cui, Shuguang},
	month = mar,
	year = {2023},
	keywords = {/unread},
}

@inproceedings{zhang_digging_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Digging into {Radiance} {Grid} for {Real}-{Time} {View} {Synthesis} with {Detail} {Preservation}},
	isbn = {978-3-031-19784-0},
	doi = {10.1007/978-3-031-19784-0_42},
	abstract = {Neural Radiance Fields (NeRF) [31] series are impressive in representing scenes and synthesizing high-quality novel views. However, most previous works fail to preserve texture details and suffer from slow training speed. A recent method SNeRG [11] demonstrates that baking a trained NeRF as a Sparse Neural Radiance Grid enables real-time view synthesis with slight scarification of rendering quality. In this paper, we dig into the Radiance Grid representation and present a set of improvements, which together result in boosted performance in terms of both speed and quality. First, we propose an HieRarchical Sparse Radiance Grid (HrSRG) representation that has higher voxel resolution for informative spaces and fewer voxels for other spaces. HrSRG leverages a hierarchical voxel grid building process inspired by [30, 55], and can describe a scene at high resolution without excessive memory footprint. Furthermore, we show that directly optimizing the voxel grid leads to surprisingly good texture details in rendered images. This direct optimization is memory-friendly and requires multiple orders of magnitude less time than conventional NeRFs as it only involves a tiny MLP. Finally, we find that a critical factor that prevents fine details restoration is the misaligned 2D pixels among images caused by camera pose errors. We propose to use the perceptual loss to add tolerance to misalignments, leading to the improved visual quality of rendered images.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Jian and Huang, Jinchi and Cai, Bowen and Fu, Huan and Gong, Mingming and Wang, Chaohui and Wang, Jiaming and Luo, Hongchen and Jia, Rongfei and Zhao, Binqiang and Tang, Xing},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, 3D representation, Real-time rendering, View synthesis},
	pages = {724--740},
}

@inproceedings{lindell_bacon_2022,
	title = {{BACON}: {Band}-{Limited} {Coordinate} {Networks} for {Multiscale} {Scene} {Representation}},
	shorttitle = {{BACON}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-12},
	author = {Lindell, David B. and Van Veen, Dave and Park, Jeong Joon and Wetzstein, Gordon},
	year = {2022},
	keywords = {/unread},
	pages = {16252--16262},
}

@inproceedings{kundu_panoptic_2022,
	title = {Panoptic {Neural} {Fields}: {A} {Semantic} {Object}-{Aware} {Neural} {Scene} {Representation}},
	shorttitle = {Panoptic {Neural} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-12},
	author = {Kundu, Abhijit and Genova, Kyle and Yin, Xiaoqi and Fathi, Alireza and Pantofaru, Caroline and Guibas, Leonidas J. and Tagliasacchi, Andrea and Dellaert, Frank and Funkhouser, Thomas},
	year = {2022},
	keywords = {/unread},
	pages = {12871--12881},
}

@book{__nodate-1,
	title = {数学要素},
	url = {https://github.com/Visualize-ML/Book3_Elements-of-Mathematics},
	author = {生姜, DrGinger},
	keywords = {/unread},
}

@incollection{noauthor_notitle_nodate,
	keywords = {/unread},
}

@misc{chen_local--global_2023,
	title = {Local-to-{Global} {Registration} for {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2211.11505},
	doi = {10.48550/arXiv.2211.11505},
	abstract = {Neural Radiance Fields (NeRF) have achieved photorealistic novel views synthesis; however, the requirement of accurate camera poses limits its application. Despite analysis-by-synthesis extensions for jointly learning neural 3D representations and registering camera frames exist, they are susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF, a Local-to-Global registration method for bundle-adjusting Neural Radiance Fields: first, a pixel-wise flexible alignment, followed by a frame-wise constrained parametric alignment. Pixel-wise local alignment is learned in an unsupervised way via a deep network which optimizes photometric reconstruction errors. Frame-wise global alignment is performed using differentiable parameter estimation solvers on the pixel-wise correspondences to find a global transformation. Experiments on synthetic and real-world data show that our method outperforms the current state-of-the-art in terms of high-fidelity reconstruction and resolving large camera pose misalignment. Our module is an easy-to-use plugin that can be applied to NeRF variants and other neural field applications. The Code and supplementary materials are available at https://rover-xingyu.github.io/L2G-NeRF/.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Chen, Yue and Chen, Xingyu and Wang, Xuan and Zhang, Qi and Guo, Yu and Shan, Ying and Wang, Fei},
	month = mar,
	year = {2023},
	note = {arXiv:2211.11505 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{guo_vid2avatar_2023,
	title = {{Vid2Avatar}: {3D} {Avatar} {Reconstruction} from {Videos} in the {Wild} via {Self}-supervised {Scene} {Decomposition}},
	shorttitle = {{Vid2Avatar}},
	url = {http://arxiv.org/abs/2302.11566},
	abstract = {We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Guo, Chen and Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11566 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{song_rethinking_2022,
	title = {Rethinking {Performance} {Gains} in {Image} {Dehazing} {Networks}},
	url = {http://arxiv.org/abs/2209.11448},
	abstract = {Image dehazing is an active topic in low-level vision, and many image dehazing networks have been proposed with the rapid development of deep learning. Although these networks' pipelines work fine, the key mechanism to improving image dehazing performance remains unclear. For this reason, we do not target to propose a dehazing network with fancy modules; rather, we make minimal modifications to popular U-Net to obtain a compact dehazing network. Specifically, we swap out the convolutional blocks in U-Net for residual blocks with the gating mechanism, fuse the feature maps of main paths and skip connections using the selective kernel, and call the resulting U-Net variant gUNet. As a result, with a significantly reduced overhead, gUNet is superior to state-of-the-art methods on multiple image dehazing datasets. Finally, we verify these key designs to the performance gain of image dehazing networks through extensive ablation studies.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Song, Yuda and Zhou, Yang and Qian, Hui and Du, Xin},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11448 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{song_deep_2019,
	title = {Deep network for simultaneous stereo matching and dehazing},
	shorttitle = {Deep network for simultaneous stereo matching and dehazing},
	url = {http://www.scopus.com/inward/record.url?scp=85072312755&partnerID=8YFLogxK},
	abstract = {Unveiling the image structure and dense correspondence under the haze layer remains a challenging task, since the scattering effects cause image features to be less distinctive. In this paper, we introduce a deep network that simultaneously estimates a clear latent image and disparity from a hazy stereo image pair. To this end, inspired by a physical model of hazy image acquisition, we propose a dehazing loss function which serves as an additional cue for establishing dense correspondence. We show that stereo matching and dehazing can be synergistically formulated by incorporating depth information from haze transmission into the stereo matching process, and vice versa. As a result, our method estimates high-quality disparity for scenes in scattering media, and produces appearance images with enhanced visibility. We quantitatively evaluate the proposed method on synthetic datasets and provide an extensive ablation study. Experimental results demonstrate that our approach outperforms the recent state-of-the-art methods on both dehazing and stereo matching tasks.},
	urldate = {2023-03-07},
	author = {Song, Taeyong and Kim, Youngjung and Oh, Changjae and Sohn, Kwanghoon},
	month = jan,
	year = {2019},
	keywords = {/unread},
}

@inproceedings{wang_programmable_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Programmable {Triangulation} {Light} {Curtains}},
	isbn = {978-3-030-01219-9},
	doi = {10.1007/978-3-030-01219-9_2},
	abstract = {A vehicle on a road or a robot in the field does not need a full-featured 3D depth sensor to detect potential collisions or monitor its blind spot. Instead, it needs to only monitor if any object comes within its near proximity which is an easier task than full depth scanning. We introduce a novel device that monitors the presence of objects on a virtual shell near the device, which we refer to as a light curtain. Light curtains offer a light-weight, resource-efficient and programmable approach to proximity awareness for obstacle avoidance and navigation. They also have additional benefits in terms of improving visibility in fog as well as flexibility in handling light fall-off. Our prototype for generating light curtains works by rapidly rotating a line sensor and a line laser, in synchrony. The device is capable of generating light curtains of various shapes with a range of 20–30 m in sunlight (40 m under cloudy skies and 50 m indoors) and adapts dynamically to the demands of the task. We analyze properties of light curtains and various approaches to optimize their thickness as well as power requirements. We showcase the potential of light curtains using a range of real-world scenarios.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wang, Jian and Bartels, Joseph and Whittaker, William and Sankaranarayanan, Aswin C. and Narasimhan, Srinivasa G.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {/unread, Computational imaging, Proximity sensors},
	pages = {20--35},
}

@article{song_simultaneous_2020,
	title = {Simultaneous {Deep} {Stereo} {Matching} and {Dehazing} with {Feature} {Attention}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01294-2},
	doi = {10.1007/s11263-020-01294-2},
	abstract = {Unveiling the dense correspondence under the haze layer remains a challenging task, since the scattering effects result in less distinctive image features. Contrarily, dehazing is often confused by the airlight-albedo ambiguity which cannot be resolved independently at each pixel. In this paper, we introduce a deep convolutional neural network that simultaneously estimates a disparity and clear image from a hazy stereo image pair. Both tasks are synergistically formulated by fusing depth information from the matching cost and haze transmission. To learn the optimal fusion of depth-related features, we present a novel encoder-decoder architecture that extends the core idea of attention mechanism to the simultaneous stereo matching and dehazing. As a result, our method estimates high-quality disparity for the stereo images in scattering media, and produces appearance images with enhanced visibility. Finally, we further propose an effective strategy for adaptation to camera-captured images by distilling the cross-domain knowledge. Experiments on both synthetic and real-world scenarios including comparisons with state-of-the-art methods demonstrate the effectiveness and flexibility of our approach.},
	language = {en},
	number = {4},
	urldate = {2023-03-07},
	journal = {International Journal of Computer Vision},
	author = {Song, Taeyong and Kim, Youngjung and Oh, Changjae and Jang, Hyunsung and Ha, Namkoo and Sohn, Kwanghoon},
	month = apr,
	year = {2020},
	keywords = {/unread},
	pages = {799--817},
}

@inproceedings{satat_towards_2018,
	title = {Towards photography through realistic fog},
	doi = {10.1109/ICCPHOT.2018.8368463},
	abstract = {Imaging through fog has important applications in industries such as self-driving cars, augmented driving, airplanes, helicopters, drones and trains. Here we show that time profiles of light reflected from fog have a distribution (Gamma) that is different from light reflected from objects occluded by fog (Gaussian). This helps to distinguish between background photons reflected from the fog and signal photons reflected from the occluded object. Based on this observation, we recover reflectance and depth of a scene obstructed by dense, dynamic, and heterogeneous fog. For practical use cases, the imaging system is designed in optical reflection mode with minimal footprint and is based on LIDAR hardware. Specifically, we use a single photon avalanche diode (SPAD) camera that time-tags individual detected photons. A probabilistic computational framework is developed to estimate the fog properties from the measurement itself, without prior knowledge. Other solutions are based on radar that suffers from poor resolution (due to the long wavelength), or on time gating that suffers from low signal-to-noise ratio. The suggested technique is experimentally evaluated in a wide range of fog densities created in a fog chamber It demonstrates recovering objects 57cm away from the camera when the visibility is 37cm. In that case it recovers depth with a resolution of 5cm and scene reflectance with an improvement of 4dB in PSNR and 3.4× reconstruction quality in SSIM over time gating techniques.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	author = {Satat, Guy and Tancik, Matthew and Raskar, Ramesh},
	month = may,
	year = {2018},
	note = {ISSN: 2472-7636},
	keywords = {/unread, Cameras, Estimation, Media, Photonics, Scattering, Signal to noise ratio},
	pages = {1--10},
}

@article{heide_imaging_2014,
	title = {Imaging in scattering media using correlation image sensors and sparse convolutional coding},
	volume = {22},
	copyright = {© 2014 Optical Society of America},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-22-21-26338},
	doi = {10.1364/OE.22.026338},
	abstract = {Correlation image sensors have recently become popular low-cost devices for time-of-flight, or range cameras. They usually operate under the assumption of a single light path contributing to each pixel. We show that a more thorough analysis of the sensor data from correlation sensors can be used can be used to analyze the light transport in much more complex environments, including applications for imaging through scattering and turbid media. The key of our method is a new convolutional sparse coding approach for recovering transient (light-in-flight) images from correlation image sensors. This approach is enabled by an analysis of sparsity in complex transient images, and the derivation of a new physically-motivated model for transient images with drastically improved sparsity.},
	language = {EN},
	number = {21},
	urldate = {2023-03-07},
	journal = {Optics Express},
	author = {Heide, Felix and Xiao, Lei and Kolb, Andreas and Hullin, Matthias B. and Heidrich, Wolfgang},
	month = oct,
	year = {2014},
	note = {Publisher: Optica Publishing Group},
	keywords = {/unread, Femtosecond pulses, Gated imaging, Image sensors, Scattering media, Streak cameras, Underwater imaging},
	pages = {26338--26350},
}

@inproceedings{fujimura_photometric_2018,
	title = {Photometric {Stereo} in {Participating} {Media} {Considering} {Shape}-{Dependent} {Forward} {Scatter}},
	doi = {10.1109/CVPR.2018.00777},
	abstract = {Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Fujimura, Yuki and Iiyama, Masaaki and Hashimoto, Atsushi and Minoh, Michihiko},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Backscatter, Cameras, Image reconstruction, Media, Scattering, Shape, Three-dimensional displays},
	pages = {7445--7453},
}

@article{murez_photometric_2017,
	title = {Photometric {Stereo} in a {Scattering} {Medium}},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2613862},
	abstract = {Photometric stereo is widely used for 3D reconstruction. However, its use in scattering media such as water, biological tissue and fog has been limited until now, because of forward scattered light from both the source and object, as well as light scattered back from the medium (backscatter). Here we make three contributions to address the key modes of light propagation, under the common single scattering assumption for dilute media. First, we show through extensive simulations that single-scattered light from a source can be approximated by a point light source with a single direction. This alleviates the need to handle light source blur explicitly. Next, we model the blur due to scattering of light from the object. We measure the object point-spread function and introduce a simple deconvolution method. Finally, we show how imaging fluorescence emission where available, eliminates the backscatter component and increases the signal-to-noise ratio. Experimental results in a water tank, with different concentrations of scattering media added, show that deconvolution produces higher-quality 3D reconstructions than previous techniques, and that when combined with fluorescence, can produce results similar to that in clear water even for highly turbid media.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Murez, Zak and Treibitz, Tali and Ramamoorthi, Ravi and Kriegman, David J.},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {/unread, Backscatter, Cameras, Light sources, Media, Photometric stereo, Scattering, Surface reconstruction, Three-dimensional displays, fluorescence, scattering medium},
	pages = {1880--1891},
}

@inproceedings{tsiotsios_backscatter_2014,
	title = {Backscatter {Compensated} {Photometric} {Stereo} with 3 {Sources}},
	doi = {10.1109/CVPR.2014.289},
	abstract = {Photometric stereo offers the possibility of object shape reconstruction via reasoning about the amount of light reflected from oriented surfaces. However, in murky media such as sea water, the illuminating light interacts with the medium and some of it is backscattered towards the camera. Due to this additive light component, the standard Photometric Stereo equations lead to poor quality shape estimation. Previous authors have attempted to reformulate the approach but have either neglected backscatter entirely or disregarded its non-uniformity on the sensor when camera and lights are close to each other. We show that by compensating effectively for the backscatter component, a linear formulation of Photometric Stereo is allowed which recovers an accurate normal map using only 3 lights. Our backscatter compensation method for point-sources can be used for estimating the uneven backscatter directly from single images without any prior knowledge about the characteristics of the medium or the scene. We compare our method with previous approaches through extensive experimental results, where a variety of objects are imaged in a big water tank whose turbidity is systematically increased, and show reconstruction quality which degrades little relative to clean water results even with a very significant scattering level.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Tsiotsios, Chourmouzios and Angelopoulou, Maria E. and Kim, Tae-Kyun and Davison, Andrew J.},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Backscatter, Brightness, Cameras, Lighting, Mathematical model, Scattering, Shape, backscatter, photometric stereo, restoration, scattering, underwater vision},
	pages = {2259--2266},
}

@inproceedings{narasimhan_structured_2005,
	title = {Structured light in scattering media},
	volume = {1},
	doi = {10.1109/ICCV.2005.232},
	abstract = {Virtually all structured light methods assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.},
	booktitle = {Tenth {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}'05) {Volume} 1},
	author = {Narasimhan, S.G. and Nayar, S.K. and Sun, Bo and Koppal, S.J.},
	month = oct,
	year = {2005},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Computer science, Layout, Light scattering, Optical attenuators, Optical imaging, Optical scattering, Photometry, Robots, Sun, Surface reconstruction},
	pages = {420--427 Vol. 1},
}

@inproceedings{caraffa_stereo_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stereo {Reconstruction} and {Contrast} {Restoration} in {Daytime} {Fog}},
	isbn = {978-3-642-37447-0},
	doi = {10.1007/978-3-642-37447-0_2},
	abstract = {Stereo reconstruction serves many outdoor applications, and thus sometimes faces foggy weather. The quality of the reconstruction by state of the art algorithms is then degraded as contrast is reduced with the distance because of scattering. However, as shown by defogging algorithms from a single image, fog provides an extra depth cue in the gray level of far away objects. Our idea is thus to take advantage of both stereo and atmospheric veil depth cues to achieve better stereo reconstructions in foggy weather. To our knowledge, this subject has never been investigated earlier by the computer vision community. We thus propose a Markov Random Field model of the stereo reconstruction and defogging problem which can be optimized iteratively using the α-expansion algorithm. Outputs are a dense disparity map and an image where contrast is restored. The proposed model is evaluated on synthetic images. This evaluation shows that the proposed method achieves very good results on both stereo reconstruction and defogging compared to standard stereo reconstruction and single image defogging.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer},
	author = {Caraffa, Laurent and Tarel, Jean-Philippe},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	keywords = {/unread, Data Stereo, Markov Random Field, Markov Random Field Model, Remote Object, Stereo Pair},
	pages = {13--25},
}

@article{wang_eaa-net_2021,
	title = {{EAA}-{Net}: {A} novel edge assisted attention network for single image dehazing},
	volume = {228},
	issn = {0950-7051},
	shorttitle = {{EAA}-{Net}},
	url = {https://doi.org/10.1016/j.knosys.2021.107279},
	doi = {10.1016/j.knosys.2021.107279},
	abstract = {Traditional dehazing convolutional neural networks (CNNs) learn the feature maps only from hazy images to the corresponding hazy-free ones, which usually lead to some important feature loss like texture information. The paper proposes an effective edge assisted attention network (EAA-Net) for single image haze removal, aiming to preserve the texture information and improve the overall quality of the dehazed outcomes. The proposed EAA-Net mainly consists of three parts, i.e., a dehaze branch (DB), an edge branch (EB), and a feature fusion residual block (FFRB). In order to solve the loss of the texture information during the dehazing process, the forward information of the EB is concatenated with the DB’s information, and then the fusion result is passed to the FFRB. In addition, the multilevel information boost module is employed into the dehaze branch as a dehaze unit, and it can enhance the multilevel feature information for a better dehazing effect. Extensive experiments conducted on benchmark datasets demonstrate that the proposed EAA-Net performs favorably against some recent state-of-the-art approaches, which includes a better capacity of detail and color maintaining, and competitive consequences both quantitatively and qualitatively.},
	number = {C},
	urldate = {2023-03-07},
	journal = {Knowledge-Based Systems},
	author = {Wang, Chao and Shen, Hao-Zhen and Fan, Fan and Shao, Ming-Wen and Yang, Chuan-Sheng and Luo, Jian-Cheng and Deng, Liang-Jian},
	year = {2021},
	keywords = {/unread, Attention network, CNN, Edge assisted, Single image dehazing},
}

@inproceedings{deng_hardgan_2020,
	title = {{HardGAN}: {A} {Haze}-{Aware} {Representation} {Distillation} {GAN} for {Single} {Image} {Dehazing}},
	shorttitle = {{HardGAN}},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-58539-6_43},
	doi = {10.1007/978-3-030-58539-6_43},
	abstract = {In this paper, we present a Haze-Aware Representation Distillation Generative Adversarial Network (HardGAN) for single-image dehazing. Unlike previous studies that intend to model the transmission map and global atmospheric light jointly to restore a clear...},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer, Cham},
	author = {Deng, Qili and Huang, Ziling and Tsai, Chung-Chi and Lin, Chia-Wen},
	year = {2020},
	keywords = {/unread},
	pages = {722--738},
}

@inproceedings{dong_physics-based_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Physics-{Based} {Feature} {Dehazing} {Networks}},
	isbn = {978-3-030-58577-8},
	doi = {10.1007/978-3-030-58577-8_12},
	abstract = {We propose a physics-based feature dehazing network for image dehazing. In contrast to most existing end-to-end trainable network-based dehazing methods, we explicitly consider the physics model of the haze process in the network design and remove haze in a deep feature space. We propose an effective feature dehazing unit (FDU), which is applied to the deep feature space to explore useful features for image dehazing based on the physics model. The FDU is embedded into an encoder and decoder architecture with residual learning, so that the proposed network can be trained in an end-to-end fashion and effectively help haze removal. The encoder and decoder modules are adopted for feature extraction and clear image reconstruction, respectively. The residual learning is applied to increase the accuracy and ease the training of deep neural networks. We analyze the effectiveness of the proposed network and demonstrate that it can effectively dehaze images with favorable performance against state-of-the-art methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Dong, Jiangxin and Pan, Jinshan},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread, Deep convolutional neural networks, Feature dehazing unit, Image dehazing, Physics model},
	pages = {188--204},
}

@inproceedings{chen_gated_2019,
	title = {Gated {Context} {Aggregation} {Network} for {Image} {Dehazing} and {Deraining}},
	doi = {10.1109/WACV.2019.00151},
	abstract = {Image dehazing aims to recover the uncorrupted content from a hazy image. Instead of leveraging traditional low-level or handcrafted image priors as the restoration constraints, e.g., dark channels and increased contrast, we propose an end-to-end gated context aggregation network to directly restore the final haze-free image. In this network, we adopt the latest smoothed dilation technique to help remove the gridding artifacts caused by the widely-used dilated convolution with negligible extra parameters, and leverage a gated sub-network to fuse the features from different levels. Extensive experiments demonstrate that our method can surpass previous state-of-the-art methods by a large margin both quantitatively and qualitatively. In addition, to demonstrate the generality of the proposed method, we further apply it to the image deraining task, which also achieves the state-of-the-art performance.},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chen, Dongdong and He, Mingming and Fan, Qingnan and Liao, Jing and Zhang, Liheng and Hou, Dongdong and Yuan, Lu and Hua, Gang},
	month = jan,
	year = {2019},
	note = {ISSN: 1550-5790},
	keywords = {/unread, Atmospheric modeling, Convolution, Fuses, Image restoration, Logic gates, Mathematical model, Task analysis},
	pages = {1375--1383},
}

@article{song_vision_2022,
	title = {Vision {Transformers} for {Single} {Image} {Dehazing}},
	abstract = {Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25\% \#Param and 5\% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous state-of-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method's capability to remove highly non-homogeneous haze.},
	language = {en},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Image Processing},
	author = {Song, Yuda and He, Zhuqing and Qian, Hui and Du, Xin},
	month = apr,
	year = {2022},
	keywords = {/unread},
}

@inproceedings{zhang_unreasonable_2018,
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	doi = {10.1109/CVPR.2018.00068},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer architecture, Distortion, Measurement, Network architecture, Task analysis, Training, Visualization},
	pages = {586--595},
}

@inproceedings{pearl_nan_2022,
	title = {{NAN}: {Noise}-{Aware} {NeRFs} for {Burst}-{Denoising}},
	shorttitle = {{NAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Pearl_NAN_Noise-Aware_NeRFs_for_Burst-Denoising_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-06},
	author = {Pearl, Naama and Treibitz, Tali and Korman, Simon},
	year = {2022},
	keywords = {/unread},
	pages = {12672--12681},
}

@article{zhao_loss_2017,
	title = {Loss {Functions} for {Image} {Restoration} {With} {Neural} {Networks}},
	volume = {3},
	issn = {2333-9403},
	doi = {10.1109/TCI.2016.2644865},
	abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is ℓ2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
	number = {1},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
	month = mar,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Computational Imaging},
	keywords = {/unread, Image processing, Image quality, Image restoration, Measurement, Neural networks, image restoration, loss functions, neural networks},
	pages = {47--57},
}

@inproceedings{dong_multi-scale_2020,
	title = {Multi-{Scale} {Boosted} {Dehazing} {Network} {With} {Dense} {Feature} {Fusion}},
	doi = {10.1109/CVPR42600.2020.00223},
	abstract = {In this paper, we propose a Multi-Scale Boosted Dehazing Network with Dense Feature Fusion based on the U-Net architecture. The proposed method is designed based on two principles, boosting and error feedback, and we show that they are suitable for the dehazing problem. By incorporating the Strengthen-Operate-Subtract boosting strategy in the decoder of the proposed model, we develop a simple yet effective boosted decoder to progressively restore the haze-free image. To address the issue of preserving spatial information in the U-Net architecture, we design a dense feature fusion module using the back-projection feedback scheme. We show that the dense feature fusion module can simultaneously remedy the missing spatial information from high-resolution features and exploit the non-adjacent features. Extensive evaluations demonstrate that the proposed model performs favorably against the state-of-the-art approaches on the benchmark datasets as well as real-world hazy images.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Dong, Hang and Pan, Jinshan and Xiang, Lei and Hu, Zhe and Zhang, Xinyi and Wang, Fei and Yang, Ming-Hsuan},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Boosting, Decoding, Feature extraction, Fuses, Image restoration, Task analysis},
	pages = {2154--2164},
}

@inproceedings{qu_enhanced_2019,
	title = {Enhanced {Pix2pix} {Dehazing} {Network}},
	doi = {10.1109/CVPR.2019.00835},
	abstract = {In this paper, we reduce the image dehazing problem to an image-to-image translation problem, and propose Enhanced Pix2pix Dehazing Network (EPDN), which generates a haze-free image without relying on the physical scattering model. EPDN is embedded by a generative adversarial network, which is followed by a well-designed enhancer. Inspired by visual perception global-first theory, the discriminator guides the generator to create a pseudo realistic image on a coarse scale, while the enhancer following the generator is required to produce a realistic dehazing image on the fine scale. The enhancer contains two enhancing blocks based on the receptive field model, which reinforces the dehazing effect in both color and details. The embedded GAN is jointly trained with the enhancer. Extensive experiment results on synthetic datasets and real-world datasets show that the proposed EPDN is superior to the state-of-the-art methods in terms of PSNR, SSIM, PI, and subjective visual effect.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qu, Yanyun and Chen, Yizi and Huang, Jingying and Xie, Yuan},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Low-level Vision},
	pages = {8152--8160},
}

@article{qin_ffa-net_2020,
	title = {{FFA}-{Net}: {Feature} {Fusion} {Attention} {Network} for {Single} {Image} {Dehazing}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{FFA}-{Net}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6865},
	doi = {10.1609/aaai.v34i07.6865},
	abstract = {In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components:1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers.The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23 dB to 36.39 dB on the SOTS indoor test dataset. Code has been made available at GitHub.},
	language = {en},
	number = {07},
	urldate = {2023-03-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Qin, Xu and Wang, Zhilin and Bai, Yuanchao and Xie, Xiaodong and Jia, Huizhu},
	month = apr,
	year = {2020},
	note = {Number: 07},
	keywords = {/unread},
	pages = {11908--11915},
}

@inproceedings{liu_griddehazenet_2019,
	title = {{GridDehazeNet}: {Attention}-{Based} {Multi}-{Scale} {Network} for {Image} {Dehazing}},
	shorttitle = {{GridDehazeNet}},
	doi = {10.1109/ICCV.2019.00741},
	abstract = {We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Xiaohong and Ma, Yongrui and Shi, Zhihao and Chen, Jun},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Atmospheric modeling, Data models, Estimation, Image color analysis, Image restoration, Scattering},
	pages = {7313--7322},
}

@inproceedings{ren_gated_2018,
	title = {Gated {Fusion} {Network} for {Single} {Image} {Dehazing}},
	doi = {10.1109/CVPR.2018.00343},
	abstract = {In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale approach such that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ren, Wenqi and Ma, Lin and Zhang, Jiawei and Pan, Jinshan and Cao, Xiaochun and Liu, Wei and Yang, Ming-Hsuan},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Attenuation, Decoding, Estimation, Image color analysis, Image restoration, Logic gates},
	pages = {3253--3261},
}

@inproceedings{zhang_densely_2018,
	title = {Densely {Connected} {Pyramid} {Dehazing} {Network}},
	doi = {10.1109/CVPR.2018.00337},
	abstract = {We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, He and Patel, Vishal M.},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Degradation, Estimation, Gallium nitride, Generative adversarial networks, Image edge detection, Optimization},
	pages = {3194--3203},
}

@inproceedings{li_aod-net_2017,
	title = {{AOD}-{Net}: {All}-in-{One} {Dehazing} {Network}},
	shorttitle = {{AOD}-{Net}},
	doi = {10.1109/ICCV.2017.511},
	abstract = {This paper proposes an image dehazing model built with a convolutional neural network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed based on a re-formulated atmospheric scattering model. Instead of estimating the transmission matrix and the atmospheric light separately as most previous models did, AOD-Net directly generates the clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models, e.g., Faster R-CNN, for improving high-level tasks on hazy images. Experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of PSNR, SSIM and the subjective visual quality. Furthermore, when concatenating AOD-Net with Faster R-CNN, we witness a large improvement of the object detection performance on hazy images.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Li, Boyi and Peng, Xiulian and Wang, Zhangyang and Xu, Jizheng and Feng, Dan},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Atmospheric modeling, Computational modeling, Estimation, Image restoration, Scattering, Visualization},
	pages = {4780--4788},
}

@article{ren_single_2020,
	title = {Single {Image} {Dehazing} via {Multi}-scale {Convolutional} {Neural} {Networks} with {Holistic} {Edges}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01235-8},
	doi = {10.1007/s11263-019-01235-8},
	abstract = {Single image dehazing has been a challenging problem which aims to recover clear images from hazy ones. The performance of existing image dehazing methods is limited by hand-designed features and priors. In this paper, we propose a multi-scale deep neural network for single image dehazing by learning the mapping between hazy images and their transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines dehazed results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. In addition, we propose a holistic edge guided network to refine edges of the estimated transmission map. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Ren, Wenqi and Pan, Jinshan and Zhang, Hua and Cao, Xiaochun and Yang, Ming-Hsuan},
	month = jan,
	year = {2020},
	keywords = {/unread},
	pages = {240--259},
}

@article{cai_dehazenet_2016,
	title = {{DehazeNet}: {An} {End}-to-{End} {System} for {Single} {Image} {Haze} {Removal}},
	volume = {25},
	issn = {1941-0042},
	shorttitle = {{DehazeNet}},
	doi = {10.1109/TIP.2016.2598681},
	abstract = {Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, the layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called bilateral rectified linear unit, which is able to improve the quality of recovered haze-free image. We establish connections between the components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.},
	number = {11},
	journal = {IEEE Transactions on Image Processing},
	author = {Cai, Bolun and Xu, Xiangmin and Jia, Kui and Qing, Chunmei and Tao, Dacheng},
	month = nov,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {/unread, Atmospheric modeling, Attenuation, BReLU, Computational modeling, Dehaze, Estimation, Image color analysis, Image restoration, Scattering, deep CNN, image restoration},
	pages = {5187--5198},
}

@article{cantor_optics_1978,
	title = {Optics of the atmosphere–{Scattering} by molecules and particles},
	volume = {14},
	issn = {1558-1713},
	doi = {10.1109/JQE.1978.1069864},
	number = {9},
	journal = {IEEE Journal of Quantum Electronics},
	author = {Cantor, A.},
	month = sep,
	year = {1978},
	note = {Conference Name: IEEE Journal of Quantum Electronics},
	keywords = {/unread, Books, Equations, Meteorology, Mie scattering, Optical refraction, Optical scattering, Optical variables control, Particle scattering, Rayleigh scattering, Stimulated emission},
	pages = {698--699},
}

@inproceedings{berman_non-local_2016,
	title = {Non-local {Image} {Dehazing}},
	doi = {10.1109/CVPR.2016.185},
	abstract = {Haze limits visibility and reduces image contrast in outdoor images. The degradation is different for every pixel and depends on the distance of the scene point from the camera. This dependency is expressed in the transmission coefficients, that control the scene attenuation and amount of haze in every pixel. Previous methods solve the single image dehazing problem using various patch-based priors. We, on the other hand, propose an algorithm based on a new, non-local prior. The algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, that form tight clusters in RGB space. Our key observation is that pixels in a given cluster are often non-local, i.e., they are spread over the entire image plane and are located at different distances from the camera. In the presence of haze these varying distances translate to different transmission coefficients. Therefore, each color cluster in the clear image becomes a line in RGB space, that we term a haze-line. Using these haze-lines, our algorithm recovers both the distance map and the haze-free image. The algorithm is linear in the size of the image, deterministic and requires no training. It performs well on a wide variety of images and is competitive with other stateof-the-art methods.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Berman, Dana and Treibitz, Tali and Avidan, Shai},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Atmospheric modeling, Cameras, Clustering algorithms, Histograms, Image color analysis, Image segmentation, Quantization (signal)},
	pages = {1674--1682},
}

@article{fattal_dehazing_2015,
	title = {Dehazing {Using} {Color}-{Lines}},
	volume = {34},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2651362},
	doi = {10.1145/2651362},
	abstract = {Photographs of hazy scenes typically suffer having low contrast and offer a limited visibility of the scene. This article describes a new method for single-image dehazing that relies on a generic regularity in natural images where pixels of small image patches typically exhibit a 1D distribution in RGB color space, known as color-lines. We derive a local formation model that explains the color-lines in the context of hazy scenes and use it for recovering the scene transmission based on the lines' offset from the origin. The lack of a dominant color-line inside a patch or its lack of consistency with the formation model allows us to identify and avoid false predictions. Thus, unlike existing approaches that follow their assumptions across the entire image, our algorithm validates its hypotheses and obtains more reliable estimates where possible. In addition, we describe a Markov random field model dedicated to producing complete and regularized transmission maps given noisy and scattered estimates. Unlike traditional field models that consist of local coupling, the new model is augmented with long-range connections between pixels of similar attributes. These connections allow our algorithm to properly resolve the transmission in isolated regions where nearby pixels do not offer relevant information. An extensive evaluation of our method over different types of images and its comparison to state-of-the-art methods over established benchmark images show a consistent improvement in the accuracy of the estimated scene transmission and recovered haze-free radiances.},
	number = {1},
	urldate = {2023-03-06},
	journal = {ACM Transactions on Graphics},
	author = {Fattal, Raanan},
	year = {2015},
	keywords = {/unread, Image dehazing, contrast enhancement, transmission estimation},
	pages = {13:1--13:14},
}

@article{nishino_bayesian_2012,
	title = {Bayesian {Defogging}},
	volume = {98},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-011-0508-1},
	doi = {10.1007/s11263-011-0508-1},
	abstract = {Atmospheric conditions induced by suspended particles, such as fog and haze, severely alter the scene appearance. Restoring the true scene appearance from a single observation made in such bad weather conditions remains a challenging task due to the inherent ambiguity that arises in the image formation process. In this paper, we introduce a novel Bayesian probabilistic method that jointly estimates the scene albedo and depth from a single foggy image by fully leveraging their latent statistical structures. Our key idea is to model the image with a factorial Markov random field in which the scene albedo and depth are two statistically independent latent layers and to jointly estimate them. We show that we may exploit natural image and depth statistics as priors on these hidden layers and estimate the scene albedo and depth with a canonical expectation maximization algorithm with alternating minimization. We experimentally evaluate the effectiveness of our method on a number of synthetic and real foggy images. The results demonstrate that the method achieves accurate factorization even on challenging scenes for past methods that only constrain and estimate one of the latent variables.},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Nishino, Ko and Kratz, Louis and Lombardi, Stephen},
	month = jul,
	year = {2012},
	keywords = {/unread},
	pages = {263--278},
}

@inproceedings{li_simultaneous_2015,
	title = {Simultaneous video defogging and stereo reconstruction},
	doi = {10.1109/CVPR.2015.7299133},
	abstract = {We present a method to jointly estimate scene depth and recover the clear latent image from a foggy video sequence. In our formulation, the depth cues from stereo matching and fog information reinforce each other, and produce superior results than conventional stereo or defogging algorithms. We first improve the photo-consistency term to explicitly model the appearance change due to the scattering effects. The prior matting Laplacian constraint on fog transmission imposes a detail-preserving smoothness constraint on the scene depth. We further enforce the ordering consistency between scene depth and fog transmission at neighboring points. These novel constraints are formulated together in an MRF framework, which is optimized iteratively by introducing auxiliary variables. The experiment results on real videos demonstrate the strength of our method.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Zhuwen and Tan, Ping and Tan, Robby T. and Zou, Danping and Zhou, Steven Zhiying and Cheong, Loong-Fah},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Cameras, Coherence, Image color analysis, Laplace equations, Mathematical model, Media},
	pages = {4988--4997},
}

@inproceedings{sun_direct_2022,
	title = {Direct {Voxel} {Grid} {Optimization}: {Super}-fast {Convergence} for {Radiance} {Fields} {Reconstruction}},
	shorttitle = {Direct {Voxel} {Grid} {Optimization}},
	doi = {10.1109/CVPR52688.2022.00538},
	abstract = {We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Done Reading, Geometry, Interpolation, Optimization, Pattern recognition, Solid modeling, Task analysis, Training},
	pages = {5449--5459},
}

@inproceedings{wu_contrastive_2021,
	title = {Contrastive {Learning} for {Compact} {Single} {Image} {Dehazing}},
	doi = {10.1109/CVPR46437.2021.01041},
	abstract = {Single image dehazing is a challenging ill-posed problem due to the severe information degeneration. However, existing deep learning based dehazing methods only adopt clear images as positive samples to guide the training of dehazing network while negative information is unexploited. Moreover, most of them focus on strengthening the dehazing network with an increase of depth and width, leading to a significant requirement of computation and memory. In this paper, we propose a novel contrastive regularization (CR) built upon contrastive learning to exploit both the information of hazy images and clear images as negative and positive samples, respectively. CR ensures that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in the representation space.Furthermore, considering trade-off between performance and memory storage, we develop a compact dehazing network based on autoencoder-like (AE) framework. It involves an adaptive mixup operation and a dynamic feature enhancement module, which can benefit from preserving information flow adaptively and expanding the receptive field to improve the network’s transformation capability, respectively. We term our dehazing network with autoencoder and contrastive regularization as AECR-Net. The extensive experiments on synthetic and real-world datasets demonstrate that our AECR-Net surpass the state-of-the-art approaches. The code is released in https://github.com/GlassyWu/AECR-Net.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wu, Haiyan and Qu, Yanyun and Lin, Shaohui and Zhou, Jian and Qiao, Ruizhi and Zhang, Zhizhong and Xie, Yuan and Ma, Lizhuang},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Adaptive systems, Codes, Computer vision, Deep learning, Memory management, Performance gain, Training},
	pages = {10546--10555},
}

@article{fujimura_dehazing_2021,
	title = {Dehazing cost volume for deep multi-view stereo in scattering media with airlight and scattering coefficient estimation},
	volume = {211},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314221000977},
	doi = {10.1016/j.cviu.2021.103253},
	abstract = {We propose a learning-based multi-view stereo (MVS) method in scattering media, such as fog or smoke, with a novel cost volume, called the dehazing cost volume. Images captured in scattering media are degraded due to light scattering and attenuation caused by suspended particles. This degradation depends on scene depth; thus, it is difficult for traditional MVS methods to evaluate photometric consistency because the depth is unknown before three-dimensional (3D) reconstruction. The dehazing cost volume can solve this chicken-and-egg problem of depth estimation and image restoration by computing the scattering effect using swept planes in the cost volume. We also propose a method of estimating scattering parameters, such as airlight, and a scattering coefficient, which are required for our dehazing cost volume. The output depth of a network with our dehazing cost volume can be regarded as a function of these parameters; thus, they are geometrically optimized with a sparse 3D point cloud obtained at a structure-from-motion step. Experimental results on synthesized hazy images indicate the effectiveness of our dehazing cost volume against the ordinary cost volume regarding scattering media. We also demonstrated the applicability of our dehazing cost volume to real foggy scenes.},
	language = {en},
	urldate = {2023-03-04},
	journal = {Computer Vision and Image Understanding},
	author = {Fujimura, Yuki and Sonogashira, Motoharu and Iiyama, Masaaki},
	month = oct,
	year = {2021},
	keywords = {/unread, Airlight, Depth estimation, Multi-view stereo, Scattering coefficient, Scattering media},
	pages = {103253},
}

@inproceedings{NEURIPS2021_89fcd07f,
	title = {{DROID}-{SLAM}: {Deep} visual {SLAM} for monocular, stereo, and {RGB}-{D} cameras},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Teed, Zachary and Deng, Jia},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {16558--16569},
}

@inproceedings{teed_droid-slam_2021,
	title = {{DROID}-{SLAM}: {Deep} {Visual} {SLAM} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
	volume = {34},
	shorttitle = {{DROID}-{SLAM}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
	abstract = {We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.},
	urldate = {2023-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Teed, Zachary and Deng, Jia},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {16558--16569},
}

@inproceedings{teed_droid-slam_2021-1,
	title = {{DROID}-{SLAM}: {Deep} {Visual} {SLAM} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
	volume = {34},
	shorttitle = {{DROID}-{SLAM}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
	abstract = {We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.},
	urldate = {2023-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Teed, Zachary and Deng, Jia},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {16558--16569},
}

@misc{tancik_nerfstudio_2023,
	title = {Nerfstudio: {A} {Modular} {Framework} for {Neural} {Radiance} {Field} {Development}},
	shorttitle = {Nerfstudio},
	url = {http://arxiv.org/abs/2302.04264},
	doi = {10.48550/arXiv.2302.04264},
	abstract = {Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing at https://nerf.studio.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and McAllister, David and Kanazawa, Angjoo},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04264 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{xie_s-nerf_nodate,
	title = {S-{NeRF}: {Neural} {Radiance} {Fields} for {Street} {Views}},
	shorttitle = {S-{NeRF}},
	url = {https://ziyang-xie.github.io/s-nerf/},
	abstract = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.},
	urldate = {2023-03-02},
	author = {Xie, Ziyang and Zhang, Junzhe and Li, Wenye and Zhang, Feihu and Zhang, Li},
	keywords = {/unread},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14988 [cs, stat]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bhat_zoedepth_2023,
	title = {{ZoeDepth}: {Zero}-shot {Transfer} by {Combining} {Relative} and {Metric} {Depth}},
	shorttitle = {{ZoeDepth}},
	url = {http://arxiv.org/abs/2302.12288},
	doi = {10.48550/arXiv.2302.12288},
	abstract = {This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21\% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/ZoeDepth .},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12288 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{ye_decoupling_2023,
	title = {Decoupling {Human} and {Camera} {Motion} from {Videos} in the {Wild}},
	url = {https://arxiv.org/abs/2302.12827v1},
	doi = {10.48550/arXiv.2302.12827},
	abstract = {We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and video results can be found at https://vye16.github.io/slahmr.},
	language = {en},
	urldate = {2023-02-27},
	author = {Ye, Vickie and Pavlakos, Georgios and Malik, Jitendra and Kanazawa, Angjoo},
	month = feb,
	year = {2023},
	keywords = {/unread},
}

@misc{reiser_merf_2023,
	title = {{MERF}: {Memory}-{Efficient} {Radiance} {Fields} for {Real}-time {View} {Synthesis} in {Unbounded} {Scenes}},
	shorttitle = {{MERF}},
	url = {http://arxiv.org/abs/2302.12249},
	doi = {10.48550/arXiv.2302.12249},
	abstract = {Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Reiser, Christian and Szeliski, Richard and Verbin, Dor and Srinivasan, Pratul P. and Mildenhall, Ben and Geiger, Andreas and Barron, Jonathan T. and Hedman, Peter},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12249 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{seo_mixnerf_2023,
	title = {{MixNeRF}: {Modeling} a {Ray} with {Mixture} {Density} for {Novel} {View} {Synthesis} from {Sparse} {Inputs}},
	shorttitle = {{MixNeRF}},
	url = {http://arxiv.org/abs/2302.08788},
	doi = {10.48550/arXiv.2302.08788},
	abstract = {Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.},
	urldate = {2023-02-20},
	publisher = {arXiv},
	author = {Seo, Seunghyeon and Han, Donghoon and Chang, Yeonjin and Kwak, Nojun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08788 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yariv_volume_2021,
	title = {Volume {Rendering} of {Neural} {Implicit} {Surfaces}},
	url = {http://arxiv.org/abs/2106.12052},
	doi = {10.48550/arXiv.2106.12052},
	abstract = {Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Yariv, Lior and Gu, Jiatao and Kasten, Yoni and Lipman, Yaron},
	month = dec,
	year = {2021},
	note = {arXiv:2106.12052 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{weng_personnerf_2023,
	title = {{PersonNeRF}: {Personalized} {Reconstruction} from {Photo} {Collections}},
	shorttitle = {{PersonNeRF}},
	url = {http://arxiv.org/abs/2302.08504},
	doi = {10.48550/arXiv.2302.08504},
	abstract = {We present PersonNeRF, a method that takes a collection of photos of a subject (e.g. Roger Federer) captured across multiple years with arbitrary body poses and appearances, and enables rendering the subject with arbitrary novel combinations of viewpoint, body pose, and appearance. PersonNeRF builds a customized neural volumetric 3D model of the subject that is able to render an entire space spanned by camera viewpoint, body pose, and appearance. A central challenge in this task is dealing with sparse observations; a given body pose is likely only observed by a single viewpoint with a single appearance, and a given appearance is only observed under a handful of different body poses. We address this issue by recovering a canonical T-pose neural volumetric representation of the subject that allows for changing appearance across different observations, but uses a shared pose-dependent motion field across all observations. We demonstrate that this approach, along with regularization of the recovered volumetric geometry to encourage smoothness, is able to recover a model that renders compelling images from novel combinations of viewpoint, pose, and appearance from these challenging unstructured photo collections, outperforming prior work for free-viewpoint human rendering.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Weng, Chung-Yi and Srinivasan, Pratul P. and Curless, Brian and Kemelmacher-Shlizerman, Ira},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08504 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{kocaballi_conversational_2023,
	title = {Conversational {AI}-{Powered} {Design}: {ChatGPT} as {Designer}, {User}, and {Product}},
	shorttitle = {Conversational {AI}-{Powered} {Design}},
	url = {http://arxiv.org/abs/2302.07406},
	doi = {10.48550/arXiv.2302.07406},
	abstract = {The recent advancements in Large Language Models (LLMs), particularly conversational LLMs like ChatGPT, have prompted changes in a range of fields, including design. This study aims to examine the capabilities of ChatGPT in a human-centered design process. To this end, a hypothetical design project was conducted, where ChatGPT was utilized to generate personas, simulate interviews with fictional users, create new design ideas, simulate usage scenarios and conversations between an imaginary prototype and fictional users, and lastly evaluate user experience. The results show that ChatGPT effectively performed the tasks assigned to it as a designer, user, or product, providing mostly appropriate responses. The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity. The paper explains the potential benefits and limitations of using conversational LLMs in design, discusses its implications, and suggests directions for future research in this rapidly evolving area.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Kocaballi, A. Baki},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07406 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649},
	shorttitle = {Vision meets robotics},
	url = {https://doi.org/10.1177/0278364913491297},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	number = {11},
	urldate = {2023-02-17},
	journal = {International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	year = {2013},
	keywords = {/unread, Dataset, GPS, KITTI, SLAM, autonomous driving, benchmarks, cameras, computer vision, field robotics, laser, mobile robotics, object detection, optical flow, stereo, tracking},
	pages = {1231--1237},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {{KITTI}},
	doi = {10.1109/CVPR.2012.6248074},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Benchmark testing, Cameras, Measurement, Optical imaging, Optical sensors, Visualization},
	pages = {3354--3361},
}

@misc{brissman_camera_2023,
	title = {Camera {Calibration} without {Camera} {Access} -- {A} {Robust} {Validation} {Technique} for {Extended} {PnP} {Methods}},
	url = {http://arxiv.org/abs/2302.06949},
	doi = {10.48550/arXiv.2302.06949},
	abstract = {A challenge in image based metrology and forensics is intrinsic camera calibration when the used camera is unavailable. The unavailability raises two questions. The first question is how to find the projection model that describes the camera, and the second is to detect incorrect models. In this work, we use off-the-shelf extended PnP-methods to find the model from 2D-3D correspondences, and propose a method for model validation. The most common strategy for evaluating a projection model is comparing different models' residual variances - however, this naive strategy cannot distinguish whether the projection model is potentially underfitted or overfitted. To this end, we model the residual errors for each correspondence, individually scale all residuals using a predicted variance and test if the new residuals are drawn from a standard normal distribution. We demonstrate the effectiveness of our proposed validation in experiments on synthetic data, simulating 2D detection and Lidar measurements. Additionally, we provide experiments using data from an actual scene and compare non-camera access and camera access calibrations. Last, we use our method to validate annotations in MegaDepth.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Brissman, Emil and Forssén, Per-Erik and Edstedt, Johan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06949 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liang_hr-neus_2023,
	title = {{HR}-{NeuS}: {Recovering} {High}-{Frequency} {Surface} {Geometry} via {Neural} {Implicit} {Surfaces}},
	shorttitle = {{HR}-{NeuS}},
	url = {http://arxiv.org/abs/2302.06793},
	doi = {10.48550/arXiv.2302.06793},
	abstract = {Recent advances in neural implicit surfaces for multi-view 3D reconstruction primarily focus on improving large-scale surface reconstruction accuracy, but often produce over-smoothed geometries that lack fine surface details. To address this, we present High-Resolution NeuS (HR-NeuS), a novel neural implicit surface reconstruction method that recovers high-frequency surface geometry while maintaining large-scale reconstruction accuracy. We achieve this by utilizing (i) multi-resolution hash grid encoding rather than positional encoding at high frequencies, which boosts our model's expressiveness of local geometry details; (ii) a coarse-to-fine algorithmic framework that selectively applies surface regularization to coarse geometry without smoothing away fine details; (iii) a coarse-to-fine grid annealing strategy to train the network. We demonstrate through experiments on DTU and BlendedMVS datasets that our approach produces 3D geometries that are qualitatively more detailed and quantitatively of similar accuracy compared to previous approaches.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Liang, Erich and Deng, Kenan and Zhang, Xi and Wang, Chun-Kai},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06793 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{kasten_learning_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning to {Estimate} {Multi}-view {Pose} from {Object} {Silhouettes}},
	isbn = {978-3-031-25085-9},
	doi = {10.1007/978-3-031-25085-9_8},
	abstract = {While Structure-from-Motion pipelines certainly have their success cases in the task of 3D object reconstruction from multiple images, they still fail on many common objects that lack distinctive texture or have complex appearance qualities. The central problem lies in 6DOF camera pose estimation for the source images: without the ability to obtain a good estimate of the epipolar geometries, all state-of-the-art methods will fail. Although alternative solutions exist for specific objects, general solutions have proved elusive. In this work, we revisit the notion that silhouette cues can provide reasonable constraints on multi-view pose configurations when texture and priors are unavailable. Specifically, we train a neural network to holistically predict camera poses and pose confidences for a given set of input silhouette images, with the hypothesis that the network will be able to learn cues for multi-view relationships in a data-driven way. We show that our network generalizes to unseen synthetic and real object instances under reasonable assumptions about the input pose distribution of the images, and that the estimates are suitable to initialize state-of-the-art 3D reconstruction methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Kasten, Yoni and Price, True and Geraghty, David and Frahm, Jan-Michael},
	editor = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
	year = {2023},
	keywords = {/unread},
	pages = {129--147},
}

@misc{de_luigi_deep_2023,
	title = {Deep {Learning} on {Implicit} {Neural} {Representations} of {Shapes}},
	url = {http://arxiv.org/abs/2302.05438},
	doi = {10.48550/arXiv.2302.05438},
	abstract = {Implicit Neural Representations (INRs) have emerged in the last few years as a powerful tool to encode continuously a variety of different signals like images, videos, audio and 3D shapes. When applied to 3D shapes, INRs allow to overcome the fragmentation and shortcomings of the popular discrete representations used so far. Yet, considering that INRs consist in neural networks, it is not clear whether and how it may be possible to feed them into deep learning pipelines aimed at solving a downstream task. In this paper, we put forward this research problem and propose inr2vec, a framework that can compute a compact latent representation for an input INR in a single inference pass. We verify that inr2vec can embed effectively the 3D shapes represented by the input INRs and show how the produced embeddings can be fed into deep learning pipelines to solve several tasks by processing exclusively INRs.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {De Luigi, Luca and Cardace, Adriano and Spezialetti, Riccardo and Ramirez, Pierluigi Zama and Salti, Samuele and Di Stefano, Luigi},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05438 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{rudnev_nerf_2022,
	title = {{NeRF} for {Outdoor} {Scene} {Relighting}},
	shorttitle = {{NeRF}-{OSR}},
	url = {http://arxiv.org/abs/2112.05140},
	doi = {10.48550/arXiv.2112.05140},
	abstract = {Photorealistic editing of outdoor scenes from photographs requires a profound understanding of the image formation process and an accurate estimation of the scene geometry, reflectance and illumination. A delicate manipulation of the lighting can then be performed while keeping the scene albedo and geometry unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene relighting based on neural radiance fields. In contrast to the prior art, our technique allows simultaneous editing of both scene illumination and camera viewpoint using only a collection of outdoor photos shot in uncontrolled settings. Moreover, it enables direct control over the scene illumination, as defined through a spherical harmonics model. For evaluation, we collect a new benchmark dataset of several outdoor sites photographed from multiple viewpoints and at different times. For each time, a 360 degree environment map is captured together with a colour-calibration chequerboard to allow accurate numerical evaluations on real data against ground truth. Comparisons against SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at higher quality and with realistic self-shadowing reproduction. Our method and the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Rudnev, Viktor and Elgharib, Mohamed and Smith, William and Liu, Lingjie and Golyanik, Vladislav and Theobalt, Christian},
	month = jul,
	year = {2022},
	note = {arXiv:2112.05140 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading},
}

@inproceedings{klein_parallel_2007,
	title = {Parallel {Tracking} and {Mapping} for {Small} {AR} {Workspaces}},
	doi = {10.1109/ISMAR.2007.4538852},
	abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
	booktitle = {2007 6th {IEEE} and {ACM} {International} {Symposium} on {Mixed} and {Augmented} {Reality}},
	author = {Klein, Georg and Murray, David},
	month = nov,
	year = {2007},
	keywords = {/unread, Algorithm design and analysis, Cameras, Concurrent computing, Handheld computers, Layout, Robot vision systems, Robustness, Simultaneous localization and mapping, Tracking, Yarn},
	pages = {225--234},
}

@misc{guo_shadowformer_2023,
	title = {{ShadowFormer}: {Global} {Context} {Helps} {Image} {Shadow} {Removal}},
	shorttitle = {{ShadowFormer}},
	url = {http://arxiv.org/abs/2302.01650},
	doi = {10.48550/arXiv.2302.01650},
	abstract = {Recent deep learning methods have achieved promising results in image shadow removal. However, most of the existing approaches focus on working locally within shadow and non-shadow regions, resulting in severe artifacts around the shadow boundaries as well as inconsistent illumination between shadow and non-shadow regions. It is still challenging for the deep shadow removal model to exploit the global contextual correlation between shadow and non-shadow regions. In this work, we first propose a Retinex-based shadow model, from which we derive a novel transformer-based network, dubbed ShandowFormer, to exploit non-shadow regions to help shadow region restoration. A multi-scale channel attention framework is employed to hierarchically capture the global information. Based on that, we propose a Shadow-Interaction Module (SIM) with Shadow-Interaction Attention (SIA) in the bottleneck stage to effectively model the context correlation between shadow and non-shadow regions. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to evaluate the proposed method. Our method achieves state-of-the-art performance by using up to 150X fewer model parameters.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Guo, Lanqing and Huang, Siyu and Liu, Ding and Cheng, Hao and Wen, Bihan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_polynomial_2023,
	title = {Polynomial {Neural} {Fields} for {Subband} {Decomposition} and {Manipulation}},
	url = {http://arxiv.org/abs/2302.04862},
	doi = {10.48550/arXiv.2302.04862},
	abstract = {Neural fields have emerged as a new paradigm for representing signals, thanks to their ability to do it compactly while being easy to optimize. In most applications, however, neural fields are treated like black boxes, which precludes many signal manipulation tasks. In this paper, we propose a new class of neural fields called polynomial neural fields (PNFs). The key advantage of a PNF is that it can represent a signal as a composition of a number of manipulable and interpretable components without losing the merits of neural fields representation. We develop a general theoretical framework to analyze and design PNFs. We use this framework to design Fourier PNFs, which match state-of-the-art performance in signal representation tasks that use neural fields. In addition, we empirically demonstrate that Fourier PNFs enable signal manipulation applications such as texture transfer and scale-space interpolation. Code is available at https://github.com/stevenygd/PNF.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Yang, Guandao and Benaim, Sagie and Jampani, Varun and Genova, Kyle and Barron, Jonathan T. and Funkhouser, Thomas and Hariharan, Bharath and Belongie, Serge},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04862 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhong_snake_2022,
	title = {{SNAKE}: {Shape}-aware {Neural} {3D} {Keypoint} {Field}},
	shorttitle = {{SNAKE}},
	url = {http://arxiv.org/abs/2206.01724},
	doi = {10.48550/arXiv.2206.01724},
	abstract = {Detecting 3D keypoints from point clouds is important for shape reconstruction, while this work investigates the dual question: can shape reconstruction benefit 3D keypoint detection? Existing methods either seek salient features according to statistics of different orders or learn to predict keypoints that are invariant to transformation. Nevertheless, the idea of incorporating shape reconstruction into 3D keypoint detection is under-explored. We argue that this is restricted by former problem formulations. To this end, a novel unsupervised paradigm named SNAKE is proposed, which is short for shape-aware neural 3D keypoint field. Similar to recent coordinate-based radiance or distance field, our network takes 3D coordinates as inputs and predicts implicit shape indicators and keypoint saliency simultaneously, thus naturally entangling 3D keypoint detection and shape reconstruction. We achieve superior performance on various public benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness brings several advantages as follows. (1) SNAKE generates 3D keypoints consistent with human semantic annotation, even without such supervision. (2) SNAKE outperforms counterparts in terms of repeatability, especially when the input point clouds are down-sampled. (3) the generated keypoints allow accurate geometric registration, notably in a zero-shot setting. Codes are available at https://github.com/zhongcl-thu/SNAKE},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Zhong, Chengliang and You, Peixing and Chen, Xiaoxue and Zhao, Hao and Sun, Fuchun and Zhou, Guyue and Mu, Xiaodong and Gan, Chuang and Huang, Wenbing},
	month = oct,
	year = {2022},
	note = {arXiv:2206.01724 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_distance-aware_2022,
	title = {Distance-{Aware} {Occlusion} {Detection} {With} {Focused} {Attention}},
	volume = {31},
	issn = {1941-0042},
	doi = {10.1109/TIP.2022.3197984},
	abstract = {For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8\% to 38.6\% and boosts the occlusion F1-score from 34.4\% to 41.2\%. Our code is publicly available.},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, Yang and Tu, Yucheng and Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {/unread, Decoding, Feature extraction, Focused attention, Legged locomotion, Semantics, Task analysis, Transformers, Visualization, object pair detection, relative distance detection, relative occlusion detection, transformer model, visualizations of attention weights},
	pages = {5661--5676},
}

@misc{ranftl_towards_2020,
	title = {Towards {Robust} {Monocular} {Depth} {Estimation}: {Mixing} {Datasets} for {Zero}-shot {Cross}-dataset {Transfer}},
	shorttitle = {Towards {Robust} {Monocular} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/1907.01341},
	doi = {10.48550/arXiv.1907.01341},
	abstract = {The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer\vphantom{\{}\}, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation. Some results are shown in the supplementary video at https://youtu.be/D46FzVyL9I8},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Ranftl, René and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
	month = aug,
	year = {2020},
	note = {arXiv:1907.01341 [cs]
version: 3},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sabour_robustnerf_2023,
	title = {{RobustNeRF}: {Ignoring} {Distractors} with {Robust} {Losses}},
	shorttitle = {{RobustNeRF}},
	url = {http://arxiv.org/abs/2302.00833},
	doi = {10.48550/arXiv.2302.00833},
	abstract = {Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene. When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as view-dependent effects or 'floaters'. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distractors in training data as outliers of an optimization problem. Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern NeRF frameworks, with few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects. More results on our page https://robustnerf.github.io/public.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Sabour, Sara and Vora, Suhani and Duckworth, Daniel and Krasin, Ivan and Fleet, David J. and Tagliasacchi, Andrea},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00833 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Done Reading},
}

@inproceedings{ham_robust_2015,
	title = {Robust image filtering using joint static and dynamic guidance},
	doi = {10.1109/CVPR.2015.7299115},
	abstract = {Regularizing images under a guidance signal has been used in various tasks in computer vision and computational photography, particularly for noise reduction and joint upsampling. The aim is to transfer fine structures of guidance signals to input images, restoring noisy or altered structures. One of main drawbacks in such a data-dependent framework is that it does not handle differences in structure between guidance and input images. We address this problem by jointly leveraging structural information of guidance and input images. Image filtering is formulated as a nonconvex optimization problem, which is solved by the majorization-minimization algorithm. The proposed algorithm converges quickly while guaranteeing a local minimum. It effectively controls image structures at different scales and can handle a variety of types of data from different sensors. We demonstrate the flexibility and effectiveness of our model in several applications including depth super-resolution, scale-space filtering, texture removal, flash/non-flash denoising, and RGB/NIR denoising.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ham, Bumsub and Cho, Minsu and Ponce, Jean},
	month = jun,
	year = {2015},
	keywords = {/unread, Color, Image edge detection, Image resolution, Joints, Kernel, Linear programming, Noise reduction},
	pages = {4823--4831},
}

@misc{zheng_revisiting_2023,
	title = {Revisiting {Discriminative} vs. {Generative} {Classifiers}: {Theory} and {Implications}},
	shorttitle = {Revisiting {Discriminative} vs. {Generative} {Classifiers}},
	url = {http://arxiv.org/abs/2302.02334},
	doi = {10.48550/arXiv.2302.02334},
	abstract = {A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires \$O({\textbackslash}log n)\$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires \$O(n)\$ samples, where \$n\$ is the feature dimension. To establish it, we present a multiclass \${\textbackslash}mathcal\{H\}\$-consistency bound framework and an explicit bound for logistic loss, which are of independent interests. Simulation results on a mixture of Gaussian validate our theoretical findings. Experiments on various pre-trained deep vision models show that naive Bayes consistently converges faster as the number of data increases. Besides, naive Bayes shows promise in few-shot cases and we observe the ``two regimes'' phenomenon in pre-trained supervised models. Our code is available at https://github.com/ML-GSAI/Revisiting-Dis-vs-Gen-Classifiers.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Zheng, Chenyu and Wu, Guoqiang and Bao, Fan and Cao, Yue and Li, Chongxuan and Zhu, Jun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02334 [cs, stat]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chun_local_2023,
	title = {Local {Neural} {Descriptor} {Fields}: {Locally} {Conditioned} {Object} {Representations} for {Manipulation}},
	shorttitle = {Local {Neural} {Descriptor} {Fields}},
	url = {http://arxiv.org/abs/2302.03573},
	doi = {10.48550/arXiv.2302.03573},
	abstract = {A robot operating in a household environment will see a wide range of unique and unfamiliar objects. While a system could train on many of these, it is infeasible to predict all the objects a robot will see. In this paper, we present a method to generalize object manipulation skills acquired from a limited number of demonstrations, to novel objects from unseen shape categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes neural descriptors defined on the local geometry of the object to effectively transfer manipulation demonstrations to novel objects at test time. In doing so, we leverage the local geometry shared between objects to produce a more general manipulation framework. We illustrate the efficacy of our approach in manipulating novel objects in novel poses -- both in simulation and in the real world.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Chun, Ethan and Du, Yilun and Simeonov, Anthony and Lozano-Perez, Tomas and Kaelbling, Leslie},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03573 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{pascual_systematic_2023,
	title = {A {Systematic} {Review} on {Human} {Modeling}: {Digging} into {Human} {Digital} {Twin} {Implementations}},
	shorttitle = {A {Systematic} {Review} on {Human} {Modeling}},
	url = {http://arxiv.org/abs/2302.03593},
	doi = {10.48550/arXiv.2302.03593},
	abstract = {Human Digital Twins (HDTs) are digital replicas of humans that either mirror a complete human body, some parts of it as can be organs, flows, cells, or even human behaviors. An HDT is a human specific replica application inferred from the digital twin (DT) manufacturing concept, defined as a technique that creates digital replicas of physical systems or processes aimed at optimizing their performance and supporting more accurate decision-making processes. The main goal of this paper is to provide readers with a comprehensive overview of current efforts in the HDT field, by browsing its basic concepts, differences with DTs, existing developments, and the distinct areas of application. The review methodology includes an exhaustive review of scientific literature, patents, and industrial initiatives, as well as a discussion about ongoing and foreseen HDT research activity, emphasizing its potential benefits and limitations.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Pascual, Heribert and Bruin, Xavi Masip and Alonso, Albert and Cerdà, Judit},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03593 [cs]},
	keywords = {/unread, Computer Science - Other Computer Science},
}

@misc{huang_s4r_2023,
	title = {{S4R}: {Self}-{Supervised} {Semantic} {Scene} {Reconstruction} from {RGB}-{D} {Scans}},
	shorttitle = {{S4R}},
	url = {http://arxiv.org/abs/2302.03640},
	doi = {10.48550/arXiv.2302.03640},
	abstract = {Most deep learning approaches to comprehensive semantic modeling of 3D indoor spaces require costly dense annotations in the 3D domain. In this work, we explore a central 3D scene modeling task, namely, semantic scene reconstruction, using a fully self-supervised approach. To this end, we design a trainable model that employs both incomplete 3D reconstructions and their corresponding source RGB-D images, fusing cross-domain features into volumetric embeddings to predict complete 3D geometry, color, and semantics. Our key technical innovation is to leverage differentiable rendering of color and semantics, using the observed RGB images and a generic semantic segmentation model as color and semantics supervision, respectively. We additionally develop a method to synthesize an augmented set of virtual training views complementing the original real captures, enabling more efficient self-supervision for semantics. In this work we propose an end-to-end trainable solution jointly addressing geometry completion, colorization, and semantic mapping from a few RGB-D images, without 3D or 2D ground-truth. Our method is the first, to our knowledge, fully self-supervised method addressing completion and semantic segmentation of real-world 3D scans. It performs comparably well with the 3D supervised baselines, surpasses baselines with 2D supervision on real datasets, and generalizes well to unseen scenes.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Huang, Junwen and Artemorv, Alexey and Chen, Yujin and Zhi, Shuaifeng and Xu, Kai and Niessner, Matthias},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03640 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_nicer-slam_2023,
	title = {{NICER}-{SLAM}: {Neural} {Implicit} {Scene} {Encoding} for {RGB} {SLAM}},
	shorttitle = {{NICER}-{SLAM}},
	url = {http://arxiv.org/abs/2302.03594},
	doi = {10.48550/arXiv.2302.03594},
	abstract = {Neural implicit representations have recently become popular in simultaneous localization and mapping (SLAM), especially in dense visual SLAM. However, previous works in this direction either rely on RGB-D sensors, or require a separate monocular SLAM approach for camera tracking and do not produce high-fidelity dense 3D scene reconstruction. In this paper, we present NICER-SLAM, a dense RGB SLAM system that simultaneously optimizes for camera poses and a hierarchical neural implicit map representation, which also allows for high-quality novel view synthesis. To facilitate the optimization process for mapping, we integrate additional supervision signals including easy-to-obtain monocular geometric cues and optical flow, and also introduce a simple warping loss to further enforce geometry consistency. Moreover, to further boost performance in complicated indoor scenes, we also propose a local adaptive transformation from signed distance functions (SDFs) to density in the volume rendering equation. On both synthetic and real-world datasets we demonstrate strong performance in dense mapping, tracking, and novel view synthesis, even competitive with recent RGB-D SLAM systems.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Cui, Zhaopeng and Oswald, Martin R. and Geiger, Andreas and Pollefeys, Marc},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03594 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{fridovich-keil_k-planes_2023,
	title = {K-{Planes}: {Explicit} {Radiance} {Fields} in {Space}, {Time}, and {Appearance}},
	shorttitle = {K-{Planes}},
	url = {https://arxiv.org/abs/2301.10241v1},
	doi = {10.48550/arXiv.2301.10241},
	abstract = {We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see sarafridov.github.io/K-Planes.},
	language = {en},
	urldate = {2023-01-28},
	author = {Fridovich-Keil, Sara and Meanti, Giacomo and Warburg, Frederik and Recht, Benjamin and Kanazawa, Angjoo},
	month = jan,
	year = {2023},
	keywords = {/unread, Done Reading},
}

@inproceedings{rematas_urban_2022,
	title = {Urban {Radiance} {Fields}},
	doi = {10.1109/CVPR52688.2022.01259},
	abstract = {The goal of this work is to perform 3D reconstruction and novel view synthesis from data captured by scanning platforms commonly deployed for world mapping in urban outdoor environments (e.g., Street View). Given a sequence of posed RGB images and lidar sweeps acquired by cameras and scanners moving through an outdoor scene, we produce a model from which 3D surfaces can be extracted and novel RGB images can be synthesized. Our approach extends Neural Radiance Fields, which has been demonstrated to synthesize realistic novel images for small scenes in controlled settings, with new methods for leveraging asynchronously captured lidar data, for addressing exposure variation between captured images, and for leveraging predicted image segmentations to supervise densities on rays pointing at the sky. Each of these three extensions provides significant performance improvements in experiments on Street View data. Our system produces state-of-the-art 3D surface reconstructions and synthesizes higher quality novel views in comparison to both traditional methods (e.g. COLMAP) and recent neural representations (e.g. Mip-NeRF).},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Rematas, Konstantinos and Liu, Andrew and Srinivasan, Pratul and Barron, Jonathan and Tagliasacchi, Andrea and Funkhouser, Thomas and Ferrari, Vittorio},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Image and video synthesis and generation, Image segmentation, Laser radar, NeRF, Neural networks, Read Later, Solid modeling, Surface reconstruction, Three-dimensional displays, Urban areas, Vision + graphics},
	pages = {12922--12932},
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	language = {en},
	number = {2},
	urldate = {2023-02-07},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	keywords = {/unread},
	pages = {91--110},
}

@phdthesis{schonberger_robust_2018,
	type = {Doctoral {Thesis}},
	title = {Robust {Methods} for {Accurate} and {Efficient} {3D} {Modeling} from {Unstructured} {Imagery}},
	copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/295763},
	language = {en},
	urldate = {2023-02-07},
	school = {ETH Zurich},
	author = {Schönberger, Johannes L.},
	year = {2018},
	doi = {10.3929/ethz-b-000295763},
	note = {Accepted: 2018-10-15T06:22:19Z},
	keywords = {/unread},
}

@inproceedings{schonberger_structure--motion_2016,
	title = {Structure-from-{Motion} {Revisited}},
	doi = {10.1109/CVPR.2016.445},
	abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schönberger, Johannes L. and Frahm, Jan-Michael},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Cameras, Image reconstruction, Image registration, Internet, Pipelines, Robustness, Transmission line matrix methods},
	pages = {4104--4113},
}

@misc{wang_inv_2023,
	title = {{INV}: {Towards} {Streaming} {Incremental} {Neural} {Videos}},
	shorttitle = {{INV}},
	url = {http://arxiv.org/abs/2302.01532},
	doi = {10.48550/arXiv.2302.01532},
	abstract = {Recent works in spatiotemporal radiance fields can produce photorealistic free-viewpoint videos. However, they are inherently unsuitable for interactive streaming scenarios (e.g. video conferencing, telepresence) because have an inevitable lag even if the training is instantaneous. This is because these approaches consume videos and thus have to buffer chunks of frames (often seconds) before processing. In this work, we take a step towards interactive streaming via a frame-by-frame approach naturally free of lag. Conventional wisdom believes that per-frame NeRFs are impractical due to prohibitive training costs and storage. We break this belief by introducing Incremental Neural Videos (INV), a per-frame NeRF that is efficiently trained and streamable. We designed INV based on two insights: (1) Our main finding is that MLPs naturally partition themselves into Structure and Color Layers, which store structural and color/texture information respectively. (2) We leverage this property to retain and improve upon knowledge from previous frames, thus amortizing training across frames and reducing redundant learning. As a result, with negligible changes to NeRF, INV can achieve good qualities ({\textgreater}28.6db) in 8min/frame. It can also outperform prior SOTA in 19\% less training time. Additionally, our Temporal Weight Compression reduces the per-frame size to 0.3MB/frame (6.6\% of NeRF). More importantly, INV is free from buffer lag and is naturally fit for streaming. While this work does not achieve real-time training, it shows that incremental approaches like INV present new possibilities in interactive 3D streaming. Moreover, our discovery of natural information partition leads to a better understanding and manipulation of MLPs. Code and dataset will be released soon.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Wang, Shengze and Supikov, Alexey and Ratcliff, Joshua and Fuchs, Henry and Azuma, Ronald},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01532 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{kong_vmap_2023,
	title = {{vMAP}: {Vectorised} {Object} {Mapping} for {Neural} {Field} {SLAM}},
	shorttitle = {{vMAP}},
	url = {http://arxiv.org/abs/2302.01838},
	doi = {10.48550/arXiv.2302.01838},
	abstract = {We present vMAP, an object-level dense SLAM system using neural field representations. Each object is represented by a small MLP, enabling efficient, watertight object modelling without the need for 3D priors. As an RGB-D camera browses a scene with no prior information, vMAP detects object instances on-the-fly, and dynamically adds them to its map. Specifically, thanks to the power of vectorised training, vMAP can optimise as many as 50 individual objects in a single scene, with an extremely efficient training speed of 5Hz map update. We experimentally demonstrate significantly improved scene-level and object-level reconstruction quality compared to prior neural field SLAM systems.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Kong, Xin and Liu, Shikun and Taher, Marwan and Davison, Andrew J.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01838 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{kuang_ir-mcl_2023,
	title = {{IR}-{MCL}: {Implicit} {Representation}-{Based} {Online} {Global} {Localization}},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{IR}-{MCL}},
	url = {http://arxiv.org/abs/2210.03113},
	doi = {10.1109/LRA.2023.3239318},
	abstract = {Determining the state of a mobile robot is an essential building block of robot navigation systems. In this paper, we address the problem of estimating the robots pose in an indoor environment using 2D LiDAR data and investigate how modern environment models can improve gold standard Monte-Carlo localization (MCL) systems. We propose a neural occupancy field to implicitly represent the scene using a neural network. With the pretrained network, we can synthesize 2D LiDAR scans for an arbitrary robot pose through volume rendering. Based on the implicit representation, we can obtain the similarity between a synthesized and actual scan as an observation model and integrate it into an MCL system to perform accurate localization. We evaluate our approach on self-recorded datasets and three publicly available ones. We show that we can accurately and efficiently localize a robot using our approach surpassing the localization performance of state-of-the-art methods. The experiments suggest that the presented implicit representation is able to predict more accurate 2D LiDAR scans leading to an improved observation model for our particle filter-based localization. The code of our approach will be available at: https://github.com/PRBonn/ir-mcl.},
	urldate = {2023-02-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kuang, Haofei and Chen, Xieyuanli and Guadagnino, Tiziano and Zimmerman, Nicky and Behley, Jens and Stachniss, Cyrill},
	year = {2023},
	note = {arXiv:2210.03113 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	pages = {1--8},
}

@misc{heo_robust_2023,
	title = {Robust {Camera} {Pose} {Refinement} for {Multi}-{Resolution} {Hash} {Encoding}},
	url = {http://arxiv.org/abs/2302.01571},
	doi = {10.48550/arXiv.2302.01571},
	abstract = {Multi-resolution hash encoding has recently been proposed to reduce the computational cost of neural renderings, such as NeRF. This method requires accurate camera poses for the neural renderings of given scenes. However, contrary to previous methods jointly optimizing camera poses and 3D scenes, the naive gradient-based camera pose refinement method using multi-resolution hash encoding severely deteriorates performance. We propose a joint optimization algorithm to calibrate the camera pose and learn a geometric representation using efficient multi-resolution hash encoding. Showing that the oscillating gradient flows of hash encoding interfere with the registration of camera poses, our method addresses the issue by utilizing smooth interpolation weighting to stabilize the gradient oscillation for the ray samplings across hash grids. Moreover, the curriculum training procedure helps to learn the level-wise hash encoding, further increasing the pose refinement. Experiments on the novel-view synthesis datasets validate that our learning frameworks achieve state-of-the-art performance and rapid convergence of neural rendering, even when initial camera poses are unknown.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Heo, Hwan and Kim, Taekyung and Lee, Jiyoung and Lee, Jaewon and Kim, Soohyun and Kim, Hyunwoo J. and Kim, Jin-Hwa},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01571 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{ma_neural_2022,
	title = {Neural {Point}-based {Shape} {Modeling} of {Humans} in {Challenging} {Clothing}},
	url = {http://arxiv.org/abs/2209.06814},
	doi = {10.48550/arXiv.2209.06814},
	abstract = {Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to clothing because they have a fixed mesh topology and resolution. To address these limitations, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose-independent "coarse shape" that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person-specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes at \{{\textbackslash}small{\textbackslash}url\{https://qianlim.github.io/SkiRT\}\}.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Ma, Qianli and Yang, Jinlong and Black, Michael J. and Tang, Siyu},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06814 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wang_normalized_2019,
	title = {Normalized {Object} {Coordinate} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	url = {http://arxiv.org/abs/1901.02970},
	doi = {10.48550/arXiv.1901.02970},
	abstract = {The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to "instance-level" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
	month = jun,
	year = {2019},
	note = {arXiv:1901.02970 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{xu_mr-gnn_2019,
	title = {{MR}-{GNN}: {Multi}-{Resolution} and {Dual} {Graph} {Neural} {Network} for {Predicting} {Structured} {Entity} {Interactions}},
	shorttitle = {{MR}-{GNN}},
	url = {http://arxiv.org/abs/1905.09558},
	doi = {10.24963/ijcai.2019/551},
	abstract = {Predicting interactions between structured entities lies at the core of numerous tasks such as drug regimen and new material design. In recent years, graph neural networks have become attractive. They represent structured entities as graphs and then extract features from each individual graph using graph convolution operations. However, these methods have some limitations: i) their networks only extract features from a fix-sized subgraph structure (i.e., a fix-sized receptive field) of each node, and ignore features in substructures of different sizes, and ii) features are extracted by considering each entity independently, which may not effectively reflect the interaction between two entities. To resolve these problems, we present MR-GNN, an end-to-end graph neural network with the following features: i) it uses a multi-resolution based architecture to extract node features from different neighborhoods of each node, and, ii) it uses dual graph-state long short-term memory networks (L-STMs) to summarize local features of each graph and extracts the interaction features between pairwise graphs. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Xu, Nuo and Wang, Pinghui and Chen, Long and Tao, Jing and Zhao, Junzhou},
	month = aug,
	year = {2019},
	note = {arXiv:1905.09558 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3968--3974},
}

@misc{medina_evolving_2023,
	title = {Evolving {Flying} {Machines} in {Minecraft} {Using} {Quality} {Diversity}},
	url = {http://arxiv.org/abs/2302.00782},
	doi = {10.48550/arXiv.2302.00782},
	abstract = {Minecraft is a great testbed for human creativity that has inspired the design of various structures and even functioning machines, including flying machines. EvoCraft is an API for programmatically generating structures in Minecraft, but the initial work in this domain was not capable of evolving flying machines. This paper applies fitness-based evolution and quality diversity search in order to evolve flying machines. Although fitness alone can occasionally produce flying machines, thanks in part to a more sophisticated fitness function than was used previously, the quality diversity algorithm MAP-Elites is capable of discovering flying machines much more reliably, at least when an appropriate behavior characterization is used to guide the search for diverse solutions.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Medina, Alejandro and Richey, Melanie and Mueller, Mark and Schrum, Jacob},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00782 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{chen_factor_2023,
	title = {Factor {Fields}: {A} {Unified} {Framework} for {Neural} {Fields} and {Beyond}},
	shorttitle = {Factor {Fields}},
	url = {http://arxiv.org/abs/2302.01226},
	doi = {10.48550/arXiv.2302.01226},
	abstract = {We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when reconstructing 3D signed distance fields and higher compactness for radiance field reconstruction tasks compared to previous fast reconstruction methods. Besides, our CoBaFa representation enables generalization by sharing the basis across signals during training, enabling generalization tasks such as image regression with sparse observations and few-shot radiance field reconstruction.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Chen, Anpei and Xu, Zexiang and Wei, Xinyue and Tang, Siyu and Su, Hao and Geiger, Andreas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01226 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{mingyang_graphreg_2023,
	title = {{GraphReg}: {Dynamical} {Point} {Cloud} {Registration} with {Geometry}-aware {Graph} {Signal} {Processing}},
	shorttitle = {{GraphReg}},
	url = {http://arxiv.org/abs/2302.01109},
	doi = {10.48550/arXiv.2302.01109},
	abstract = {This study presents a high-accuracy, efficient, and physically induced method for 3D point cloud registration, which is the core of many important 3D vision problems. In contrast to existing physics-based methods that merely consider spatial point information and ignore surface geometry, we explore geometry aware rigid-body dynamics to regulate the particle (point) motion, which results in more precise and robust registration. Our proposed method consists of four major modules. First, we leverage the graph signal processing (GSP) framework to define a new signature, (i.e., point response intensity for each point), by which we succeed in describing the local surface variation, resampling keypoints, and distinguishing different particles. Then, to address the shortcomings of current physics-based approaches that are sensitive to outliers, we accommodate the defined point response intensity to median absolute deviation (MAD) in robust statistics and adopt the X84 principle for adaptive outlier depression, ensuring a robust and stable registration. Subsequently, we propose a novel geometric invariant under rigid transformations to incorporate higher-order features of point clouds, which is further embedded for force modeling to guide the correspondence between pairwise scans credibly. Finally, we introduce an adaptive simulated annealing (ASA) method to search for the global optimum and substantially accelerate the registration process. We perform comprehensive experiments to evaluate the proposed method on various datasets captured from range scanners to LiDAR. Results demonstrate that our proposed method outperforms representative state-of-the-art approaches in terms of accuracy and is more suitable for registering large-scale point clouds. Furthermore, it is considerably faster and more robust than most competitors.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Mingyang, Zhao and Lei, Ma and Xiaohong, Jia and Dong-Ming, Yan and Tiejun, Huang},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01109 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ding_efficient_2023,
	title = {An {Efficient} {Convex} {Hull}-{Based} {Vehicle} {Pose} {Estimation} {Method} for {3D} {LiDAR}},
	url = {http://arxiv.org/abs/2302.01034},
	doi = {10.48550/arXiv.2302.01034},
	abstract = {Vehicle pose estimation is essential in the perception technology of autonomous driving. However, due to the different density distributions of the LiDAR point cloud, it is challenging to achieve accurate direction extraction based on 3D LiDAR by using the existing pose estimation methods. In this paper, we proposed a novel convex hull-based vehicle pose estimation method. The extracted 3D cluster is reduced to the convex hull, reducing the computation burden. Then a novel criterion based on the minimum occlusion area is developed for the search-based algorithm, which can achieve accurate pose estimation. The proposed algorithm is validated on the KITTI dataset and a manually labeled dataset acquired at an industrial park. The results show that our proposed method can achieve better accuracy than the three mainstream algorithms while maintaining real-time speed.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Ding, Ningning},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01034 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ost_neural_2021,
	title = {Neural {Scene} {Graphs} for {Dynamic} {Scenes}},
	url = {http://arxiv.org/abs/2011.10379},
	doi = {10.48550/arXiv.2011.10379},
	abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
	month = mar,
	year = {2021},
	note = {arXiv:2011.10379 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{cao_hexplane_2023,
	title = {{HexPlane}: {A} {Fast} {Representation} for {Dynamic} {Scenes}},
	shorttitle = {{HexPlane}},
	url = {http://arxiv.org/abs/2301.09632},
	doi = {10.48550/arXiv.2301.09632},
	abstract = {Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than \$100{\textbackslash}times\$. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlanes are a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Cao, Ang and Johnson, Justin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09632 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yoshiyama_ndjir_2023,
	title = {{NDJIR}: {Neural} {Direct} and {Joint} {Inverse} {Rendering} for {Geometry}, {Lights}, and {Materials} of {Real} {Object}},
	shorttitle = {{NDJIR}},
	url = {http://arxiv.org/abs/2302.00675},
	doi = {10.48550/arXiv.2302.00675},
	abstract = {The goal of inverse rendering is to decompose geometry, lights, and materials given pose multi-view images. To achieve this goal, we propose neural direct and joint inverse rendering, NDJIR. Different from prior works which relies on some approximations of the rendering equation, NDJIR directly addresses the integrals in the rendering equation and jointly decomposes geometry: signed distance function, lights: environment and implicit lights, materials: base color, roughness, specular reflectance using the powerful and flexible volume rendering framework, voxel grid feature, and Bayesian prior. Our method directly uses the physically-based rendering, so we can seamlessly export an extracted mesh with materials to DCC tools and show material conversion examples. We perform intensive experiments to show that our proposed method can decompose semantically well for real object in photogrammetric setting and what factors contribute towards accurate inverse rendering.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Yoshiyama, Kazuki and Narihira, Takuya},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00675 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{max_optical_1995,
	title = {Optical models for direct volume rendering},
	volume = {1},
	issn = {1941-0506},
	doi = {10.1109/2945.468400},
	abstract = {This tutorial survey paper reviews several different models for light interaction with volume densities of absorbing, glowing, reflecting, and/or scattering material. They are, in order of increasing realism, absorption only, emission only, emission and absorption combined, single scattering of external illumination without shadows, single scattering with shadows, and multiple scattering. For each model the paper provides the physical assumptions, describes the applications for which it is appropriate, derives the differential or integral equations for light transport, presents calculation methods for solving them, and shows output images for a data set representing a cloud. Special attention is given to calculation methods for the multiple scattering model.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Max, N.},
	month = jun,
	year = {1995},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {/unread, Absorption, Computational modeling, Grid computing, Interpolation, Light scattering, Optical computing, Optical scattering, Rendering (computer graphics), Stimulated emission, X-ray scattering},
	pages = {99--108},
}

@misc{li_nerfacc_2022,
	title = {{NerfAcc}: {A} {General} {NeRF} {Acceleration} {Toolbox}},
	shorttitle = {{NerfAcc}},
	url = {http://arxiv.org/abs/2210.04847},
	doi = {10.48550/arXiv.2210.04847},
	abstract = {We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance fields. We build on the techniques proposed in Instant-NGP, and extend these techniques to not only support bounded static scenes, but also for dynamic scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and is ready for plug-and-play acceleration of most NeRFs. Various examples are provided to show how to use this toolbox. Code can be found here: https://github.com/KAIR-BAIR/nerfacc.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Li, Ruilong and Tancik, Matthew and Kanazawa, Angjoo},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04847 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{cheng_continuous_2023,
	title = {Continuous {U}-{Net}: {Faster}, {Greater} and {Noiseless}},
	shorttitle = {Continuous {U}-{Net}},
	url = {http://arxiv.org/abs/2302.00626},
	doi = {10.48550/arXiv.2302.00626},
	abstract = {Image segmentation is a fundamental task in image analysis and clinical practice. The current state-of-the-art techniques are based on U-shape type encoder-decoder networks with skip connections, called U-Net. Despite the powerful performance reported by existing U-Net type networks, they suffer from several major limitations. Issues include the hard coding of the receptive field size, compromising the performance and computational cost, as well as the fact that they do not account for inherent noise in the data. They have problems associated with discrete layers, and do not offer any theoretical underpinning. In this work we introduce continuous U-Net, a novel family of networks for image segmentation. Firstly, continuous U-Net is a continuous deep neural network that introduces new dynamic blocks modelled by second order ordinary differential equations. Secondly, we provide theoretical guarantees for our network demonstrating faster convergence, higher robustness and less sensitivity to noise. Thirdly, we derive qualitative measures to tailor-made segmentation tasks. We demonstrate, through extensive numerical and visual results, that our model outperforms existing U-Net blocks for several medical image segmentation benchmarking datasets.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Cheng, Chun-Wun and Runkel, Christina and Liu, Lihao and Chan, Raymond H. and Schönlieb, Carola-Bibiane and Aviles-Rivero, Angelica I.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00626 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{nolte_stroke-based_2022,
	title = {Stroke-based {Rendering}: {From} {Heuristics} to {Deep} {Learning}},
	shorttitle = {Stroke-based {Rendering}},
	url = {http://arxiv.org/abs/2302.00595},
	doi = {10.48550/arXiv.2302.00595},
	abstract = {In the last few years, artistic image-making with deep learning models has gained a considerable amount of traction. A large number of these models operate directly in the pixel space and generate raster images. This is however not how most humans would produce artworks, for example, by planning a sequence of shapes and strokes to draw. Recent developments in deep learning methods help to bridge the gap between stroke-based paintings and pixel photo generation. With this survey, we aim to provide a structured introduction and understanding of common challenges and approaches in stroke-based rendering algorithms. These algorithms range from simple rule-based heuristics to stroke optimization and deep reinforcement agents, trained to paint images with differentiable vector graphics and neural rendering.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Nolte, Florian and Melnik, Andrew and Ritter, Helge},
	month = dec,
	year = {2022},
	note = {arXiv:2302.00595 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chen_uncertainty-driven_2023,
	title = {Uncertainty-{Driven} {Dense} {Two}-{View} {Structure} from {Motion}},
	url = {http://arxiv.org/abs/2302.00523},
	doi = {10.48550/arXiv.2302.00523},
	abstract = {This work introduces an effective and practical solution to the dense two-view structure from motion (SfM) problem. One vital question addressed is how to mindfully use per-pixel optical flow correspondence between two frames for accurate pose estimation -- as perfect per-pixel correspondence between two images is difficult, if not impossible, to establish. With the carefully estimated camera pose and predicted per-pixel optical flow correspondences, a dense depth of the scene is computed. Later, an iterative refinement procedure is introduced to further improve optical flow matching confidence, camera pose, and depth, exploiting their inherent dependency in rigid SfM. The fundamental idea presented is to benefit from per-pixel uncertainty in the optical flow estimation and provide robustness to the dense SfM system via an online refinement. Concretely, we introduce a pipeline consisting of (i) an uncertainty-aware dense optical flow estimation approach that provides per-pixel correspondence with their confidence score of matching; (ii) a weighted dense bundle adjustment formulation that depends on optical flow uncertainty and bidirectional optical flow consistency to refine both pose and depth; (iii) a depth estimation network that considers its consistency with the estimated poses and optical flow respecting epipolar constraint. Extensive experiments show that the proposed approach achieves remarkable depth accuracy and state-of-the-art camera pose results superseding SuperPoint and SuperGlue accuracy when tested on benchmark datasets such as DeMoN, YFCC100M, and ScanNet.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Chen, Weirong and Kumar, Suryansh and Yu, Fisher},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00523 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{hu_neural_2023,
	title = {Neural {Wavelet}-domain {Diffusion} for {3D} {Shape} {Generation}, {Inversion}, and {Manipulation}},
	url = {http://arxiv.org/abs/2302.00190},
	doi = {10.48550/arXiv.2302.00190},
	abstract = {This paper presents a new approach for 3D shape generation, inversion, and manipulation, through a direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a diffusion-based generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details. Further, we may jointly train an encoder network to learn a latent space for inverting shapes, allowing us to enable a rich variety of whole-shape and region-aware shape manipulations. Both quantitative and qualitative experimental results manifest the compelling shape generation, inversion, and manipulation capabilities of our approach over the state-of-the-art methods.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Hu, Jingyu and Hui, Ka-Hei and Liu, Zhengzhe and Li, Ruihui and Fu, Chi-Wing},
	month = jan,
	year = {2023},
	note = {arXiv:2302.00190 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{barron_shape_2020,
	title = {Shape, {Illumination}, and {Reflectance} from {Shading}},
	url = {http://arxiv.org/abs/2010.03592},
	doi = {10.48550/arXiv.2010.03592},
	abstract = {A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from flat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reflectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison -- there are an infinite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and define an optimization problem that searches for the *most likely* explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems.},
	urldate = {2023-02-02},
	journal = {TPAMI 2015},
	author = {Barron, Jonathan T. and Malik, Jitendra},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03592 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ma_intrinsic_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Intrinsic {Image} {Decomposition}: {A} {Comprehensive} {Review}},
	isbn = {978-3-319-71607-7},
	shorttitle = {Intrinsic {Image} {Decomposition}},
	doi = {10.1007/978-3-319-71607-7_55},
	abstract = {Image understanding and analysis is one of the important tasks in the image processing. Multiple factors influence the appearance of an object in an image. However, extracting the intrinsic images from the observer image can eliminate the environmental impact effectively and make the image understanding more accurately. The intrinsic images represent the inherent shape, color and texture information of the object. Intrinsic image decomposition is recovering shading image and reflectance image from a single input image and remains a challenging problem because of its severely ill-posed problem. In order to deal with these problems, researches have proposed various algorithms for decomposing the intrinsic image. In this paper we survey the recent advances in intrinsic image decomposition. First, we introduce the existing datasets for intrinsic image decomposition. Second, we introduce and analyze the existing intrinsic image decomposition algorithms. Finally, we use the existing algorithms to experiment on the intrinsic image datasets, and analyze and summarize the experimental results.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Ma, Yupeng and Feng, Xiaoyi and Jiang, Xiaoyue and Xia, Zhaoqiang and Peng, Jinye},
	editor = {Zhao, Yao and Kong, Xiangwei and Taubman, David},
	year = {2017},
	keywords = {/unread, Computer vision, Intrinsic image dataset, Intrinsic image decomposition, Retinex theory},
	pages = {626--638},
}

@misc{fonseca_continuous_2023,
	title = {Continuous {Spatiotemporal} {Transformers}},
	url = {http://arxiv.org/abs/2301.13338},
	doi = {10.48550/arXiv.2301.13338},
	abstract = {Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Fonseca, Antonio H. de O. and Zappala, Emanuele and Caro, Josue Ortega and van Dijk, David},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13338 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{caddeo_collision-aware_2023,
	title = {Collision-aware {In}-hand {6D} {Object} {Pose} {Estimation} using {Multiple} {Vision}-based {Tactile} {Sensors}},
	url = {http://arxiv.org/abs/2301.13667},
	doi = {10.48550/arXiv.2301.13667},
	abstract = {In this paper, we address the problem of estimating the in-hand 6D pose of an object in contact with multiple vision-based tactile sensors. We reason on the possible spatial configurations of the sensors along the object surface. Specifically, we filter contact hypotheses using geometric reasoning and a Convolutional Neural Network (CNN), trained on simulated object-agnostic images, to promote those that better comply with the actual tactile images from the sensors. We use the selected sensors configurations to optimize over the space of 6D poses using a Gradient Descent-based approach. We finally rank the obtained poses by penalizing those that are in collision with the sensors. We carry out experiments in simulation using the DIGIT vision-based sensor with several objects, from the standard YCB model set. The results demonstrate that our approach estimates object poses that are compatible with actual object-sensor contacts in \$87.5{\textbackslash}\%\$ of cases while reaching an average positional error in the order of \$2\$ centimeters. Our analysis also includes qualitative results of experiments with a real DIGIT sensor.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Caddeo, Gabriele M. and Piga, Nicola A. and Bottarel, Fabrizio and Natale, Lorenzo},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13667 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{tinchev_real-time_2023,
	title = {Real-time {LIDAR} localization in natural and urban environments},
	url = {http://arxiv.org/abs/2301.13583},
	doi = {10.48550/arXiv.2301.13583},
	abstract = {Localization is a key challenge in many robotics applications. In this work we explore LIDAR-based global localization in both urban and natural environments and develop a method suitable for online application. Our approach leverages efficient deep learning architecture capable of learning compact point cloud descriptors directly from 3D data. The method uses an efficient feature space representation of a set of segmented point clouds to match between the current scene and the prior map. We show that down-sampling in the inner layers of the network can significantly reduce computation time without sacrificing performance. We present substantial evaluation of LIDAR-based global localization methods on nine scenarios from six datasets varying between urban, park, forest, and industrial environments. Part of which includes post-processed data from 30 sequences of the Oxford RobotCar dataset, which we make publicly available. Our experiments demonstrate a factor of three reduction of computation, 70\% lower memory consumption with marginal loss in localization frequency. The proposed method allows the full pipeline to run on robots with limited computation payload such as drones, quadrupeds, and UGVs as it does not require a GPU at run time.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Tinchev, Georgi and Penate-Sanchez, Adrian and Fallon, Maurice},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13583 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{shan_fine_2023,
	title = {Fine {Robotic} {Manipulation} without {Force}/{Torque} {Sensor}},
	url = {http://arxiv.org/abs/2301.13413},
	doi = {10.48550/arXiv.2301.13413},
	abstract = {Force Sensing and Force Control are essential to many industrial applications. Typically, a 6-axis Force/Torque (F/T) sensor is mounted between the robot's wrist and the end-effector in order to measure the forces and torques exerted by the environment onto the robot (the external wrench). Although a typical 6-axis F/T sensor can provide highly accurate measurements, it is expensive and vulnerable to drift and external impacts. Existing methods aiming at estimating the external wrench using only the robot's internal signals are limited in scope: for example, wrench estimation accuracy was mostly validated in free-space motions and simple contacts as opposed to tasks like assembly that require high-precision force control. Here we present a Neural Network based method and argue that by devoting particular attention to the training data structure, it is possible to accurately estimate the external wrench in a wide range of scenarios based solely on internal signals. As an illustration, we demonstrate a pin insertion experiment with 100-micron clearance and a hand-guiding experiment, both performed without external F/T sensors or joint torque sensors. Our result opens the possibility of equipping the existing 2.7 million industrial robots with Force Sensing and Force Control capabilities without any additional hardware.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Shan, Shilin and Pham, Quang-Cuong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13413 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{michaux_cant_2023,
	title = {Can't {Touch} {This}: {Real}-{Time}, {Safe} {Motion} {Planning} and {Control} for {Manipulators} {Under} {Uncertainty}},
	shorttitle = {Can't {Touch} {This}},
	url = {http://arxiv.org/abs/2301.13308},
	doi = {10.48550/arXiv.2301.13308},
	abstract = {A key challenge to the widespread deployment of robotic manipulators is the need to ensure safety in arbitrary environments while generating new motion plans in real-time. In particular, one must ensure that a manipulator does not collide with obstacles, collide with itself, or exceed its joint torque limits. This challenge is compounded by the need to account for uncertainty in the mass and inertia of manipulated objects, and potentially the robot itself. The present work addresses this challenge by proposing Autonomous Robust Manipulation via Optimization with Uncertainty-aware Reachability (ARMOUR), a provably-safe, receding-horizon trajectory planner and tracking controller framework for serial link manipulators. ARMOUR works by first constructing a robust, passivity-based controller that is proven to enable a manipulator to track desired trajectories with bounded error despite uncertain dynamics. Next, ARMOUR uses a novel variation on the Recursive Newton-Euler Algorithm (RNEA) to compute the set of all possible inputs required to track any trajectory within a continuum of desired trajectories. Finally, the method computes an over-approximation to the swept volume of the manipulator; this enables one to formulate an optimization problem, which can be solved in real-time, to synthesize provably-safe motion. The proposed method is compared to state of the art methods and demonstrated on a variety of challenging manipulation examples in simulation and on real hardware, such as maneuvering a dumbbell with uncertain mass around obstacles.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Michaux, Jonathan and Holmes, Patrick and Zhang, Bohao and Chen, Che and Wang, Baiyue and Sahgal, Shrey and Zhang, Tiancheng and Dey, Sidhartha and Kousik, Shreyas and Vasudevan, Ram},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13308 [cs, eess, math]},
	keywords = {/unread, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@misc{chang_mono-star_2023,
	title = {Mono-{STAR}: {Mono}-camera {Scene}-level {Tracking} and {Reconstruction}},
	shorttitle = {Mono-{STAR}},
	url = {http://arxiv.org/abs/2301.13244},
	doi = {10.48550/arXiv.2301.13244},
	abstract = {We present Mono-STAR, the first real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change under a unified framework. The proposed system solves a new optimization problem incorporating optical-flow-based 2D constraints to deal with fast motion and a novel semantic-aware deformation graph (SAD-graph) for handling topology change. We test the proposed system under various challenging scenes and demonstrate that it significantly outperforms existing state-of-the-art methods.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Chang, Haonan and Ramesh, Dhruv Metha and Geng, Shijie and Gan, Yuqiu and Boularias, Abdeslam},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13244 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{gupta_patch_2023,
	title = {Patch {Gradient} {Descent}: {Training} {Neural} {Networks} on {Very} {Large} {Images}},
	shorttitle = {Patch {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2301.13817},
	doi = {10.48550/arXiv.2301.13817},
	abstract = {Traditional CNN models are trained and tested on relatively low resolution images ({\textless}300 px), and cannot be directly operated on large-scale images due to compute and memory constraints. We propose Patch Gradient Descent (PatchGD), an effective learning strategy that allows to train the existing CNN architectures on large-scale images in an end-to-end manner. PatchGD is based on the hypothesis that instead of performing gradient-based updates on an entire image at once, it should be possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. PatchGD thus extensively enjoys better memory and compute efficiency when training models on large scale images. PatchGD is thoroughly evaluated on two datasets - PANDA and UltraMNIST with ResNet50 and MobileNetV2 models under different memory constraints. Our evaluation clearly shows that PatchGD is much more stable and efficient than the standard gradient-descent method in handling large images, and especially when the compute memory is limited.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Gupta, Deepak K. and Mago, Gowreesh and Chavan, Arnav and Prasad, Dilip K.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13817 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sulzer_survey_2023,
	title = {A {Survey} and {Benchmark} of {Automatic} {Surface} {Reconstruction} from {Point} {Clouds}},
	url = {http://arxiv.org/abs/2301.13656},
	doi = {10.48550/arXiv.2301.13656},
	abstract = {We survey and benchmark traditional and novel learning-based algorithms that address the problem of surface reconstruction from point clouds. Surface reconstruction from point clouds is particularly challenging when applied to real-world acquisitions, due to noise, outliers, non-uniform sampling and missing data. Traditionally, different handcrafted priors of the input points or the output surface have been proposed to make the problem more tractable. However, hyperparameter tuning for adjusting priors to different acquisition defects can be a tedious task. To this end, the deep learning community has recently addressed the surface reconstruction problem. In contrast to traditional approaches, deep surface reconstruction methods can learn priors directly from a training set of point clouds and corresponding true surfaces. In our survey, we detail how different handcrafted and learned priors affect the robustness of methods to defect-laden input and their capability to generate geometric and topologically accurate reconstructions. In our benchmark, we evaluate the reconstructions of several traditional and learning-based methods on the same grounds. We show that learning-based methods can generalize to unseen shape categories, but their training and test sets must share the same point cloud characteristics. We also provide the code and data to compete in our benchmark and to further stimulate the development of learning-based surface reconstruction https://github.com/raphaelsulzer/dsr-benchmark.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Sulzer, Raphael and Landrieu, Loic and Marlet, Renaud and Vallet, Bruno},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13656 [cs]},
	keywords = {/unread, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yuan_monocular_2023,
	title = {Monocular {Scene} {Reconstruction} with {3D} {SDF} {Transformers}},
	url = {http://arxiv.org/abs/2301.13510},
	doi = {10.48550/arXiv.2301.13510},
	abstract = {Monocular scene reconstruction from posed images is challenging due to the complexity of a large environment. Recent volumetric methods learn to directly predict the TSDF volume and have demonstrated promising results in this task. However, most methods focus on how to extract and fuse the 2D features to a 3D feature volume, but none of them improve the way how the 3D volume is aggregated. In this work, we propose an SDF transformer network, which replaces the role of 3D CNN for better 3D feature aggregation. To reduce the explosive computation complexity of the 3D multi-head attention, we propose a sparse window attention module, where the attention is only calculated between the non-empty voxels within a local window. Then a top-down-bottom-up 3D attention network is built for 3D feature aggregation, where a dilate-attention structure is proposed to prevent geometry degeneration, and two global modules are employed to equip with global receptive fields. The experiments on multiple datasets show that this 3D transformer network generates a more accurate and complete reconstruction, which outperforms previous methods by a large margin. Remarkably, the mesh accuracy is improved by 41.8\%, and the mesh completeness is improved by 25.3\% on the ScanNet dataset. Project page: https://weihaosky.github.io/sdfformer.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Yuan, Weihao and Gu, Xiaodong and Li, Heng and Dong, Zilong and Zhu, Siyu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13510 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_end--end_2018,
	title = {End-to-{End} {United} {Video} {Dehazing} and {Detection}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12287},
	doi = {10.1609/aaai.v32i1.12287},
	abstract = {The recent development of CNN-based image dehazing has revealed the effectiveness of end-to-end modeling. However, extending the idea to end-to-end video dehazing has not been explored yet. In this paper, we propose an End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal consistency between consecutive video frames. A thorough study has been conducted over a number of structure options, to identify the best temporal fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and Detection Network (EVDD-Net), which concatenates and jointly trains EVD-Net with a video object detection model. The resulting augmented end-to-end pipeline has demonstrated much more stable and accurate detection results in hazy video.},
	language = {en},
	number = {1},
	urldate = {2023-02-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Boyi and Peng, Xiulian and Wang, Zhangyang and Xu, Jizheng and Feng, Dan},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {/unread, Machine Learning Applications},
}

@article{ren_deep_2019,
	title = {Deep {Video} {Dehazing} {With} {Semantic} {Segmentation}},
	volume = {28},
	issn = {1941-0042},
	doi = {10.1109/TIP.2018.2876178},
	abstract = {Recent research have shown the potential of using convolutional neural networks (CNNs) to accomplish single image dehazing. In this paper, we take one step further to explore the possibility of exploiting a network to perform haze removal for videos. Unlike single image dehazing, video-based approaches can take advantage of the abundant information that exists across neighboring frames. In this paper, assuming that a scene point yields highly correlated transmission values between adjacent video frames, we develop a deep learning solution for video dehazing, where a CNN is trained end-to-end to learn how to accumulate information across frames for transmission estimation. The estimated transmission map is subsequently used to recover a haze-free frame via atmospheric scattering model. In addition, as the semantic information of a scene provides a strong prior for image restoration, we propose to incorporate global semantic priors as input to regularize the transmission maps so that the estimated maps can be smooth in the regions of the same object and only discontinuous across the boundaries of different objects. To train this network, we generate a dataset consisted of synthetic hazy and haze-free videos for supervision based on the NYU depth dataset. We show that the features learned from this dataset are capable of removing haze that arises in outdoor scenes in a wide range of videos. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world videos.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Ren, Wenqi and Zhang, Jingang and Xu, Xiangyu and Ma, Lin and Cao, Xiaochun and Meng, Gaofeng and Liu, Wei},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {/unread, Atmospheric modeling, Coherence, Convolutional neural networks, Image color analysis, Image segmentation, Semantics, Task analysis, Video dehazing, convolutional neural network, defogging, transmission map},
	pages = {1895--1908},
}

@misc{raymond_online_2023,
	title = {Online {Loss} {Function} {Learning}},
	url = {http://arxiv.org/abs/2301.13247},
	doi = {10.48550/arXiv.2301.13247},
	abstract = {Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently outperforms the cross-entropy loss and offline loss function learning techniques on a diverse range of neural network architectures and datasets.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Raymond, Christian and Chen, Qi and Xue, Bing and Zhang, Mengjie},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13247 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{tzathas_3d_2023,
	title = {{3D} {Neural} {Sculpting} ({3DNS}): {Editing} {Neural} {Signed} {Distance} {Functions}},
	shorttitle = {{3D} {Neural} {Sculpting} ({3DNS})},
	url = {http://arxiv.org/abs/2209.13971},
	doi = {10.48550/arXiv.2209.13971},
	abstract = {In recent years, implicit surface representations through neural networks that encode the signed distance have gained popularity and have achieved state-of-the-art results in various tasks (e.g. shape representation, shape reconstruction, and learning shape priors). However, in contrast to conventional shape representations such as polygon meshes, the implicit representations cannot be easily edited and existing works that attempt to address this problem are extremely limited. In this work, we propose the first method for efficient interactive editing of signed distance functions expressed through neural networks, allowing free-form editing. Inspired by 3D sculpting software for meshes, we use a brush-based framework that is intuitive and can in the future be used by sculptors and digital artists. In order to localize the desired surface deformations, we regulate the network by using a copy of it to sample the previously expressed surface. We introduce a novel framework for simulating sculpting-style surface edits, in conjunction with interactive surface sampling and efficient adaptation of network weights. We qualitatively and quantitatively evaluate our method in various different 3D objects and under many different edits. The reported results clearly show that our method yields high accuracy, in terms of achieving the desired edits, while at the same time preserving the geometry outside the interaction areas.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Tzathas, Petros and Maragos, Petros and Roussos, Anastasios},
	month = jan,
	year = {2023},
	note = {arXiv:2209.13971 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{xu_novel_2023,
	title = {Novel {Intensity} {Mapping} {Functions}: {Weighted} {Histogram} {Averaging}},
	shorttitle = {Novel {Intensity} {Mapping} {Functions}},
	url = {http://arxiv.org/abs/2111.07283},
	doi = {10.48550/arXiv.2111.07283},
	abstract = {It is challenging to align the brightness distribution of the images with different exposures due to possible color distortion and loss of details in the brightest and darkest regions of input images. In this paper, a novel intensity mapping algorithm is first proposed by introducing a new concept of weighted histogram averaging (WHA). The proposed WHA algorithm leverages the correspondence between the histogram bins of two images which are built up by using the non-decreasing property of the intensity mapping functions (IMFs). Extensive experiments indicate that the proposed WHA algorithm significantly surpasses the related state-of-the-art intensity mapping methods.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Xu, Yilun and Li, Zhengguo and Chen, Weihai and Wen, Changyun},
	month = jan,
	year = {2023},
	note = {arXiv:2111.07283 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shim_snerl_2023,
	title = {{SNeRL}: {Semantic}-aware {Neural} {Radiance} {Fields} for {Reinforcement} {Learning}},
	shorttitle = {{SNeRL}},
	url = {http://arxiv.org/abs/2301.11520},
	doi = {10.48550/arXiv.2301.11520},
	abstract = {As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Shim, Dongseok and Lee, Seungjae and Kim, H. Jin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11520 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ahmadi_hdpv-slam_2023,
	title = {{HDPV}-{SLAM}: {Hybrid} {Depth}-augmented {Panoramic} {Visual} {SLAM} for {Mobile} {Mapping} {System} with {Tilted} {LiDAR} and {Panoramic} {Visual} {Camera}},
	shorttitle = {{HDPV}-{SLAM}},
	url = {http://arxiv.org/abs/2301.11823},
	doi = {10.48550/arXiv.2301.11823},
	abstract = {This paper proposes a novel visual simultaneous localization and mapping (SLAM), called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM), generating accurate and metrically scaled vehicle trajectories using a panoramic camera and a titled multi-beam LiDAR scanner. RGB-D SLAM served as the design foundation for HDPV-SLAM, adding depth information to visual features. It seeks to overcome the two problems that limit the performance of RGB-D SLAM systems. The first barrier is the sparseness of LiDAR depth, which makes it challenging to connect it with visual features extracted from the RGB image. We address this issue by proposing a depth estimation module for iteratively densifying sparse LiDAR depth based on deep learning (DL). The second issue relates to the challenges in the depth association caused by a significant deficiency of horizontal overlapping coverage between the panoramic camera and the tilted LiDAR sensor. To overcome this difficulty, we present a hybrid depth association module that optimally combines depth information estimated by two independent procedures, feature triangulation and depth estimation. This hybrid depth association module intends to maximize the use of more accurate depth information between the triangulated depth with visual features tracked and the DL-based corrected depth during a phase of feature tracking. We assessed HDPV-SLAM's performance using the 18.95 km-long York University and Teledyne Optech (YUTO) MMS dataset. Experimental results demonstrate that the proposed two modules significantly contribute to HDPV-SLAM's performance, which outperforms the state-of-the-art (SOTA) SLAM systems.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Ahmadi, Mostafa and Naeini, Amin Alizadeh and Arjmandi, Zahra and Zhang, Yujia and Sheikholeslami, Mohammad Moein and Sohn, Gunho},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11823 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{rana_inter-view_2023,
	title = {Inter-{View} {Depth} {Consistency} {Testing} in {Depth} {Difference} {Subspace}},
	url = {http://arxiv.org/abs/2301.11752},
	doi = {10.48550/arXiv.2301.11752},
	abstract = {Multiview depth imagery will play a critical role in free-viewpoint television. This technology requires high quality virtual view synthesis to enable viewers to move freely in a dynamic real world scene. Depth imagery at different viewpoints is used to synthesize an arbitrary number of novel views. Usually, depth images at multiple viewpoints are estimated individually by stereo-matching algorithms, and hence, show lack of interview consistency. This inconsistency affects the quality of view synthesis negatively. This paper proposes a method for depth consistency testing in depth difference subspace to enhance the depth representation of a scene across multiple viewpoints. Furthermore, we propose a view synthesis algorithm that uses the obtained consistency information to improve the visual quality of virtual views at arbitrary viewpoints. Our method helps us to find a linear subspace for our depth difference measurements in which we can test the inter-view consistency efficiently. With this, our approach is able to enhance the depth information for real world scenes. In combination with our consistency-adaptive view synthesis, we improve the visual experience of the free-viewpoint user. The experiments show that our approach enhances the objective quality of virtual views by up to 1.4 dB. The advantage for the subjective quality is also demonstrated.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Rana, Pravin Kumar and Flierl, Markus},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11752 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{zhang_3dshape2vecset_2023,
	title = {{3DShape2VecSet}: {A} {3D} {Shape} {Representation} for {Neural} {Fields} and {Generative} {Diffusion} {Models}},
	shorttitle = {{3DShape2VecSet}},
	url = {http://arxiv.org/abs/2301.11445},
	doi = {10.48550/arXiv.2301.11445},
	abstract = {We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Zhang, Biao and Tang, Jiapeng and Niessner, Matthias and Wonka, Peter},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11445 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{gauthier_alien_2023,
	title = {Alien {Coding}},
	url = {http://arxiv.org/abs/2301.11479},
	doi = {10.48550/arXiv.2301.11479},
	abstract = {We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Gauthier, Thibault and Olšák, Miroslav and Urban, Josef},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11479 [cs, math]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Number Theory},
}

@inproceedings{ansari_wireless_2019,
	title = {Wireless {Software} {Synchronization} of {Multiple} {Distributed} {Cameras}},
	doi = {10.1109/ICCPHOT.2019.8747340},
	abstract = {We present a method for precisely time-synchronizing the capture of image sequences from a collection of smartphone cameras connected over WiFi. Our method is entirely software-based, has only modest hardware requirements, and achieves an accuracy of less than 250 µs on unmodified commodity hardware. It does not use image content and synchronizes cameras prior to capture. The algorithm operates in two stages. In the first stage, we designate one device as the leader and synchronize each client device’s clock to it by estimating network delay. Once clocks are synchronized, the second stage initiates continuous image streaming, estimates the relative phase of image timestamps between each client and the leader, and shifts the streams into alignment. We quantitatively validate our results on a multi-camera rig imaging a high-precision LED array and qualitatively demonstrate significant improvements to multi-view stereo depth estimation and stitching of dynamic scenes. We release as open source libsoftwaresync, an Android implementation of our system, to inspire new types of collective capture applications.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	author = {Ansari, Sameer and Wadhwa, Neal and Garg, Rahul and Chen, Jiawen},
	month = may,
	year = {2019},
	note = {ISSN: 2472-7636},
	keywords = {/unread, Cameras, Clocks, Hardware, Software, Streaming media, Synchronization, Wireless fidelity, camera synchronization, computational photography, multi-view stereo, rolling shutter, wireless},
	pages = {1--9},
}

@inproceedings{elhayek_spatio-temporal_2012,
	title = {Spatio-temporal motion tracking with unsynchronized cameras},
	doi = {10.1109/CVPR.2012.6247886},
	abstract = {We present a new spatio-temporal method for markerless motion capture. We reconstruct the pose and motion of a character from a multi-view video sequence without requiring the cameras to be synchronized and without aligning captured frames in time. By formulating the model-to-image similarity measure as a temporally continuous functional, we are also able to reconstruct motion in much higher temporal detail than was possible with previous synchronized approaches. By purposefully running cameras unsynchronized we can capture even very fast motion at speeds that off-the-shelf but high quality cameras provide.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Elhayek, A. and Stoll, C. and Hasler, N. and Kim, K. I. and Seidel, H.-P. and Theobalt, C.},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Cameras, Image color analysis, Joints, Motion segmentation, Streaming media, Synchronization, Tracking},
	pages = {1870--1877},
}

@inproceedings{zhou_nerd_2021,
	title = {{NeRD}: {Neural} {3D} {Reflection} {Symmetry} {Detector}},
	shorttitle = {{NeRD}},
	doi = {10.1109/CVPR46437.2021.01568},
	abstract = {Recent advances have shown that symmetry, a structural prior that most objects exhibit, can support a variety of single-view 3D understanding tasks. However, detecting 3D symmetry from an image remains a challenging task. Previous works either assume the symmetry is given or detect the symmetry with a heuristic-based method. In this paper, we present NeRD, a Neural 3D Reflection Symmetry Detector, which combines the strength of learning-based recognition and geometry-based reconstruction to accurately recover the normal direction of objects’ mirror planes. Specifically, we enumerate the symmetry planes with a coarse-to-fine strategy and find the best ones by building 3D cost volumes to examine the intra-image pixel correspondence from the symmetry. Our experiments show that the symmetry planes detected with our method are significantly more accurate than the planes from direct CNN regression on both synthetic and real datasets. More importantly, we also demonstrate that the detected symmetry can be used to improve the performance of downstream tasks such as pose estimation and depth map regression by a wide margin over existing methods. The code of this paper has been made public at https://github.com/zhou13/nerd.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhou, Yichao and Liu, Shichen and Ma, Yi},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer vision, Costs, Detectors, Pattern recognition, Pose estimation, Reflection, Three-dimensional displays},
	pages = {15935--15944},
}

@inproceedings{moreau_imposing_2023,
	title = {{ImPosing}: {Implicit} {Pose} {Encoding} for {Efficient} {Visual} {Localization}},
	shorttitle = {{ImPosing}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Moreau_ImPosing_Implicit_Pose_Encoding_for_Efficient_Visual_Localization_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-01-19},
	author = {Moreau, Arthur and Gilles, Thomas and Piasco, Nathan and Tsishkou, Dzmitry and Stanciulescu, Bogdan and de La Fortelle, Arnaud},
	year = {2023},
	keywords = {/unread},
	pages = {2892--2902},
}

@misc{liu_robust_2023,
	title = {Robust {Dynamic} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2301.02239},
	doi = {10.48550/arXiv.2301.02239},
	abstract = {Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Liu, Yu-Lun and Gao, Chen and Meuleman, Andreas and Tseng, Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and Kopf, Johannes and Huang, Jia-Bin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.02239 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tian_mononerf_2022,
	title = {{MonoNeRF}: {Learning} a {Generalizable} {Dynamic} {Radiance} {Field} from {Monocular} {Videos}},
	shorttitle = {{MonoNeRF}},
	url = {http://arxiv.org/abs/2212.13056},
	doi = {10.48550/arXiv.2212.13056},
	abstract = {In this paper, we target at the problem of learning a generalizable dynamic radiance field from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambiguity along the view direction in estimating point features and scene flows. Previous studies such as DynNeRF disambiguate point features by positional encoding, which is not transferable and severely limits the generalization ability. As a result, these methods have to train one independent model for each scene and suffer from heavy computational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene flows with point trajectory and feature correspondence constraints across frames. More specifically, we learn an implicit velocity field to estimate point trajectory from temporal features with Neural ODE, which is followed by a flow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features by training the network in an end-to-end manner. Experiments show that our MonoNeRF is able to learn from multiple scenes and support new applications such as scene editing, unseen frame synthesis, and fast novel scene adaptation.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Tian, Fengrui and Du, Shaoyi and Duan, Yueqi},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13056 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{yuan_neural_2022,
	title = {Neural {Radiance} {Fields} from {Sparse} {RGB}-{D} {Images} for {High}-{Quality} {View} {Synthesis}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3232502},
	abstract = {The recently proposed neural radiance fields (NeRF) use a continuous function formulated as a multi-layer perceptron (MLP) to model the appearance and geometry of a 3D scene. This enables realistic synthesis of novel views, even for scenes with view dependent appearance. Many follow-up works have since extended NeRFs in different ways. However, a fundamental restriction of the method remains that it requires a large number of images captured from densely placed viewpoints for high-quality synthesis and the quality of the results quickly degrades when the number of captured views is insufficient. To address this problem, we propose a novel NeRF-based framework capable of high-quality view synthesis using only a sparse set of RGB-D images, which can be easily captured using cameras and LiDAR sensors on current consumer devices. First, a geometric proxy of the scene is reconstructed from the captured RGB-D images. Renderings of the reconstructed scene along with precise camera parameters can then be used to pre-train a network. Finally, the network is fine-tuned with a small number of real captured images. We further introduce a patch discriminator to supervise the network under novel views during fine-tuning, as well as a 3D color prior to improve synthesis quality. We demonstrate that our method can generate arbitrary novel views of a 3D scene from as few as 6 RGB-D images. Extensive experiments show the improvements of our method compared with the existing NeRF-based methods, including approaches that also aim to reduce the number of input images.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yuan, Yu-Jie and Lai, Yu-Kun and Huang, Yi-Hua and Kobbelt, Leif and Gao, Lin},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {/unread, Cameras, Geometry, Image reconstruction, Neural Radiance Fields, Neural Rendering, Novel View Synthesis, Rendering (computer graphics), Task analysis, Three-dimensional displays, Training},
	pages = {1--16},
}

@misc{guo_incremental_2022,
	title = {Incremental {Learning} for {Neural} {Radiance} {Field} with {Uncertainty}-{Filtered} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2212.10950},
	doi = {10.48550/arXiv.2212.10950},
	abstract = {Recent neural radiance field (NeRF) representation has achieved great success in the tasks of novel view synthesis and 3D reconstruction. However, they suffer from the catastrophic forgetting problem when continuously learning from streaming data without revisiting the previous training data. This limitation prohibits the application of existing NeRF models to scenarios where images come in sequentially. In view of this, we explore the task of incremental learning for neural radiance field representation in this work. We first propose a student-teacher pipeline to mitigate the catastrophic forgetting problem. Specifically, we iterate the process of using the student as the teacher at the end of each incremental step and let the teacher guide the training of the student in the next step. In this way, the student network is able to learn new information from the streaming data and retain old knowledge from the teacher network simultaneously. Given that not all information from the teacher network is helpful since it is only trained with the old data, we further introduce a random inquirer and an uncertainty-based filter to filter useful information. We conduct experiments on the NeRF-synthetic360 and NeRF-real360 datasets, where our approach significantly outperforms the baselines by 7.3\% and 25.2\% in terms of PSNR. Furthermore, we also show that our approach can be applied to the large-scale camera facing-outwards dataset ScanNet, where we surpass the baseline by 60.0\% in PSNR.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Guo, Mengqi and Li, Chen and Lee, Gim Hee},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10950 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{venkat_geometry-biased_2023,
	title = {Geometry-biased {Transformers} for {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2301.04650},
	doi = {10.48550/arXiv.2301.04650},
	abstract = {We tackle the task of synthesizing novel views of an object given a few input images and associated camera viewpoints. Our work is inspired by recent 'geometry-free' approaches where multi-view images are encoded as a (global) set-latent representation, which is then used to predict the color for arbitrary query rays. While this representation yields (coarsely) accurate images corresponding to novel viewpoints, the lack of geometric reasoning limits the quality of these outputs. To overcome this limitation, we propose 'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive biases in the set-latent representation-based inference to encourage multi-view geometric consistency. We induce the geometric bias by augmenting the dot-product attention mechanism to also incorporate 3D distances between rays associated with tokens as a learnable bias. We find that this, along with camera-aware embeddings as input, allows our models to generate significantly more accurate outputs. We validate our approach on the real-world CO3D dataset, where we train our system over 10 categories and evaluate its view-synthesis ability for novel objects as well as unseen categories. We empirically validate the benefits of the proposed geometric biases and show that our approach significantly improves over prior works.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Venkat, Naveen and Agarwal, Mayank and Singh, Maneesh and Tulsiani, Shubham},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shi_learning_2023,
	title = {Learning {3D}-aware {Image} {Synthesis} with {Unknown} {Pose} {Distribution}},
	url = {http://arxiv.org/abs/2301.07702},
	doi = {10.48550/arXiv.2301.07702},
	abstract = {Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes PoF3D that frees generative radiance fields from the requirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized images with the predicted pose as the condition. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality, is on par with state of the art. To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Shi, Zifan and Shen, Yujun and Xu, Yinghao and Peng, Sida and Liao, Yiyi and Guo, Sheng and Chen, Qifeng and Yeung, Dit-Yan},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07702 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{iftekhar_dds_2023,
	title = {{DDS}: {Decoupled} {Dynamic} {Scene}-{Graph} {Generation} {Network}},
	shorttitle = {{DDS}},
	url = {http://arxiv.org/abs/2301.07666},
	doi = {10.48550/arXiv.2301.07666},
	abstract = {Scene-graph generation involves creating a structural representation of the relationships between objects in a scene by predicting subject-object-relation triplets from input data. However, existing methods show poor performance in detecting triplets outside of a predefined set, primarily due to their reliance on dependent feature learning. To address this issue we propose DDS -- a decoupled dynamic scene-graph generation network -- that consists of two independent branches that can disentangle extracted features. The key innovation of the current paper is the decoupling of the features representing the relationships from those of the objects, which enables the detection of novel object-relationship combinations. The DDS model is evaluated on three datasets and outperforms previous methods by a significant margin, especially in detecting previously unseen triplets.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Iftekhar, A. S. M. and Ruschel, Raphael and Kumar, Satish and You, Suya and Manjunath, B. S.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07666 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{meli_logic_2023,
	title = {Logic programming for deliberative robotic task planning},
	issn = {0269-2821, 1573-7462},
	url = {http://arxiv.org/abs/2301.07550},
	doi = {10.1007/s10462-022-10389-w},
	abstract = {Over the last decade, the use of robots in production and daily life has increased. With increasingly complex tasks and interaction in different environments including humans, robots are required a higher level of autonomy for efficient deliberation. Task planning is a key element of deliberation. It combines elementary operations into a structured plan to satisfy a prescribed goal, given specifications on the robot and the environment. In this manuscript, we present a survey on recent advances in the application of logic programming to the problem of task planning. Logic programming offers several advantages compared to other approaches, including greater expressivity and interpretability which may aid in the development of safe and reliable robots. We analyze different planners and their suitability for specific robotic applications, based on expressivity in domain representation, computational efficiency and software implementation. In this way, we support the robotic designer in choosing the best tool for his application.},
	urldate = {2023-01-19},
	journal = {Artificial Intelligence Review},
	author = {Meli, Daniele and Nakawala, Hirenkumar and Fiorini, Paolo},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07550 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Robotics},
}

@misc{jiang_continuous_2023,
	title = {Continuous {Trajectory} {Generation} {Based} on {Two}-{Stage} {GAN}},
	url = {http://arxiv.org/abs/2301.07103},
	doi = {10.48550/arXiv.2301.07103},
	abstract = {Simulating the human mobility and generating large-scale trajectories are of great use in many real-world applications, such as urban planning, epidemic spreading analysis, and geographic privacy protect. Although many previous works have studied the problem of trajectory generation, the continuity of the generated trajectories has been neglected, which makes these methods useless for practical urban simulation scenarios. To solve this problem, we propose a novel two-stage generative adversarial framework to generate the continuous trajectory on the road network, namely TS-TrajGen, which efficiently integrates prior domain knowledge of human mobility with model-free learning paradigm. Specifically, we build the generator under the human mobility hypothesis of the A* algorithm to learn the human mobility behavior. For the discriminator, we combine the sequential reward with the mobility yaw reward to enhance the effectiveness of the generator. Finally, we propose a novel two-stage generation process to overcome the weak point of the existing stochastic generation process. Extensive experiments on two real-world datasets and two case studies demonstrate that our framework yields significant improvements over the state-of-the-art methods.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Jiang, Wenjun and Zhao, Wayne Xin and Wang, Jingyuan and Jiang, Jiawei},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07103 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zhong_shine-mapping_2022,
	title = {{SHINE}-{Mapping}: {Large}-{Scale} {3D} {Mapping} {Using} {Sparse} {Hierarchical} {Implicit} {Neural} {Representations}},
	shorttitle = {{SHINE}-{Mapping}},
	url = {http://arxiv.org/abs/2210.02299},
	doi = {10.48550/arXiv.2210.02299},
	abstract = {Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems. Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy. This paper addresses the problems of achieving large-scale 3D reconstructions with implicit representations using 3D LiDAR measurements. We learn and store implicit features through an octree-based hierarchical structure, which is sparse and extensible. The features can be turned into signed distance values through a shallow neural network. We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision. Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of catastrophic forgetting in continual learning. Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02299 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Done Reading},
}

@inproceedings{oechsle_unisurf_2021,
	title = {{UNISURF}: {Unifying} {Neural} {Implicit} {Surfaces} and {Radiance} {Fields} for {Multi}-{View} {Reconstruction}},
	shorttitle = {{UNISURF}},
	doi = {10.1109/ICCV48922.2021.00554},
	abstract = {Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF’s estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Oechsle, Michael and Peng, Songyou and Geiger, Andreas},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from a single image and shape-from-x, 3D from multiview and other sensors, Computer vision, Done Reading, Image reconstruction, Rendering (computer graphics), Solid modeling, Stereo, Surface reconstruction, Three-dimensional displays},
	pages = {5569--5579},
}

@misc{moreno_laser_2023,
	title = {Laser: {Latent} {Set} {Representations} for {3D} {Generative} {Modeling}},
	shorttitle = {Laser},
	url = {http://arxiv.org/abs/2301.05747},
	doi = {10.48550/arXiv.2301.05747},
	abstract = {NeRF provides unparalleled fidelity of novel view synthesis: rendering a 3D scene from an arbitrary viewpoint. NeRF requires training on a large number of views that fully cover a scene, which limits its applicability. While these issues can be addressed by learning a prior over scenes in various forms, previous approaches have been either applied to overly simple scenes or struggling to render unobserved parts. We introduce Laser-NV: a generative model which achieves high modelling capacity, and which is based on a set-valued latent representation modelled by normalizing flows. Similarly to previous amortized approaches, Laser-NV learns structure from multiple scenes and is capable of fast, feed-forward inference from few views. To encourage higher rendering fidelity and consistency with observed views, Laser-NV further incorporates a geometry-informed attention mechanism over the observed views. Laser-NV further produces diverse and plausible completions of occluded parts of a scene while remaining consistent with observations. Laser-NV shows state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on a novel simulated City dataset, which features high uncertainty in the unobserved regions of the scene.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moreno, Pol and Kosiorek, Adam R. and Strathmann, Heiko and Zoran, Daniel and Schneider, Rosalia G. and Winckler, Björn and Markeeva, Larisa and Weber, Théophane and Rezende, Danilo J.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05747 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lu_large-scale_2023,
	title = {A {Large}-{Scale} {Outdoor} {Multi}-modal {Dataset} and {Benchmark} for {Novel} {View} {Synthesis} and {Implicit} {Scene} {Reconstruction}},
	url = {http://arxiv.org/abs/2301.06782},
	doi = {10.48550/arXiv.2301.06782},
	abstract = {Neural Radiance Fields (NeRF) has achieved impressive results in single object scene reconstruction and novel view synthesis, which have been demonstrated on many single modality and single object focused indoor scene datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on large-scale outdoor scene reconstruction is still limited, as there is no unified outdoor scene dataset for large-scale NeRF evaluation due to expensive data acquisition and calibration costs. In this paper, we propose a large-scale outdoor multi-modal dataset, OMMO dataset, containing complex land objects and scenes with calibrated images, point clouds and prompt annotations. Meanwhile, a new benchmark for several outdoor NeRF-based tasks is established, such as novel view synthesis, surface reconstruction, and multi-modal NeRF. To create the dataset, we capture and collect a large number of real fly-view videos and select high-quality and high-resolution clips from them. Then we design a quality review module to refine images, remove low-quality frames and fail-to-calibrate scenes through a learning-based automatic evaluation plus manual review. Finally, a number of volunteers are employed to add the text descriptions for each scene and key-frame to meet the potential multi-modal requirements in the future. Compared with existing NeRF datasets, our dataset contains abundant real-world urban and natural scenes with various scales, camera trajectories, and lighting conditions. Experiments show that our dataset can benchmark most state-of-the-art NeRF methods on different tasks. We will release the dataset and model weights very soon.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Lu, Chongshan and Yin, Fukun and Chen, Xin and Chen, Tao and YU, Gang and Fan, Jiayuan},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06782 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_dynamic_2023,
	title = {Dynamic {Local} {Feature} {Aggregation} for {Learning} on {Point} {Clouds}},
	url = {http://arxiv.org/abs/2301.02836},
	doi = {10.48550/arXiv.2301.02836},
	abstract = {Existing point cloud learning methods aggregate features from neighbouring points relying on constructing graph in the spatial domain, which results in feature update for each point based on spatially-fixed neighbours throughout layers. In this paper, we propose a dynamic feature aggregation (DFA) method that can transfer information by constructing local graphs in the feature domain without spatial constraints. By finding k-nearest neighbors in the feature domain, we perform relative position encoding and semantic feature encoding to explore latent position and feature similarity information, respectively, so that rich local features can be learned. At the same time, we also learn low-dimensional global features from the original point cloud for enhancing feature representation. Between DFA layers, we dynamically update the constructed local graph structure, so that we can learn richer information, which greatly improves adaptability and efficiency. We demonstrate the superiority of our method by conducting extensive experiments on point cloud classification and segmentation tasks. Implementation code is available: https://github.com/jiamang/DFA.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Li, Zihao and Gao, Pan and Yuan, Hui and Wei, Ran},
	month = jan,
	year = {2023},
	note = {arXiv:2301.02836 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_benchmarking_2023,
	title = {Benchmarking {Robustness} in {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2301.04075},
	doi = {10.48550/arXiv.2301.04075},
	abstract = {Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view synthesis, thanks to its ability to model 3D object geometries in a concise formulation. However, current approaches to NeRF-based models rely on clean images with accurate camera calibration, which can be difficult to obtain in the real world, where data is often subject to corruption and distortion. In this work, we provide the first comprehensive analysis of the robustness of NeRF-based novel view synthesis algorithms in the presence of different types of corruptions. We find that NeRF-based models are significantly degraded in the presence of corruption, and are more sensitive to a different set of corruptions than image recognition models. Furthermore, we analyze the robustness of the feature encoder in generalizable methods, which synthesize images using neural features extracted via convolutional neural networks or transformers, and find that it only contributes marginally to robustness. Finally, we reveal that standard data augmentation techniques, which can significantly improve the robustness of recognition models, do not help the robustness of NeRF-based models. We hope that our findings will attract more researchers to study the robustness of NeRF-based approaches and help to improve their performance in the real world.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Wang, Chen and Wang, Angtian and Li, Junbo and Yuille, Alan and Xie, Cihang},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04075 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{saragadam_wire_2023,
	title = {{WIRE}: {Wavelet} {Implicit} {Neural} {Representations}},
	shorttitle = {{WIRE}},
	url = {http://arxiv.org/abs/2301.05187},
	doi = {10.48550/arXiv.2301.05187},
	abstract = {Implicit neural representations (INRs) have recently advanced numerous vision-related areas. INR performance depends strongly on the choice of the nonlinear activation function employed in its multilayer perceptron (MLP) network. A wide range of nonlinearities have been explored, but, unfortunately, current INRs designed to have high accuracy also suffer from poor robustness (to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we develop a new, highly accurate and robust INR that does not exhibit this tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous complex Gabor wavelet activation function that is well-known to be optimally concentrated in space-frequency and to have excellent biases for representing images. A wide range of experiments (image denoising, image inpainting, super-resolution, computed tomography reconstruction, image overfitting, and novel view synthesis with neural radiance fields) demonstrate that WIRE defines the new state of the art in INR accuracy, training time, and robustness.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Saragadam, Vishwanath and LeJeune, Daniel and Tan, Jasper and Balakrishnan, Guha and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05187 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{venkat_geometry-biased_2023-1,
	title = {Geometry-biased {Transformers} for {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2301.04650},
	doi = {10.48550/arXiv.2301.04650},
	abstract = {We tackle the task of synthesizing novel views of an object given a few input images and associated camera viewpoints. Our work is inspired by recent 'geometry-free' approaches where multi-view images are encoded as a (global) set-latent representation, which is then used to predict the color for arbitrary query rays. While this representation yields (coarsely) accurate images corresponding to novel viewpoints, the lack of geometric reasoning limits the quality of these outputs. To overcome this limitation, we propose 'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive biases in the set-latent representation-based inference to encourage multi-view geometric consistency. We induce the geometric bias by augmenting the dot-product attention mechanism to also incorporate 3D distances between rays associated with tokens as a learnable bias. We find that this, along with camera-aware embeddings as input, allows our models to generate significantly more accurate outputs. We validate our approach on the real-world CO3D dataset, where we train our system over 10 categories and evaluate its view-synthesis ability for novel objects as well as unseen categories. We empirically validate the benefits of the proposed geometric biases and show that our approach significantly improves over prior works.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Venkat, Naveen and Agarwal, Mayank and Singh, Maneesh and Tulsiani, Shubham},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_tensorf_2022,
	address = {Berlin, Heidelberg},
	title = {{TensoRF}: {Tensorial} {Radiance} {Fields}},
	isbn = {978-3-031-19823-6},
	shorttitle = {{TensoRF}},
	url = {https://doi.org/10.1007/978-3-031-19824-3_20},
	doi = {10.1007/978-3-031-19824-3_20},
	abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CANDECOMP/PARAFAC (CP) decomposition – that factorizes tensors into rank-one components with compact vectors – in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ({\textless}30 min) with better rendering quality and even a smaller model size ({\textless}4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ({\textless}10 min) and retaining a compact model size ({\textless}75 MB).},
	urldate = {2022-11-28},
	booktitle = {Computer {Vision} – {ECCV} 2022: 17th {European} {Conference}, {Tel} {Aviv}, {Israel}, {October} 23–27, 2022, {Proceedings}, {Part} {XXXII}},
	publisher = {Springer-Verlag},
	author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
	year = {2022},
	keywords = {/unread, Done Reading},
	pages = {333--350},
}

@misc{tagliasacchi_volume_2022,
	title = {Volume {Rendering} {Digest} (for {NeRF})},
	url = {http://arxiv.org/abs/2209.02417},
	doi = {10.48550/arXiv.2209.02417},
	abstract = {Neural Radiance Fields employ simple volume rendering as a way to overcome the challenges of differentiating through ray-triangle intersections by leveraging a probabilistic notion of visibility. This is achieved by assuming the scene is composed by a cloud of light-emitting particles whose density changes in space. This technical report summarizes the derivations for differentiable volume rendering. It is a condensed version of previous reports, but rewritten in the context of NeRF, and adopting its commonly used notation.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Tagliasacchi, Andrea and Mildenhall, Ben},
	month = aug,
	year = {2022},
	note = {arXiv:2209.02417 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, Emphasis, I.3, I.4},
}

@inproceedings{xu_point-nerf_2022,
	title = {Point-{NeRF}: {Point}-based {Neural} {Radiance} {Fields}},
	shorttitle = {Point-{NeRF}},
	doi = {10.1109/CVPR52688.2022.00536},
	abstract = {Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computational photography, Done Reading, Image and video synthesis and generation, Pipelines, Point cloud compression, Rendering (computer graphics), Solid modeling, Surface reconstruction, Three-dimensional displays, Training},
	pages = {5428--5438},
}

@inproceedings{fridovich-keil_plenoxels_2022,
	title = {Plenoxels: {Radiance} {Fields} without {Neural} {Networks}},
	shorttitle = {Plenoxels},
	doi = {10.1109/CVPR52688.2022.00542},
	abstract = {We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Fridovich-Keil, Sara and Yu, Alex and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Benchmark testing, Codes, Computer vision, Done Reading, Efficient learning and inferences, Gradient methods, Neural networks, Optimization methods, Three-dimensional displays, Vision + graphics, Visualization},
	pages = {5491--5500},
}

@misc{bian_nope-nerf_2022,
	title = {{NoPe}-{NeRF}: {Optimising} {Neural} {Radiance} {Field} with {No} {Pose} {Prior}},
	shorttitle = {{NoPe}-{NeRF}},
	url = {http://arxiv.org/abs/2212.07388},
	doi = {10.48550/arXiv.2212.07388},
	abstract = {Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Bian, Wenjing and Wang, Zirui and Li, Kejie and Bian, Jia-Wang and Prisacariu, Victor Adrian},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07388 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading},
}

@misc{zhang_nerfusion_2022,
	title = {{NeRFusion}: {Fusing} {Radiance} {Fields} for {Large}-{Scale} {Scene} {Reconstruction}},
	shorttitle = {{NeRFusion}},
	url = {http://arxiv.org/abs/2203.11283},
	doi = {10.48550/arXiv.2203.11283},
	abstract = {While NeRF has shown great success for neural reconstruction and rendering, its limited MLP capacity and long per-scene optimization times make it challenging to model large-scale indoor scenes. In contrast, classical 3D reconstruction methods can handle large-scale scenes but do not produce realistic renderings. We propose NeRFusion, a method that combines the advantages of NeRF and TSDF-based fusion techniques to achieve efficient large-scale reconstruction and photo-realistic rendering. We process the input image sequence to predict per-frame local radiance fields via direct network inference. These are then fused using a novel recurrent neural network that incrementally reconstructs a global, sparse scene representation in real-time at 22 fps. This global volume can be further fine-tuned to boost rendering quality. We demonstrate that NeRFusion achieves state-of-the-art quality on both large-scale indoor and small-scale object scenes, with substantially faster reconstruction than NeRF and other recent methods.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Zhang, Xiaoshuai and Bi, Sai and Sunkavalli, Kalyan and Su, Hao and Xu, Zexiang},
	month = mar,
	year = {2022},
	note = {arXiv:2203.11283 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading},
}

@misc{zhang_nerf_2020,
	title = {{NeRF}++: {Analyzing} and {Improving} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}++},
	url = {http://arxiv.org/abs/2010.07492},
	doi = {10.48550/arXiv.2010.07492},
	abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	month = oct,
	year = {2020},
	note = {arXiv:2010.07492 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading},
}

@inproceedings{yu_monosdf_2022,
	title = {{MonoSDF}: {Exploring} {Monocular} {Geometric} {Cues} for {Neural} {Implicit} {Surface} {Reconstruction}},
	shorttitle = {{MonoSDF}},
	url = {https://openreview.net/forum?id=dMK7EwoTYp},
	abstract = {In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.},
	language = {en},
	urldate = {2022-11-05},
	author = {Yu, Zehao and Peng, Songyou and Niemeyer, Michael and Sattler, Torsten and Geiger, Andreas},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Czech Technical University, Done Reading, ETH, Max Planck, Monocular depth estimate, NIPS 2022, Shared, University of Tubingen, normal prior},
}

@misc{yifan_iso-points_2021,
	title = {Iso-{Points}: {Optimizing} {Neural} {Implicit} {Surfaces} with {Hybrid} {Representations}},
	shorttitle = {🔤等值点：使用混合表示优化神经隐式曲面},
	url = {http://arxiv.org/abs/2012.06434},
	doi = {10.48550/arXiv.2012.06434},
	abstract = {Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use {\textbackslash}emph\{iso-points\} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Yifan, Wang and Wu, Shihao and Oztireli, Cengiz and Sorkine-Hornung, Olga},
	month = apr,
	year = {2021},
	note = {arXiv:2012.06434 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, I.3.7, I.4.5},
}

@misc{xiangli_bungeenerf_2022,
	title = {{BungeeNeRF}: {Progressive} {Neural} {Radiance} {Field} for {Extreme} {Multi}-scale {Scene} {Rendering}},
	shorttitle = {{BungeeNeRF}},
	url = {http://arxiv.org/abs/2112.05504},
	doi = {10.48550/arXiv.2112.05504},
	abstract = {Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in NeRF's positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Xiangli, Yuanbo and Xu, Linning and Pan, Xingang and Zhao, Nanxuan and Rao, Anyi and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	month = jul,
	year = {2022},
	note = {arXiv:2112.05504 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Done Reading},
}

@misc{wu_asyncnerf_2022,
	title = {{AsyncNeRF}: {Learning} {Large}-scale {Radiance} {Fields} from {Asynchronous} {RGB}-{D} {Sequences} with {Time}-{Pose} {Function}},
	copyright = {All rights reserved},
	shorttitle = {{AsyncNeRF}},
	url = {http://arxiv.org/abs/2211.07459},
	doi = {10.48550/arXiv.2211.07459},
	abstract = {Large-scale radiance fields are promising mapping tools for smart transportation applications like autonomous driving or drone delivery. But for large-scale scenes, compact synchronized RGB-D cameras are not applicable due to limited sensing range, and using separate RGB and depth sensors inevitably leads to unsynchronized sequences. Inspired by the recent success of self-calibrating radiance field training methods that do not require known intrinsic or extrinsic parameters, we propose the first solution that self-calibrates the mismatch between RGB and depth frames. We leverage the important domain-specific fact that RGB and depth frames are actually sampled from the same trajectory and develop a novel implicit network called the time-pose function. Combining it with a large-scale radiance field leads to an architecture that cascades two implicit representation networks. To validate its effectiveness, we construct a diverse and photorealistic dataset that covers various RGB-D mismatch scenarios. Through a comprehensive benchmarking on this dataset, we demonstrate the flexibility of our method in different scenarios and superior performance over applicable prior counterparts. Codes, data, and models will be made publicly available.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Wu, Zirui and Chen, Yuantao and Yang, Runyi and Zhu, Zhenxin and Hou, Chao and Shi, Yongliang and Zhao, Hao and Zhou, Guyue},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07459 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading},
}

@inproceedings{zhu_latitude_2022,
	title = {{LATITUDE}: {Robotic} {Global} {Localization} with {Truncated} {Dynamic} {Low}-pass {Filter} in {City}-scale {NeRF}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	shorttitle = {{LATITUDE}},
	url = {http://arxiv.org/abs/2209.08498},
	abstract = {Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on tangent plane. To avoid convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and data will be publicly available at https://github.com/jike5/LATITUDE.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Zhu, Zhenxin and Chen, Yuantao and Wu, Zirui and Hou, Chao and Shi, Yongliang and Li, Chuxuan and Li, Pengfei and Zhao, Hao and Zhou, Guyue},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08498 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading, ICRA, My, NeRF, Simultaneous localization and mapping},
}

@inproceedings{park_nerfies_2021,
	title = {Nerfies: {Deformable} {Neural} {Radiance} {Fields}},
	shorttitle = {Nerfies},
	doi = {10.1109/ICCV48922.2021.00581},
	abstract = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Adaptation models, Computer vision, Deformable models, Geometry, Mobile handsets, Optimization methods, Rendering (computer graphics), Stereo},
	pages = {5845--5854},
}

@misc{noauthor_addon_nodate,
	title = {Addon {Item}},
	keywords = {/unread},
}

@misc{sethuraman_waternerf_2022,
	title = {{WaterNeRF}: {Neural} {Radiance} {Fields} for {Underwater} {Scenes}},
	shorttitle = {{WaterNeRF}},
	url = {http://arxiv.org/abs/2209.13091},
	doi = {10.48550/arXiv.2209.13091},
	abstract = {Underwater imaging is a critical task performed by marine robots for a wide range of applications including aquaculture, marine infrastructure inspection, and environmental monitoring. However, water column effects, such as attenuation and backscattering, drastically change the color and quality of imagery captured underwater. Due to varying water conditions and range-dependency of these effects, restoring underwater imagery is a challenging problem. This impacts downstream perception tasks including depth estimation and 3D reconstruction. In this paper, we advance state-of-the-art in neural radiance fields (NeRFs) to enable physics-informed dense depth estimation and color correction. Our proposed method, WaterNeRF, estimates parameters of a physics-based model for underwater image formation, leading to a hybrid data-driven and model-based solution. After determining the scene structure and radiance field, we can produce novel views of degraded as well as corrected underwater images, along with dense depth of the scene. We evaluate the proposed method qualitatively and quantitatively on a real underwater dataset.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Sethuraman, Advaith Venkatramanan and Ramanagopal, Manikandasriram Srinivasan and Skinner, Katherine A.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13091 [cs, eess]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing, NeRF, Read Later},
}

@misc{long_sparseneus_2022,
	title = {{SparseNeuS}: {Fast} {Generalizable} {Neural} {Surface} {Reconstruction} from {Sparse} {Views}},
	shorttitle = {{SparseNeuS}},
	url = {http://arxiv.org/abs/2206.05737},
	doi = {10.48550/arXiv.2206.05737},
	abstract = {We introduce SparseNeuS, a novel neural rendering based method for the task of surface reconstruction from multi-view images. This task becomes more difficult when only sparse images are provided as input, a scenario where existing neural reconstruction approaches usually produce incomplete or distorted results. Moreover, their inability of generalizing to unseen new scenes impedes their application in practice. Contrarily, SparseNeuS can generalize to new scenes and work well with sparse images (as few as 2 or 3). SparseNeuS adopts signed distance function (SDF) as the surface representation, and learns generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction. Moreover, several strategies are introduced to effectively leverage sparse views for high-quality reconstruction, including 1) a multi-level geometry reasoning framework to recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color blending scheme for more reliable color prediction; 3) a consistency-aware fine-tuning scheme to control the inconsistent regions caused by occlusion and noise. Extensive experiments demonstrate that our approach not only outperforms the state-of-the-art methods, but also exhibits good efficiency, generalizability, and flexibility.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Long, Xiaoxiao and Lin, Cheng and Wang, Peng and Komura, Taku and Wang, Wenping},
	month = aug,
	year = {2022},
	note = {arXiv:2206.05737 [cs]
version: 2},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_ponder_2022,
	title = {Ponder: {Point} {Cloud} {Pre}-training via {Neural} {Rendering}},
	shorttitle = {Ponder},
	url = {http://arxiv.org/abs/2301.00157},
	doi = {10.48550/arXiv.2301.00157},
	abstract = {We propose a novel approach to self-supervised learning of point cloud representations by differentiable neural rendering. Motivated by the fact that informative point cloud features should be able to encode rich geometry and appearance cues and render realistic images, we train a point-cloud encoder within a devised point-based neural renderer by comparing the rendered images with real images on massive RGB-D data. The learned point-cloud encoder can be easily integrated into various downstream tasks, including not only high-level tasks like 3D detection and segmentation, but low-level tasks like 3D reconstruction and image synthesis. Extensive experiments on various tasks demonstrate the superiority of our approach compared to existing pre-training methods.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Huang, Di and Peng, Sida and He, Tong and Zhou, Xiaowei and Ouyang, Wanli},
	month = dec,
	year = {2022},
	note = {arXiv:2301.00157 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{charles_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	doi = {10.1109/CVPR.2017.16},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer architecture, Feature extraction, Machine learning, Semantics, Shape, Three-dimensional displays},
	pages = {77--85},
}

@inproceedings{jiang_sdfdiff_2020,
	title = {{SDFDiff}: {Differentiable} {Rendering} of {Signed} {Distance} {Fields} for {3D} {Shape} {Optimization}},
	shorttitle = {{SDFDiff}},
	url = {https://yuejiang-nj.github.io/papers/CVPR2020_SDFDiff/project_page.html},
	doi = {10.1109/CVPR42600.2020.00133},
	abstract = {We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDFs). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.},
	urldate = {2022-12-09},
	author = {Jiang, Yue and Ji, Dantong and Han, Zhizhong and Zwicker, Matthias},
	year = {2020},
	keywords = {/unread},
}

@misc{lin_parallel_2022,
	title = {Parallel {Inversion} of {Neural} {Radiance} {Fields} for {Robust} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2210.10108},
	doi = {10.48550/arXiv.2210.10108},
	abstract = {We present a parallelized optimization method based on fast Neural Radiance Fields (NeRF) for estimating 6-DoF target poses. Given a single observed RGB image of the target, we can predict the translation and rotation of the camera by minimizing the residual between pixels rendered from a fast NeRF model and pixels in the observed image. We integrate a momentum-based camera extrinsic optimization procedure into Instant Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By introducing parallel Monte Carlo sampling into the pose estimation task, our method overcomes local minima and improves efficiency in a more extensive search space. We also show the importance of adopting a more robust pixel-based loss function to reduce error. Experiments demonstrate that our method can achieve improved generalization and robustness on both synthetic and real-world benchmarks.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Lin, Yunzhi and Müller, Thomas and Tremblay, Jonathan and Wen, Bowen and Tyree, Stephen and Evans, Alex and Vela, Patricio A. and Birchfield, Stan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10108 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{tiwari_pose-ndf_2022,
	title = {Pose-{NDF}: {Modeling} {Human} {Pose} {Manifolds} with {Neural} {Distance} {Fields}},
	shorttitle = {Pose-{NDF}},
	url = {http://arxiv.org/abs/2207.13807},
	doi = {10.48550/arXiv.2207.13807},
	abstract = {We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modeling implicit surfaces in 3D to the high-dimensional domain SO(3){\textasciicircum}K, where a human pose is defined by a single data point, represented by K quaternions. The resulting high-dimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that PoseNDF outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real-world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE-based methods.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Tiwari, Garvita and Antic, Dimitrije and Lenssen, Jan Eric and Sarafianos, Nikolaos and Tung, Tony and Pons-Moll, Gerard},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13807 [cs]},
	keywords = {2022, Best Paper, Computer Science - Computer Vision and Pattern Recognition, ECCV, Human Pose Manifold, Manifolds, Max Planck, Meta, Pose-NDF, University of Tubingen},
}

@inproceedings{azinovic_neural_2022,
	title = {Neural {RGB}-{D} {Surface} {Reconstruction}},
	doi = {10.1109/CVPR52688.2022.00619},
	abstract = {Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming applications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, virtual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radiance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not reconstruct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a surface is extracted using Marching Cubes, since during optimization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera re-finement technique which improves the overall reconstruction quality. In contrast to concurrent work on integrating depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Azinović, Dejan and Martin-Brualla, Ricardo and Goldman, Dan B and Nießner, Matthias and Thies, Justus},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, 3D from single images, Computer vision, Current measurement, Done Reading, Mixed reality, Planing, RGBD sensors and analytics, Surface reconstruction, Teleconferencing, Three-dimensional displays, Vision + graphics},
	pages = {6280--6291},
}

@inproceedings{sun_neuralrecon_2021,
	title = {{NeuralRecon}: {Real}-{Time} {Coherent} {3D} {Reconstruction} {From} {Monocular} {Video}},
	shorttitle = {{NeuralRecon}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-04},
	author = {Sun, Jiaming and Xie, Yiming and Chen, Linghao and Zhou, Xiaowei and Bao, Hujun},
	year = {2021},
	keywords = {/unread},
	pages = {15598--15607},
}

@misc{ueda_neural_2022,
	title = {Neural {Density}-{Distance} {Fields}},
	url = {http://arxiv.org/abs/2207.14455},
	abstract = {The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects’ surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis. The code is available at https://github.com/ueda0319/neddf.},
	language = {en},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Ueda, Itsuki and Fukuhara, Yoshihiro and Kataoka, Hirokatsu and Aizawa, Hiroaki and Shishido, Hidehiko and Kitahara, Itaru},
	month = jul,
	year = {2022},
	note = {arXiv:2207.14455 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Done Reading},
}

@inproceedings{wang_neus_2021,
	title = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
	shorttitle = {{NeuS}},
	url = {http://arxiv.org/abs/2106.10689},
	doi = {10.48550/arXiv.2106.10689},
	abstract = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	month = dec,
	year = {2021},
	note = {arXiv:2106.10689 [cs]},
	keywords = {2021, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, IDR, NeuS, NeurIPS, SDF},
}

@misc{wang_neuris_2022,
	title = {{NeuRIS}: {Neural} {Reconstruction} of {Indoor} {Scenes} {Using} {Normal} {Priors}},
	shorttitle = {{NeuRIS}},
	url = {http://arxiv.org/abs/2206.13597},
	doi = {10.48550/arXiv.2206.13597},
	abstract = {Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, named NeuRIS, for high quality reconstruction of indoor scenes. The key idea of NeuRIS is to integrate estimated normal of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, to do this in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in the regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that NeuRIS significantly outperforms the state-of-the-art methods in terms of reconstruction quality.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Wang, Jiepeng and Wang, Peng and Long, Xiaoxiao and Theobalt, Christian and Komura, Taku and Liu, Lingjie and Wang, Wenping},
	month = oct,
	year = {2022},
	note = {arXiv:2206.13597 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading, NeuS},
}

@misc{houchens_neuralodf_2022,
	title = {{NeuralODF}: {Learning} {Omnidirectional} {Distance} {Fields} for {3D} {Shape} {Representation}},
	shorttitle = {{NeuralODF}},
	url = {http://arxiv.org/abs/2206.05837},
	doi = {10.48550/arXiv.2206.05837},
	abstract = {In visual computing, 3D geometry is represented in many different forms including meshes, point clouds, voxel grids, level sets, and depth images. Each representation is suited for different tasks thus making the transformation of one representation into another (forward map) an important and common problem. We propose Omnidirectional Distance Fields (ODFs), a new 3D shape representation that encodes geometry by storing the depth to the object's surface from any 3D position in any viewing direction. Since rays are the fundamental unit of an ODF, it can be used to easily transform to and from common 3D representations like meshes or point clouds. Different from level set methods that are limited to representing closed surfaces, ODFs are unsigned and can thus model open surfaces (e.g., garments). We demonstrate that ODFs can be effectively learned with a neural network (NeuralODF) despite the inherent discontinuities at occlusion boundaries. We also introduce efficient forward mapping algorithms for transforming ODFs to and from common 3D representations. Specifically, we introduce an efficient Jumping Cubes algorithm for generating meshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture high-quality shape by overfitting to a single object, and also learn to generalize on common shape categories.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Houchens, Trevor and Lu, Cheng-You and Duggal, Shivam and Fu, Rao and Sridhar, Srinath},
	month = aug,
	year = {2022},
	note = {arXiv:2206.05837 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Distance Fields, Read Later},
}

@misc{jiang_neuman_2022,
	title = {{NeuMan}: {Neural} {Human} {Radiance} {Field} from a {Single} {Video}},
	shorttitle = {{NeuMan}},
	url = {http://arxiv.org/abs/2203.12575},
	doi = {10.48550/arXiv.2203.12575},
	abstract = {Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical pose-independent space, where we train the human model in. Our method is able to learn subject specific details, including cloth wrinkles and accessories, from just a 10 seconds video clip, and to provide high quality renderings of the human under novel poses, from novel views, together with the background.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag},
	month = sep,
	year = {2022},
	note = {arXiv:2203.12575 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_nerfactor_2021,
	title = {{NeRFactor}: neural factorization of shape and reflectance under an unknown illumination},
	volume = {40},
	issn = {0730-0301},
	shorttitle = {{NeRFactor}},
	url = {https://doi.org/10.1145/3478513.3480496},
	doi = {10.1145/3478513.3480496},
	abstract = {We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
	number = {6},
	urldate = {2022-12-27},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},
	year = {2021},
	keywords = {/unread, appearance factorization, inverse rendering, lighting estimation, material editing, reflectance estimation, relighting, shape estimation, view synthesis},
	pages = {237:1--237:18},
}

@misc{song_nerfplayer_2022,
	title = {{NeRFPlayer}: {A} {Streamable} {Dynamic} {Scene} {Representation} with {Decomposed} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRFPlayer}},
	url = {http://arxiv.org/abs/2210.15947},
	doi = {10.48550/arXiv.2210.15947},
	abstract = {Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and real-time rendering.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Song, Liangchen and Chen, Anpei and Li, Zhong and Chen, Zhang and Chen, Lele and Yuan, Junsong and Xu, Yi and Geiger, Andreas},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15947 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Multimedia, Dynamic Scene, ETH, InnoPeak, NeRF, New York State University, University at Buffalo, University of Tubingen},
}

@misc{mildenhall_nerf_2021,
	title = {{NeRF} in the {Dark}: {High} {Dynamic} {Range} {View} {Synthesis} from {Noisy} {Raw} {Images}},
	shorttitle = {🔤黑暗中的 {NeRF}：从嘈杂的原始图像合成高动态范围视图},
	url = {http://arxiv.org/abs/2111.13679},
	doi = {10.48550/arXiv.2111.13679},
	abstract = {Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul and Barron, Jonathan T.},
	month = nov,
	year = {2021},
	note = {arXiv:2111.13679 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{fan_nerf-sos_2022,
	title = {{NeRF}-{SOS}: {Any}-{View} {Self}-supervised {Object} {Segmentation} on {Complex} {Scenes}},
	shorttitle = {{NeRF}-{SOS}},
	url = {http://arxiv.org/abs/2209.08776},
	doi = {10.48550/arXiv.2209.08776},
	abstract = {Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance, without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation NeRF-SOS, couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, Tank \& Temple, and BlendedMVS datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer semantics masks than existing supervised counterparts. Please refer to the video on our project page for more details:https://zhiwenfan.github.io/NeRF-SOS.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Fan, Zhiwen and Wang, Peihao and Jiang, Yifan and Gong, Xinyu and Xu, Dejia and Wang, Zhangyang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08776 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rosinol_nerf-slam_2022,
	title = {{NeRF}-{SLAM}: {Real}-{Time} {Dense} {Monocular} {SLAM} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{SLAM}},
	url = {http://arxiv.org/abs/2210.13641},
	doi = {10.48550/arXiv.2210.13641},
	abstract = {We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images. To achieve this, we leverage recent advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular SLAM provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. With our proposed uncertainty-based depth loss, we achieve not only good photometric accuracy, but also great geometric accuracy. In fact, our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 179\% better PSNR and 86\% better L1 depth), while working in real-time and using only monocular images.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Rosinol, Antoni and Leonard, John J. and Carlone, Luca},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13641 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yuan_nerf-editing_2022,
	title = {{NeRF}-{Editing}: {Geometry} {Editing} of {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{Editing}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-25},
	author = {Yuan, Yu-Jie and Sun, Yang-Tian and Lai, Yu-Kun and Ma, Yuewen and Jia, Rongfei and Gao, Lin},
	year = {2022},
	keywords = {/unread},
	pages = {18353--18364},
}

@misc{barron_mip-nerf_2022,
	title = {Mip-{NeRF} 360: {Unbounded} {Anti}-{Aliased} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF} 360},
	url = {http://arxiv.org/abs/2111.12077},
	doi = {10.48550/arXiv.2111.12077},
	abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
	month = mar,
	year = {2022},
	note = {arXiv:2111.12077 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF}},
	doi = {10.1109/ICCV48922.2021.00580},
	abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (à la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Computational modeling, Computer vision, Error analysis, Image resolution, Low-level and physics-based vision, Multilayer perceptrons, Neural networks, Stereo, Training},
	pages = {5835--5844},
}

@inproceedings{turki_mega-nerf_2022,
	title = {Mega-{NeRF}: {Scalable} {Construction} of {Large}-{Scale} {NeRFs} for {Virtual} {Fly}- {Throughs}},
	shorttitle = {Mega-{NeRF}},
	doi = {10.1109/CVPR52688.2022.01258},
	abstract = {We use neural radiance fields (NeRFs) to build interac-tive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected pri-marily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thou-sands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) pro-hibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs. To address these challenges, we begin by analyzing visi-bility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to dif-ferent regions of the scene. We introduce a simple geomet-ric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF sub-modules that can be trained in parallel. We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12\%. We also evaluate re-cent NeRF fast renderers on top of Mega-NeRF and intro-duce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Turki, Haithem and Ramanan, Deva and Satyanarayanan, Mahadev},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {3D from multi-view and sensors, Buildings, Clustering algorithms, Datasets and evaluation, Deep learning architectures and techniques, Done Reading, Efficient learning and inferences, Image and video synthesis and generation, Rendering (computer graphics), Three-dimensional displays, Training, Urban areas, Vision applications and systems, Visualization},
	pages = {12912--12921},
}

@inproceedings{yen-chen_inerf_2021,
	title = {{iNeRF}: {Inverting} {Neural} {Radiance} {Fields} for {Pose} {Estimation}},
	shorttitle = {{iNeRF}},
	doi = {10.1109/IROS51168.2021.9636708},
	abstract = {We present iNeRF, a framework that performs mesh-free pose estimation by "inverting" a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis — synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation – given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset [21], iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform categorylevel object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {/unread, Cameras, Emphasis, Optimization, Pose estimation, Task analysis, Three-dimensional displays, Training, Training data},
	pages = {1323--1330},
}

@inproceedings{sucar_imap_2021,
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	shorttitle = {{iMAP}},
	doi = {10.1109/ICCV48922.2021.00617},
	abstract = {We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking.Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Efficient training and inference methods, Heuristic algorithms, Neural generative models, Representation learning, Simultaneous localization and mapping, Solid modeling, Stereo, Streaming media, Technological innovation, Three-dimensional displays, Training},
	pages = {6209--6218},
}

@article{zhang_editable_2021,
	title = {Editable {Free}-viewpoint {Video} {Using} a {Layered} {Neural} {Representation}},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {http://arxiv.org/abs/2104.14786},
	doi = {10.1145/3450626.3459756},
	abstract = {Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.},
	number = {4},
	urldate = {2022-10-25},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Jiakai and Liu, Xinhang and Ye, Xinyi and Zhao, Fuqiang and Zhang, Yanshun and Wu, Minye and Zhang, Yingliang and Xu, Lan and Yu, Jingyi},
	month = aug,
	year = {2021},
	note = {arXiv:2104.14786 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	pages = {1--18},
}

@article{zhao_human_2022,
	title = {Human {Performance} {Modeling} and {Rendering} via {Neural} {Animated} {Mesh}},
	volume = {41},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3550454.3555451},
	doi = {10.1145/3550454.3555451},
	abstract = {We have recently seen tremendous progress in the neural advances for photo-real human modeling and rendering. However, it's still challenging to integrate them into an existing mesh-based pipeline for downstream applications. In this paper, we present a comprehensive neural approach for high-quality reconstruction, compression, and rendering of human performances from dense multi-view videos. Our core intuition is to bridge the traditional animated mesh workflow with a new class of highly efficient neural techniques. We first introduce a neural surface reconstructor for high-quality surface generation in minutes. It marries the implicit volumetric rendering of the truncated signed distance field (TSDF) with multi-resolution hash encoding. We further propose a hybrid neural tracker to generate animated meshes, which combines explicit non-rigid tracking with implicit dynamic deformation in a self-supervised framework. The former provides the coarse warping back into the canonical space, while the latter implicit one further predicts the displacements using the 4D hash encoding as in our reconstructor. Then, we discuss the rendering schemes using the obtained animated meshes, ranging from dynamic texturing to lumigraph rendering under various bandwidth settings. To strike an intricate balance between quality and bandwidth, we propose a hierarchical solution by first rendering 6 virtual views covering the performer and then conducting occlusion-aware neural texture blending. We demonstrate the efficacy of our approach in a variety of mesh-based applications and photo-realistic free-view experiences on various platforms, i.e., inserting virtual human performances into real environments through mobile AR or immersively watching talent shows with VR headsets.},
	number = {6},
	urldate = {2023-01-06},
	journal = {ACM Transactions on Graphics},
	author = {Zhao, Fuqiang and Jiang, Yuheng and Yao, Kaixin and Zhang, Jiakai and Wang, Liao and Dai, Haizhao and Zhong, Yuhui and Zhang, Yingliang and Wu, Minye and Xu, Lan and Yu, Jingyi},
	year = {2022},
	keywords = {/unread, human modeling, human performance capture, neural rendering, virtual human},
	pages = {235:1--235:17},
}

@inproceedings{watson_heightfields_2023,
	title = {Heightfields for {Efficient} {Scene} {Reconstruction} for {AR}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-01-04},
	author = {Watson, Jamie and Vicente, Sara and Mac Aodha, Oisin and Godard, Clément and Brostow, Gabriel and Firman, Michael},
	year = {2023},
	keywords = {/unread},
	pages = {5850--5860},
}

@inproceedings{deng_depth-supervised_2022,
	title = {Depth-supervised {NeRF}: {Fewer} {Views} and {Faster} {Training} for {Free}},
	shorttitle = {Depth-supervised {NeRF}},
	doi = {10.1109/CVPR52688.2022.01254},
	abstract = {A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as “free” depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Geometry, Pipelines, RGBD sensors and analytics, Rendering (computer graphics), Surface fitting, Three-dimensional displays, Training, Uncertainty, Vision + graphics},
	pages = {12872--12881},
}

@inproceedings{roessle_dense_2022,
	title = {Dense {Depth} {Priors} for {Neural} {Radiance} {Fields} from {Sparse} {Input} {Views}},
	isbn = {978-1-66546-946-3},
	url = {https://www.computer.org/csdl/proceedings-article/cvpr/2022/694600m2882/1H0Nhx2wMqQ},
	doi = {10.1109/CVPR52688.2022.01255},
	abstract = {Neural radiance fields (NeRF) encode a scene into a neural representation that enables photo-realistic rendering of novel views. However, a successful reconstruction from RGB images requires a large number of input views taken under static conditions \&\#x2014; typically up to a few hundred images for room-size scenes. Our method aims to synthesize novel views of whole rooms from an order of magnitude fewer images. To this end, we leverage dense depth priors in order to constrain the NeRF optimization. First, we take advantage of the sparse depth data that is freely available from the structure from motion (SfM) preprocessing step used to estimate camera poses. Second, we use depth completion to convert these sparse points into dense depth maps and uncertainty estimates, which are used to guide NeRF optimization. Our method enables data-efficient novel view synthesis on challenging indoor scenes, using as few as 18 images for an entire scene.},
	language = {English},
	urldate = {2022-10-25},
	publisher = {IEEE Computer Society},
	author = {Roessle, Barbara and Barron, Jonathan T. and Mildenhall, Ben and Srinivasan, Pratul P. and Nießner, Matthias},
	month = jun,
	year = {2022},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
	pages = {12882--12891},
}

@inproceedings{park_deepsdf_2019,
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	shorttitle = {{DeepSDF}},
	doi = {10.1109/CVPR.2019.00025},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from Multiview and Sensors, Computational modeling, Computer vision, Deep Learning, Geometry, Interpolation, Representation Learning, Shape, Solid modeling, Three-dimensional displays, Vision + Graphics},
	pages = {165--174},
}

@inproceedings{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	shorttitle = {{BARF}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lin_BARF_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-10-13},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
	year = {2021},
	keywords = {Done Reading, Emphasis},
	pages = {5741--5751},
}

@inproceedings{tancik_block-nerf_2022,
	title = {Block-{NeRF}: {Scalable} {Large} {Scene} {Neural} {View} {Synthesis}},
	shorttitle = {Block-{NeRF}},
	doi = {10.1109/CVPR52688.2022.00807},
	abstract = {We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to de-compose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben P. and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Buildings, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer architecture, Computer vision, Image reconstruction, Pattern recognition, Photogrammetry and remote sensing, Read Later, Rendering (computer graphics), Transient analysis, Vision applications and systems},
	pages = {8238--8248},
}

@misc{lazova_control-nerf_2022,
	title = {Control-{NeRF}: {Editable} {Feature} {Volumes} for {Scene} {Rendering} and {Manipulation}},
	shorttitle = {Control-{NeRF}},
	url = {http://arxiv.org/abs/2204.10850},
	doi = {10.48550/arXiv.2204.10850},
	abstract = {We present a novel method for performing flexible, 3D-aware image content manipulation while enabling high-quality novel view synthesis. While NeRF-based approaches are effective for novel view synthesis, such models memorize the radiance for every point in a scene within a neural network. Since these models are scene-specific and lack a 3D scene representation, classical editing such as shape manipulation, or combining scenes is not possible. Hence, editing and combining NeRF-based scenes has not been demonstrated. With the aim of obtaining interpretable and controllable scene representations, our model couples learnt scene-specific feature volumes with a scene agnostic neural rendering network. With this hybrid representation, we decouple neural rendering from scene-specific geometry and appearance. We can generalize to novel scenes by optimizing only the scene-specific 3D feature representation, while keeping the parameters of the rendering network fixed. The rendering function learnt during the initial training stage can thus be easily applied to new scenes, making our approach more flexible. More importantly, since the feature volumes are independent of the rendering model, we can manipulate and combine scenes by editing their corresponding feature volumes. The edited volume can then be plugged into the rendering model to synthesize high-quality novel views. We demonstrate various scene manipulations, including mixing scenes, deforming objects and inserting objects into scenes, while still producing photo-realistic results.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Lazova, Verica and Guzov, Vladimir and Olszewski, Kyle and Tulyakov, Sergey and Pons-Moll, Gerard},
	month = apr,
	year = {2022},
	note = {arXiv:2204.10850 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{carlson_cloner_2022,
	title = {{CLONeR}: {Camera}-{Lidar} {Fusion} for {Occupancy} {Grid}-aided {Neural} {Representations}},
	shorttitle = {{CLONeR}},
	url = {http://arxiv.org/abs/2209.01194},
	doi = {10.48550/arXiv.2209.01194},
	abstract = {Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art novel view synthesis and facilitate dense estimation of scene properties. However, NeRFs often fail for large, unbounded scenes that are captured under very sparse views with the scene content concentrated far away from the camera, as is typical for field robotics applications. In particular, NeRF-style algorithms perform poorly: (1) when there are insufficient views with little pose diversity, (2) when scenes contain saturation and shadows, and (3) when finely sampling large unbounded scenes with fine structures becomes computationally intensive. This paper proposes CLONeR, which significantly improves upon NeRF by allowing it to model large outdoor driving scenes that are observed from sparse input sensor views. This is achieved by decoupling occupancy and color learning within the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained using LiDAR and camera data, respectively. In addition, this paper proposes a novel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the NeRF model, and leverage this occupancy grid for improved sampling of points along a ray for volumetric rendering in metric space. Through extensive quantitative and qualitative experiments on scenes from the KITTI dataset, this paper demonstrates that the proposed method outperforms state-of-the-art NeRF models on both novel view synthesis and dense depth prediction tasks when trained on sparse input data.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Carlson, Alexandra and Ramanagopal, Manikandasriram Srinivasan and Tseng, Nathan and Johnson-Roberson, Matthew and Vasudevan, Ram and Skinner, Katherine A.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01194 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading},
}

@misc{mehta_level_2022,
	title = {A {Level} {Set} {Theory} for {Neural} {Implicit} {Evolution} under {Explicit} {Flows}},
	url = {http://arxiv.org/abs/2204.07159},
	doi = {10.48550/arXiv.2204.07159},
	abstract = {Coordinate-based neural networks parameterizing implicit surfaces have emerged as efficient representations of geometry. They effectively act as parametric level sets with the zero-level set defining the surface of interest. We present a framework that allows applying deformation operations defined for triangle meshes onto such implicit surfaces. Several of these operations can be viewed as energy-minimization problems that induce an instantaneous flow field on the explicit surface. Our method uses the flow field to deform parametric implicit surfaces by extending the classical theory of level sets. We also derive a consolidated view for existing methods on differentiable surface extraction and rendering, by formalizing connections to the level-set theory. We show that these methods drift from the theory and that our approach exhibits improvements for applications like surface smoothing, mean-curvature flow, inverse rendering and user-defined editing on implicit geometry.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Mehta, Ishit and Chandraker, Manmohan and Ramamoorthi, Ravi},
	month = jul,
	year = {2022},
	note = {arXiv:2204.07159 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{hedman_baking_2021,
	title = {Baking {Neural} {Radiance} {Fields} for {Real}-{Time} {View} {Synthesis}},
	doi = {10.1109/ICCV48922.2021.00582},
	abstract = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Hedman, Peter and Srinivasan, Pratul P. and Mildenhall, Ben and Barron, Jonathan T. and Debevec, Paul},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Graphics, Graphics processing units, Image and video synthesis, Multilayer perceptrons, Octrees, Portable computers, Stereo, Streaming media, Three-dimensional displays},
	pages = {5855--5864},
}

@misc{anonymous_pmb_2022,
	title = {{PMB}: {Compositional} {Attribute}-object {Understanding} with {Pronouns}},
	copyright = {All rights reserved},
	shorttitle = {{PMB}},
	url = {https://openreview.net/forum?id=9RlvVqBDBy&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dthecvf.com%2FCVPR%2F2023%2FConference%2FAuthors%23your-submissions)},
	abstract = {Deep neural networks, as highly non-linear end-to-end models, still struggle to recognize compositional attribute-object pairs in a zero-shot manner. State-of-the-art methods leverage pre-trained language models to generate regression targets, so that the embeddings are better anchored in the feature space. However, we note that current text encoder outputs are not regularized thus may lose the rich structure. To this end, we introduce pronouns so that regression targets are augmented from adjectives (e.g., running) to adjective-pronoun pairs (e.g., running something). Meanwhile, we design a first-in-first-out memory bank for every and each attribute/object, which intrinsically regularizes the regression target. We evaluate our framework on three large-scale datasets MIT-States, UT-Zappos and VAW-CZSL and demonstrate clear improvements. Codes, data and models will be made publicly available.},
	language = {en},
	urldate = {2023-01-11},
	author = {Anonymous},
	month = nov,
	year = {2022},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{shen_omninerf_2022,
	title = {{OmniNeRF}: {Hybriding} {Omnidirectional} {Distance} and {Radiance} fields for {Neural} {Surface} {Reconstruction}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	shorttitle = {{OmniNeRF}},
	url = {http://arxiv.org/abs/2209.13433},
	abstract = {3D reconstruction from images has wide applications in Virtual Reality and Automatic Driving, where the precision requirement is very high. Ground-breaking research in the neural radiance field (NeRF) by utilizing Multi-Layer Perceptions has dramatically improved the representation quality of 3D objects. Some later studies improved NeRF by building truncated signed distance fields (TSDFs) but still suffer from the problem of blurred surfaces in 3D reconstruction. In this work, this surface ambiguity is addressed by proposing a novel way of 3D shape representation, OmniNeRF. It is based on training a hybrid implicit field of Omni-directional Distance Field (ODF) and neural radiance field, replacing the apparent density in NeRF with omnidirectional information. Moreover, we introduce additional supervision on the depth map to further improve reconstruction quality. The proposed method has been proven to effectively deal with NeRF defects at the edges of the surface reconstruction, providing higher quality 3D scene reconstruction results.},
	urldate = {2022-10-05},
	author = {Shen, Jiaming and {Bolin Song} and {Zirui Wu} and {Yi Xu}},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13433 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Done Reading, My, NeRF},
}

@misc{li_read_2022,
	title = {{READ}: {Large}-{Scale} {Neural} {Scene} {Rendering} for {Autonomous} {Driving}},
	shorttitle = {{READ}},
	url = {http://arxiv.org/abs/2205.05509},
	doi = {10.48550/arXiv.2205.05509},
	abstract = {Synthesizing free-view photo-realistic images is an important task in multimedia. With the development of advanced driver assistance systems{\textasciitilde}(ADAS) and their applications in autonomous vehicles, experimenting with different scenarios becomes a challenge. Although the photo-realistic street scenes can be synthesized by image-to-image translation methods, which cannot produce coherent scenes due to the lack of 3D information. In this paper, a large-scale neural rendering method is proposed to synthesize the autonomous driving scene{\textasciitilde}(READ), which makes it possible to synthesize large-scale driving scenarios on a PC through a variety of sampling schemes. In order to represent driving scenarios, we propose an \{{\textbackslash}omega\} rendering network to learn neural descriptors from sparse point clouds. Our model can not only synthesize realistic driving scenes but also stitch and edit driving scenes. Experiments show that our model performs well in large-scale driving scenarios.},
	urldate = {2022-12-11},
	publisher = {arXiv},
	author = {Li, Zhuopeng and Li, Lu and Ma, Zeyu and Zhang, Ping and Chen, Junbo and Zhu, Jianke},
	month = may,
	year = {2022},
	note = {arXiv:2205.05509 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{ruckert_adop_2022,
	title = {{ADOP}: approximate differentiable one-pixel point rendering},
	volume = {41},
	issn = {0730-0301},
	shorttitle = {{ADOP}},
	url = {https://doi.org/10.1145/3528223.3530122},
	doi = {10.1145/3528223.3530122},
	abstract = {In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP},
	number = {4},
	urldate = {2022-12-11},
	journal = {ACM Transactions on Graphics},
	author = {Rückert, Darius and Franke, Linus and Stamminger, Marc},
	year = {2022},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, image-based rendering, inverse rendering, machine learning, neural rendering, novel view synthesis},
	pages = {99:1--99:14},
}

@inproceedings{thomas_high-confidence_2015,
	title = {High-{Confidence} {Off}-{Policy} {Evaluation}},
	volume = {29},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9541},
	doi = {10.1609/aaai.v29i1.9541},
	abstract = {Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a bad policy can be costly or dangerous, techniques for evaluating the performance of the new policy without requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore have not provided confidences regarding the accuracy of their estimates. In this paper we propose an off-policy method for computing a lower confidence bound on the expected return of a policy.},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
	month = feb,
	year = {2015},
	note = {ISSN: 2374-3468, 2159-5399
Issue: 1
Journal Abbreviation: AAAI},
	keywords = {/unread},
}

@book{shirley_fundamentals_nodate,
	title = {Fundamentals of {Computer} {Graphics}},
	language = {en},
	author = {Shirley, Peter},
	keywords = {/unread, ❓ Multiple DOI},
}

@article{bongiorno_vector-based_2021,
	title = {Vector-based pedestrian navigation in cities},
	volume = {1},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-021-00130-y},
	doi = {10.1038/s43588-021-00130-y},
	abstract = {How do pedestrians choose their paths within city street networks? Researchers have tried to shed light on this matter through strictly controlled experiments, but an ultimate answer based on real-world mobility data is still lacking. Here, we analyze salient features of human path planning through a statistical analysis of a massive dataset of GPS traces, which reveals that (1) people increasingly deviate from the shortest path when the distance between origin and destination increases and (2) chosen paths are statistically different when origin and destination are swapped. We posit that direction to goal is a main driver of path planning and develop a vector-based navigation model; the resulting trajectories, which we have termed pointiest paths, are a statistically better predictor of human paths than a model based on minimizing distance with stochastic effects. Our findings generalize across two major US cities with different street networks, hinting to the fact that vector-based navigation might be a universal property of human path planning.},
	language = {en},
	number = {10},
	urldate = {2022-12-07},
	journal = {Nature Computational Science},
	author = {Bongiorno, Christian and Zhou, Yulun and Kryven, Marta and Theurel, David and Rizzo, Alessandro and Santi, Paolo and Tenenbaum, Joshua and Ratti, Carlo},
	month = oct,
	year = {2021},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {/unread, Computational science, Geography, Society},
	pages = {678--685},
}

@article{shah_airsim_2017,
	title = {{AirSim}: {High}-{Fidelity} {Visual} and {Physical} {Simulation} for {Autonomous} {Vehicles}},
	shorttitle = {{AirSim}},
	url = {https://arxiv.org/abs/1705.05065v2},
	doi = {10.48550/arXiv.1705.05065},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	language = {en},
	urldate = {2022-12-05},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	month = may,
	year = {2017},
	keywords = {/unread},
}

@inproceedings{kaiming_he_single_2009,
	address = {Miami, FL},
	title = {Single image haze removal using dark channel prior},
	isbn = {978-1-4244-3992-8},
	url = {https://ieeexplore.ieee.org/document/5206515/},
	doi = {10.1109/CVPR.2009.5206515},
	urldate = {2022-12-03},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Kaiming He} and {Jian Sun} and {Xiaoou Tang}},
	month = jun,
	year = {2009},
	keywords = {/unread},
	pages = {1956--1963},
}

@article{komorowski_artificial_2018,
	title = {The {Artificial} {Intelligence} {Clinician} learns optimal treatment strategies for sepsis in intensive care},
	volume = {24},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-018-0213-5/},
	doi = {10.1038/s41591-018-0213-5},
	abstract = {Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals1–3, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients1,4–6. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician’s selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians’ actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes.},
	language = {en},
	number = {11},
	urldate = {2022-12-02},
	journal = {Nature Medicine},
	author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
	month = nov,
	year = {2018},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {/unread, Biomedical engineering, Computational models, Machine learning, Outcomes research},
	pages = {1716--1720},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/07070111X},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N≥3) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	number = {3},
	urldate = {2022-12-01},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {/unread, 15A69, 65F99, canonical decomposition (CANDECOMP), higher-order principal components analysis (Tucker), higher-order singular value decomposition (HOSVD), multilinear algebra, multiway arrays, parallel factors (PARAFAC), tensor decompositions},
	pages = {455--500},
}

@inproceedings{kondo_immersive_2022,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '22},
	title = {Immersive {Real} {World} through {Deep} {Billboards}},
	isbn = {978-1-4503-9369-0},
	url = {https://doi.org/10.1145/3532834.3536210},
	doi = {10.1145/3532834.3536210},
	abstract = {An aspirational goal for virtual reality (VR) is to bring in a rich diversity of real world objects losslessly. Existing VR applications often convert objects into explicit 3D models with meshes or point clouds, which allow fast interactive rendering but also severely limit its quality and the types of supported objects, fundamentally upper-bounding the “realism” of VR. Inspired by the classic “billboards” technique in gaming, we develop Deep Billboards that model 3D objects implicitly using neural networks, where only 2D image is rendered at a time based on the user’s viewing direction. Our system, connecting a commercial VR headset with a server running neural rendering, allows real-time high-resolution simulation of detailed rigid objects, hairy objects, actuated dynamic objects and more in an interactive VR world, drastically narrowing the existing real-to-simulation (real2sim) gap. Additionally, we augment Deep Billboards with physical interaction capability, adapting classic billboards from screen-based games to immersive VR. At our pavilion, the visitors can use our off-the-shelf setup for quickly capturing their favorite objects, and within minutes, experience them in an immersive and interactive VR world – with minimal loss of reality. Our project page: https://sites.google.com/view/deepbillboards/},
	urldate = {2022-11-27},
	booktitle = {{ACM} {SIGGRAPH} 2022 {Immersive} {Pavilion}},
	publisher = {Association for Computing Machinery},
	author = {Kondo, Naruya and Kuroki, So and Hyakuta, Ryosuke and Matsuo, Yutaka and Gu, Shixiang Shane and Ochiai, Yoichi},
	year = {2022},
	keywords = {/unread, billboard, image based rendering, neural networks},
	pages = {1--2},
}

@article{bonatto_real-time_2021,
	title = {Real-{Time} {Depth} {Video}-{Based} {Rendering} for 6-{DoF} {HMD} {Navigation} and {Light} {Field} {Displays}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3123529},
	abstract = {This paper presents a novel approach to provide immersive free navigation with 6 Degrees of Freedom in real-time for natural and virtual scenery, for both static and dynamic content. Stemming from the state-of-the-art in Depth Image-Based Rendering and the OpenGL pipeline, this new View Synthesis method achieves free navigation at up to 90 FPS and can take any number of input views with their corresponding depth maps as priors. Video content can be played thanks to GPU decompression, supporting free navigation with full parallax in real-time. To render a novel viewpoint, each selected input view is warped using the camera pose and associated depth map, using an implicit 3D representation. The warped views are then blended all together to generate the chosen virtual view. Various view blending approaches specifically designed to avoid visual artifacts are compared. Using as few as four input views appears to be an optimal trade-off between computation time and quality, allowing to synthesize high-quality stereoscopic views in real-time, offering a genuine immersive virtual reality experience. Additionally, the proposed approach provides high-quality rendering of a 3D scenery on holographic light field displays. Our results are comparable - objectively and subjectively - to the state of the art view synthesis tools NeRF and LLFF, while maintaining an overall lower complexity and real-time rendering.},
	journal = {IEEE Access},
	author = {Bonatto, Daniele and Fachada, Sarah and Rogge, Ségolène and Munteanu, Adrian and Lafruit, Gauthier},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {/unread, Light fields, Navigation, Real-time systems, Rendering (computer graphics), Resists, Streaming media, Three-dimensional displays, Virtual reality, free viewpoint navigation, real-time view synthesis, reference view synthesizer, stereo image processing, stereo vision},
	pages = {146868--146887},
}

@article{deng_fov-nerf_2022,
	title = {{FoV}-{NeRF}: {Foveated} {Neural} {Radiance} {Fields} for {Virtual} {Reality}},
	volume = {28},
	issn = {1941-0506},
	shorttitle = {{FoV}-{NeRF}},
	doi = {10.1109/TVCG.2022.3203102},
	abstract = {Virtual Reality (VR) is becoming ubiquitous with the rise of consumer displays and commercial VR platforms. Such displays require low latency and high quality rendering of synthetic imagery with reduced compute overheads. Recent advances in neural rendering showed promise of unlocking new possibilities in 3D computer graphics via image-based representations of virtual or physical environments. Specifically, the neural radiance fields (NeRF) demonstrated that photo-realistic quality and continuous view changes of 3D scenes can be achieved without loss of view-dependent effects. While NeRF can significantly benefit rendering for VR applications, it faces unique challenges posed by high field-of-view, high resolution, and stereoscopic/egocentric viewing, typically causing low quality and high latency of the rendered images. In VR, this not only harms the interaction experience but may also cause sickness. To tackle these problems toward six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. We then jointly optimize the latency/performance and visual quality while mutually bridging human perception and neural scene synthesis to achieve perceptually high-quality immersive interaction. We conducted both objective analysis and subjective studies to evaluate the effectiveness of our approach. We find that our method significantly reduces latency (up to 99\% time reduction compared with NeRF) without loss of high-fidelity rendering (perceptually identical to full-resolution ground truth). The presented approach may serve as the first step toward future VR/AR systems that capture, teleport, and visualize remote environments in real-time.},
	number = {11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Deng, Nianchen and He, Zhenyi and Ye, Jiannan and Duinkharjav, Budmonde and Chakravarthula, Praneeth and Yang, Xubo and Sun, Qi},
	month = nov,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {/unread, Cameras, Electronic mail, Foveated Rendering, Gaze-Contingent Graphics, Neural Representation, Rendering (computer graphics), Sensitivity, Stereo image processing, Three-dimensional displays, Virtual Reality, Visualization},
	pages = {3854--3864},
}

@misc{zhang_cardiac_2022,
	title = {Cardiac {Evidence} {Mining} for {Eating} {Monitoring} using {Collocative} {Electrocardiogram} {Imagining}},
	url = {https://www.techrxiv.org/articles/preprint/Cardiac_Evidence_Mining_for_Eating_Monitoring_using_Collocative_Electrocardiogram_Imagining/18093275/2},
	doi = {10.36227/techrxiv.18093275.v2},
	abstract = {Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.},
	language = {en},
	urldate = {2022-11-26},
	publisher = {TechRxiv},
	author = {Zhang, Xulu and Yang, Zhenqun and Jiang, Dongmei and Liao, Ga and Li, Qing and Jain, Ramesh and Wei, Xiaoyong},
	month = jan,
	year = {2022},
	keywords = {/unread},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fan_image_2018,
	title = {Image {Visual} {Realism}: {From} {Human} {Perception} to {Machine} {Computation}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {Image {Visual} {Realism}},
	doi = {10.1109/TPAMI.2017.2747150},
	abstract = {Visual realism is defined as the extent to which an image appears to people as a photo rather than computer generated. Assessing visual realism is important in applications like computer graphics rendering and photo retouching. However, current realism evaluation approaches use either labor-intensive human judgments or automated algorithms largely dependent on comparing renderings to reference images. We develop a reference-free computational framework for visual realism prediction to overcome these constraints. First, we construct a benchmark dataset of 2,520 images with comprehensive human annotated attributes. From statistical modeling on this data, we identify image attributes most relevant for visual realism. We propose both empirically-based (guided by our statistical modeling of human data) and deep convolutional neural network models to predict visual realism of images. Our framework has the following advantages: (1) it creates an interpretable and concise empirical model that characterizes human perception of visual realism; (2) it links computational features to latent factors of human image perception.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Fan, Shaojing and Ng, Tian-Tsong and Koenig, Bryan Lee and Herberg, Jonathan Samuel and Jiang, Ming and Shen, Zhiqi and Zhao, Qi},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {/unread, Benchmark testing, Computational modeling, Face, Rendering (computer graphics), Solid modeling, Visual realism, Visualization, convolutional neural network, human psychophysics, statistical modeling},
	pages = {2180--2193},
}

@article{quadri_survey_2022,
	title = {A {Survey} of {Perception}-{Based} {Visualization} {Studies} by {Task}},
	volume = {28},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2021.3098240},
	abstract = {Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Quadri, Ghulam Jilani and Rosen, Paul},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {/unread, Crowdsourcing, Data visualization, Encoding, Task analysis, Taxonomy, Visual analytics, Visualization, evaluation, graphical perception, perception, survey, visual analytics tasks},
	pages = {5026--5048},
}

@article{zeng_octree-based_2013,
	series = {Computational {Visual} {Media} {Conference} 2012},
	title = {Octree-based fusion for realtime {3D} reconstruction},
	volume = {75},
	issn = {1524-0703},
	url = {https://www.sciencedirect.com/science/article/pii/S1524070312000768},
	doi = {10.1016/j.gmod.2012.09.002},
	abstract = {This paper proposes an octree-based surface representation for KinectFusion, a realtime reconstruction technique of in-door scenes using a low-cost moving depth camera and a commodity graphics hardware. In KinectFusion, the scene is represented as a signed distance function (SDF) and stored as an uniform grid of voxels. Though the grid-based SDF is suitable for parallel computation in graphics hardware, most of the storage are wasted, because the geometry is very sparse in the scene volume. In order to reduce the memory cost and save the computation time, we represent the SDF in an octree, and developed several octree-based algorithms for reconstruction update and surface prediction that are suitable for parallel computation in graphics hardware. In the reconstruction update step, the octree nodes are adaptively split in breath-first order. To handle scenes with moving objects, the corresponding nodes are automatically detected and removed to avoid storage overflow. In the surface prediction step, an octree-based ray tracing method is adopted and parallelized for graphic hardware. To further reduce the computation time, the octree is organized into four layers, called top layer, branch layer, middle layer and data layer. The experiments showed that, the proposed method consumes only less than 10\% memory of original KinectFusion method, and achieves faster performance. Consequently, it can reconstruct scenes with more than 10 times larger size than the original KinectFusion on the same hardware setup.},
	language = {en},
	number = {3},
	urldate = {2022-11-22},
	journal = {Graphical Models},
	author = {Zeng, Ming and Zhao, Fukai and Zheng, Jiaxiang and Liu, Xinguo},
	month = may,
	year = {2013},
	keywords = {/unread, 3D reconstruction, Graphics hardware, KinectFusion, Octree, Ray casting, Signed distance function},
	pages = {126--136},
}

@article{stotko_slamcast_2019,
	title = {{SLAMCast}: {Large}-{Scale}, {Real}-{Time} {3D} {Reconstruction} and {Streaming} for {Immersive} {Multi}-{Client} {Live} {Telepresence}},
	volume = {25},
	issn = {1941-0506},
	shorttitle = {{SLAMCast}},
	doi = {10.1109/TVCG.2019.2899231},
	abstract = {Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VR/AR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90\% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Stotko, Patrick and Krumpen, Stefan and Hullin, Matthias B. and Weinmann, Michael and Klein, Reinhard},
	month = may,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {/unread, Bandwidth, Collaboration, Hardware, RGB-D, Real-time systems, Remote collaboration, Servers, Telepresence, Three-dimensional displays, live telepresence, real-time reconstruction, real-time streaming, voxel hashing},
	pages = {2102--2112},
}

@article{yang_vox-fusion_2022,
	title = {Vox-{Fusion}: {Dense} {Tracking} and {Mapping} with {Voxel}-based {Neural} {Implicit} {Representation}},
	shorttitle = {Vox-{Fusion}},
	url = {https://arxiv.org/abs/2210.15858v1},
	doi = {10.1109/ISMAR55827.2022.00066},
	abstract = {In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the scene and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the environment like in previous works. Moreover, we proposed a high-performance multi-process framework to speed up the method, thus supporting some applications that require real-time performance. The evaluation results show that our methods can achieve better accuracy and completeness than previous methods. We also show that our Vox-Fusion can be used in augmented reality and virtual reality applications. Our source code is publicly available at https://github.com/zju3dv/Vox-Fusion.},
	language = {en},
	urldate = {2022-11-16},
	author = {Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng},
	month = oct,
	year = {2022},
	keywords = {/unread, ⚠️ Invalid DOI},
}

@misc{parr_matrix_2018,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1802.01528},
	doi = {10.48550/arXiv.1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Parr, Terence and Howard, Jeremy},
	month = jul,
	year = {2018},
	note = {arXiv:1802.01528 [cs, stat]
version: 2},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_1_nodate,
	title = {机器学习中的数学理论1：三步搞定矩阵求导},
	url = {https://zhuanlan.zhihu.com/p/262751195},
	abstract = {矩阵求导是一类贯穿机器学习，微分方程，概率统计，控制论，凸优化等诸多数学学科的极其重要的操作，遗憾的是，在许多工科专业大学阶段的课本中鲜有系统讲解这部分知识的章节，而许多论文默认读者已经具备了矩阵求…},
	language = {zh},
	urldate = {2022-11-16},
	journal = {知乎专栏},
	keywords = {/unread},
}

@inproceedings{yang_asynchronous_2021,
	title = {Asynchronous {Multi}-{View} {SLAM}},
	doi = {10.1109/ICRA48506.2021.9561481},
	abstract = {Existing multi-camera SLAM systems assume synchronized shutters for all cameras, which is often not the case in practice. In this work, we propose a generalized multi-camera SLAM formulation which accounts for asynchronous sensor observations. Our framework integrates a continuous-time motion model to relate information across asynchronous multi-frames during tracking, local mapping, and loop closing. For evaluation, we collected AMV-Bench, a challenging new SLAM dataset covering 482 km of driving recorded using our asynchronous multi-camera robotic platform. AMV-Bench is over an order of magnitude larger than previous multi-view HD outdoor SLAM datasets, and covers diverse and challenging motions and environments. Our experiments emphasize the necessity of asynchronous sensor modeling, and show that the use of multiple cameras is critical towards robust and accurate SLAM in challenging outdoor scenes. The supplementary material is located at: https://www.cs.toronto.edu/ ajyang/amv-slam},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Yang, Anqi Joyce and Cui, Can and Bârsan, Ioan Andrei and Urtasun, Raquel and Wang, Shenlong},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {/unread, Automation, Cameras, Conferences, Robot vision systems, Simultaneous localization and mapping, Tracking, Tracking loops},
	pages = {5669--5676},
}

@inproceedings{kulkarni_directed_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Directed {Ray} {Distance} {Functions} for {3D} {Scene} {Reconstruction}},
	isbn = {978-3-031-20086-1},
	doi = {10.1007/978-3-031-20086-1_12},
	abstract = {We present an approach for full 3D scene reconstruction from a single unseen image. We trained on dataset of realistic non-watertight scans of scenes. Our approach uses a predicted distance function, since these have shown promise in handling complex topologies and large spaces. We identify and analyze two key challenges for predicting such image conditioned distance functions that have prevented their success on real 3D scene data. First, we show that predicting a conventional scene distance from an image requires reasoning over a large receptive field. Second, we analytically show that the optimal output of the network trained to predict these distance functions does not obey all the distance function properties. We propose an alternate distance function, the Directed Ray Distance Function (DRDF), that tackles both challenges. We show that a deep network trained to predict DRDFs outperforms all other methods quantitatively and qualitatively on 3D reconstruction from single image on Matterport3D, 3DFront, and ScanNet. (Project Page: https://nileshkulkarni.github.io/scene\_drdf)},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Kulkarni, Nilesh and Johnson, Justin and Fouhey, David F.},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, Distance functions, Single image 3D},
	pages = {201--219},
}

@misc{johnson_deep_nodate,
	title = {Deep {Implicit} {Layers} - {Neural} {ODEs}, {Deep} {Equilibirum} {Models}, and {Beyond}},
	url = {http://implicit-layers-tutorial.org/},
	abstract = {This web page is the companion website to our NeurIPS 2020 tutorial, created by [Zico Kolter](http://zicokolter.com), [David Duvenaud](http://www.cs.toronto.edu/{\textasciitilde}duvenaud/), and [Matt Johnson](http://people.csail.mit.edu/mattjj/). The page constain notes to accompany our tutorial (all created via Colab notebooks, which you can experiment with as you like), as well as links to our video presentation...},
	language = {en},
	urldate = {2022-11-14},
	author = {Johnson, David Duvenaud, Matt, Zico Kolter},
	keywords = {/unread},
}

@inproceedings{bai_deep_2022,
	title = {Deep {Equilibrium} {Optical} {Flow} {Estimation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Bai_Deep_Equilibrium_Optical_Flow_Estimation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-14},
	author = {Bai, Shaojie and Geng, Zhengyang and Savani, Yash and Kolter, J. Zico},
	year = {2022},
	keywords = {/unread},
	pages = {620--630},
}

@inproceedings{bai_neural_2022,
	title = {Neural {Deep} {Equilibrium} {Solvers}},
	url = {https://openreview.net/forum?id=B0oHOwT5ENL},
	abstract = {A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer \$f\_{\textbackslash}theta\$. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden's method or Anderson acceleration. In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1\% over the original DEQ's training time), require few additional parameters (1-3\% of the original model size), yet lead to a \$2{\textbackslash}times\$ speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks.},
	language = {en},
	urldate = {2022-11-14},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	month = mar,
	year = {2022},
	keywords = {/unread},
}

@inproceedings{geng_training_2021,
	title = {On {Training} {Implicit} {Models}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Geng, Zhengyang and Zhang, Xin-Yu and Bai, Shaojie and Wang, Yisen and Lin, Zhouchen},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {24247--24260},
}

@inproceedings{bai_multiscale_2020,
	title = {Multiscale {Deep} {Equilibrium} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/3812f9a59b634c2a9c574610eaba5bed-Abstract.html},
	abstract = {We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O(1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach. The code and pre-trained models are at https://github.com/locuslab/mdeq.},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	year = {2020},
	keywords = {/unread, ⛔ No DOI found},
	pages = {5238--5250},
}

@inproceedings{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html},
	abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective “depth” of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2019},
	keywords = {/unread, ⛔ No DOI found},
}

@phdthesis{davis_visual_2016,
	type = {Thesis},
	title = {Visual vibration analysis},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	url = {https://dspace.mit.edu/handle/1721.1/107330},
	abstract = {This dissertation shows how regular cameras can be used to record and analyze the vibrations of visible objects. Through careful temporal analysis, we relate subtle changes in video to the vibrations of recorded surfaces, and use that information to reason about the physical properties of objects and the forces that drive their motion. We explore several applications of our approach to extracting vibrations from video - using it to recover sound from distant surfaces, estimate the physical properties of visible objects, and even predict how objects will respond to new, previously unseen forces. Our work impacts a variety of fields, ranging from computer vision, to long-distance structural health monitoring and nondestructive testing, surveillance, and even visual effects for film. By imaging the vibrations of objects, we offer cameras as low-cost vibration sensors with dramatically higher spatial resolution than the devices traditionally used in engineering. In doing so, we turn every camera into a powerful tool for vibration analysis, and provide an exciting new way to image the world.},
	language = {eng},
	urldate = {2022-11-14},
	school = {Massachusetts Institute of Technology},
	author = {Davis, Myers Abraham},
	year = {2016},
	note = {Accepted: 2017-03-10T15:05:39Z},
	keywords = {/unread},
}

@misc{lin_capturing_2022,
	title = {Capturing, {Reconstructing}, and {Simulating}: the {UrbanScene3D} {Dataset}},
	shorttitle = {Capturing, {Reconstructing}, and {Simulating}},
	url = {http://arxiv.org/abs/2107.04286},
	doi = {10.48550/arXiv.2107.04286},
	abstract = {We present UrbanScene3D, a large-scale data platform for research of urban scene perception and reconstruction. UrbanScene3D contains over 128k high-resolution images covering 16 scenes including large-scale real urban regions and synthetic cities with 136 km{\textasciicircum}2 area in total. The dataset also contains high-precision LiDAR scans and hundreds of image sets with different observation patterns, which provide a comprehensive benchmark to design and evaluate aerial path planning and 3D reconstruction algorithms. In addition, the dataset, which is built on Unreal Engine and Airsim simulator together with the manually annotated unique instance label for each building in the dataset, enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with physical engine and lighting system not only produce variety of data but also enable users to simulate cars or drones in the proposed urban environment for future research.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Lin, Liqiang and Liu, Yilin and Hu, Yue and Yan, Xingguang and Xie, Ke and Huang, Hui},
	month = jul,
	year = {2022},
	note = {arXiv:2107.04286 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@phdthesis{noauthor_uncertainty_nodate,
	title = {Uncertainty in {Deep} {Learning} ({PhD} {Thesis}) {\textbar} {Yarin} {Gal} - {Blog} {\textbar} {Oxford} {Machine} {Learning}},
	url = {https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html},
	abstract = {So I finally submitted my PhD thesis, collecting already published results on how to obtain uncertainty in deep learning, and lots of bits and pieces of new research I had lying around...},
	urldate = {2022-10-14},
}

@misc{sola_micro_2021,
	title = {A micro {Lie} theory for state estimation in robotics},
	url = {http://arxiv.org/abs/1812.01537},
	doi = {10.48550/arXiv.1812.01537},
	abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Solà, Joan and Deray, Jeremie and Atchuthan, Dinesh},
	month = dec,
	year = {2021},
	note = {arXiv:1812.01537 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@inproceedings{shen_sgam_2022,
	title = {{SGAM}: {Building} a {Virtual} {3D} {World} through {Simultaneous} {Generation} and {Mapping}},
	shorttitle = {{SGAM}},
	url = {https://openreview.net/forum?id=17KCLTbRymw},
	abstract = {We present simultaneous generation and mapping (SGAM), a novel 3D scene generation algorithm. Our goal is to produce a realistic, globally consistent 3D world on a large scale. Achieving this goal is challenging and goes beyond the capacities of existing 3D generation or video generation approaches, which fail to scale up to create large, globally consistent 3D scene structures. Towards tackling the challenges, we take a hybrid approach that integrates generative sensor model- ing with 3D reconstruction. Our proposed approach is an autoregressive generative framework that simultaneously generates sensor data at novel viewpoints and builds a 3D map at each timestamp. Given an arbitrary camera trajectory, our method repeatedly applies this generation-and-mapping process for thousands of steps, allowing us to create a gigantic virtual world. Our model can be trained from RGB-D sequences without having access to the complete 3D scene structure. The generated scenes are readily compatible with various interactive environments and rendering engines. Experiments on CLEVER and GoogleEarth datasets demon- strates ours can generate consistent, realistic, and geometrically-plausible scenes that compare favorably to existing view synthesis methods. Our project page is available at https://yshen47.github.io/sgam.},
	language = {en},
	urldate = {2022-11-05},
	author = {Shen, Yuan and Ma, Wei-Chiu and Wang, Shenlong},
	month = oct,
	year = {2022},
	keywords = {MIT, UIUC},
}

@article{deutsch_imap_1983,
	title = {{IMAP}: {An} interactive computer package for the assessment of multiattribute value functions},
	volume = {SMC-13},
	issn = {2168-2909},
	shorttitle = {{IMAP}},
	doi = {10.1109/TSMC.1983.6313174},
	abstract = {An interactive computer graphics software system specifically designed for assessment of multiattribute value functions is described. It is argued that interactive multiattribute assessment package (IMAP) incorporates the major advantages of existing computer packages for multiattribute utility assessment as well as significantly enhanced capabilities for assessing preference independence (PI) conditions on an attribute set. These enhanced capabilities include highly flexible procedures for testing preferential independence and weak difference independence (WDI) that preserve many of the advantages of manual procedures with much less tedium. Capabilities of IMAP not available in existing packages are described in detail.},
	number = {3},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Deutsch, Stuart J. and Malmborg, Charles J.},
	month = may,
	year = {1983},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics},
	keywords = {/unread, Computers, Cybernetics, Estimation, Modeling, Sensitivity, Testing, Vectors},
	pages = {417--421},
}

@inproceedings{dellenbach_ct-icp_2022,
	title = {{CT}-{ICP}: {Real}-time {Elastic} {LiDAR} {Odometry} with {Loop} {Closure}},
	shorttitle = {{CT}-{ICP}},
	doi = {10.1109/ICRA46639.2022.9811849},
	abstract = {Multi-beam LiDAR sensors are increasingly used in robotics, particularly with autonomous cars for localization and perception tasks, both relying on the ability to build a precise map of the environment. For this, we propose a new real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP), completed into a full SLAM with a novel loop detection procedure. The core of this method, is the introduction of the combined continuity in the scan matching, and discontinuity between scans. It allows both the elastic distortion of the scan during the registration for increased precision, and the increased robustness to high frequency motions from the discontinuity. We build a complete SLAM on top of this odometry, using a fast pure LiDAR loop detection based on elevation image 2D matching, providing a pose graph with loop constraints. To show the robustness of the method, we tested it on seven datasets: KITTI, KITTI-raw, KITTI-360, KITTICARLA, ParisLuco, Newer College, and NCLT in driving and high-frequency motion scenarios. Both the CT-ICP odometry and the loop detection are made available online. CT-ICP is currently first, among those giving access to a public code, on the KITTI odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59\% and an average time per scan of 60ms on a CPU with a single thread.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Dellenbach, Pierre and Deschaud, Jean-Emmanuel and Jacquet, Bastien and Goulette, François},
	month = may,
	year = {2022},
	keywords = {/unread, Codes, Laser radar, Location awareness, Real-time systems, Robustness, Sensors, Simultaneous localization and mapping},
	pages = {5580--5586},
}

@article{adamkiewicz_vision-only_2022,
	title = {Vision-{Only} {Robot} {Navigation} in a {Neural} {Radiance} {World}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3150497},
	abstract = {Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. Neural Radiance Fields (NeRFs) represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot’s objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot’s full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through a narrow gap.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Adamkiewicz, Michal and Chen, Timothy and Caccavale, Adam and Gardner, Rachel and Culbertson, Preston and Bohg, Jeannette and Schwager, Mac},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {/unread, Cameras, Collision avoidance, Navigation, Pipelines, Planning, Robot vision systems, Robots, Stanford, localization, motion and path planning, neural radiance fields, vision-based navigation},
	pages = {4606--4613},
}

@misc{sun_nerf-loc_2022,
	title = {{NeRF}-{Loc}: {Transformer}-{Based} {Object} {Localization} {Within} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{Loc}},
	url = {http://arxiv.org/abs/2209.12068},
	doi = {10.48550/arXiv.2209.12068},
	abstract = {Neural Radiance Fields (NeRFs) have been successfully used for scene representation. Recent works have also developed robotic navigation and manipulation systems using NeRF-based environment representations. As object localization is the foundation for many robotic applications, to further unleash the potential of NeRFs in robotic systems, we study object localization within a NeRF scene. We propose a transformer-based framework NeRF-Loc to extract 3D bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF model and camera view as input, and produces labeled 3D bounding boxes of objects as output. Concretely, we design a pair of paralleled transformer encoder branches, namely the coarse stream and the fine stream, to encode both the context and details of target objects. The encoded features are then fused together with attention layers to alleviate ambiguities for accurate object localization. We have compared our method with the conventional transformer-based method and our method achieves better performance. In addition, we also present the first NeRF samples-based object localization benchmark NeRFLocBench.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Sun, Jiankai and Xu, Yan and Ding, Mingyu and Yi, Hongwei and Wang, Jingdong and Zhang, Liangjun and Schwager, Mac},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12068 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{martin-brualla_nerf_2021,
	title = {{NeRF} in the {Wild}: {Neural} {Radiance} {Fields} for {Unconstrained} {Photo} {Collections}},
	shorttitle = {{NeRF} in the {Wild}},
	doi = {10.1109/CVPR46437.2021.00713},
	abstract = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multi-layer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Image color analysis, Learning systems, Lighting, Photorealism, Rendering (computer graphics), Solid modeling, Three-dimensional displays},
	pages = {7206--7215},
}

@inproceedings{chen_projective_2022,
	title = {Projective {Manifold} {Gradient} {Layer} for {Deep} {Rotation} {Regression}},
	doi = {10.1109/CVPR52688.2022.00653},
	abstract = {Regressing rotations on SO(3) manifold using deep neural networks is an important yet unsolved problem. The gap between the Euclidean network output space and the non-Euclidean SO(3) manifold imposes a severe challenge for neural network learning in both forward and backward passes. While several works have proposed different regression-friendly rotation representations, very few works have been devoted to improving the gradient back-propagating in the backward pass. In this paper, we propose a manifold-aware gradient that directly backpropagates into deep network weights. Leveraging Riemannian optimization to construct a novel projective gradient, our proposed regularized projective manifold gradient (RPMG) method helps networks achieve new state-of-the-art performance in a variety of rotation estimation tasks. Our proposed gradient layer can also be applied to other smooth manifolds such as the unit sphere. Our project page is at https://jychen18.github.io/RPMG.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas J. and Wang, He},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer vision, Deep learning, Manifolds, Neural networks, Pattern recognition, Pose estimation, Pose estimation and tracking, Task analysis},
	pages = {6636--6645},
}

@inproceedings{xu_multi-scale_2018,
	title = {Multi-scale {Voxel} {Hashing} and {Efficient} {3D} {Representation} for {Mobile} {Augmented} {Reality}},
	doi = {10.1109/CVPRW.2018.00200},
	abstract = {In recent years, Visual-Inertial Odometry (VIO) technologies have been making great strides in both research community and industry. With the development of ARKit and ARCore, mobile Augmented Reality (AR) applications have become popular. However, collision detection and avoidance is largely un-addressed with these applications. In this paper, we present an efficient multi-scale voxel hashing algorithm for representing a 3D environment using a set of multi-scale voxels. The input to our algorithm is the 3D point cloud generated by a VIO system (e.g., ARKit). We show that our method can process the 3D points and convert them into multi-scale 3D representation in real time, while maintaining a small memory footprint. The 3D representation can be used to efficiently detect collision between digital objects and real objects in an environment in AR applications.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Xu, Yi and Wu, Yuzhang and Zhou, Hui},
	month = jun,
	year = {2018},
	note = {ISSN: 2160-7516},
	keywords = {/unread, Augmented reality, Cameras, Collision avoidance, Real-time systems, Simultaneous localization and mapping, Three-dimensional displays, Visualization},
	pages = {1586--15867},
}

@article{niesner_real-time_2013,
	title = {Real-time {3D} reconstruction at scale using voxel hashing},
	volume = {32},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2508363.2508374},
	doi = {10.1145/2508363.2508374},
	abstract = {Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.},
	number = {6},
	urldate = {2022-10-27},
	journal = {ACM Transactions on Graphics},
	author = {Nießner, Matthias and Zollhöfer, Michael and Izadi, Shahram and Stamminger, Marc},
	year = {2013},
	keywords = {/unread, GPU, data structure, real-time reconstruction, scalable},
	pages = {169:1--169:11},
}

@article{engel_direct_2018,
	title = {Direct {Sparse} {Odometry}},
	volume = {40},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2017.2658577},
	abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {/unread, Cameras, Computational modeling, Geometry, Optimization, Robustness, Three-dimensional displays, Visual odometry, SLAM, 3D reconstruction, structure from motion, Visualization},
	pages = {611--625},
}

@article{campos_orb-slam3_2021,
	title = {{ORB}-{SLAM3}: {An} {Accurate} {Open}-{Source} {Library} for {Visual}, {Visual}–{Inertial}, and {Multimap} {SLAM}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {{ORB}-{SLAM3}},
	doi = {10.1109/TRO.2021.3075644},
	abstract = {This article presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multimap SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a tightly integrated visual-inertial SLAM system that fully relies on maximum a posteriori (MAP) estimation, even during IMU initialization, resulting in real-time robust operation in small and large, indoor and outdoor environments, being two to ten times more accurate than previous approaches. The second main novelty is a multiple map system relying on a new place recognition method with improved recall that lets ORB-SLAM3 survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting them. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information from high parallax co-visible keyframes, even if they are widely separated in time or come from previous mapping sessions, boosting accuracy. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.5 cm in the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and M. Montiel, José M. and D. Tardós, Juan},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {/unread, Computer vision, Feature extraction, Inertial navigation, Optimization, Robustness, Simultaneous localization and mapping, inertial navigation, simult- aneous localization and mapping},
	pages = {1874--1890},
}

@article{mur-artal_orb-slam_2015,
	title = {{ORB}-{SLAM}: {A} {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
	volume = {31},
	issn = {1941-0468},
	shorttitle = {{ORB}-{SLAM}},
	doi = {10.1109/TRO.2015.2463671},
	abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, Raúl and Montiel, J. M. M. and Tardós, Juan D.},
	month = oct,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {/unread, Cameras, Computational modeling, Feature extraction, Lifelong mapping, Optimization, Real-time systems, Simultaneous localization and mapping, Visualization, localization, monocular vision, recognition, simultaneous localization and mapping (SLAM)},
	pages = {1147--1163},
}

@article{zhang_microsoft_2012,
	title = {Microsoft {Kinect} {Sensor} and {Its} {Effect}},
	volume = {19},
	issn = {1070-986X},
	url = {http://ieeexplore.ieee.org/document/6190806/},
	doi = {10.1109/MMUL.2012.24},
	number = {2},
	urldate = {2022-10-26},
	journal = {IEEE Multimedia},
	author = {Zhang, Zhengyou},
	month = feb,
	year = {2012},
	keywords = {/unread},
	pages = {4--10},
}

@article{zhang_microsoft_2012-1,
	title = {Microsoft {Kinect} {Sensor} and {Its} {Effect}},
	volume = {19},
	issn = {1941-0166},
	doi = {10.1109/MMUL.2012.24},
	abstract = {Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry.},
	number = {2},
	journal = {IEEE MultiMedia},
	author = {Zhang, Zhengyou},
	month = feb,
	year = {2012},
	keywords = {/unread, Cameras, Games, Microsoft Kinect, Sensors, Three dimensional displays, Video recording, computer vision, human-computer interaction, motion capture, multimedia},
	pages = {4--10},
}

@article{zabatani_intel_2020,
	title = {Intel® {RealSense}™ {SR300} {Coded} {Light} {Depth} {Camera}},
	volume = {42},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2019.2915841},
	abstract = {Intel® RealSense™ SR300 is a depth camera capable of providing a VGA-size depth map at 60 fps and 0.125mm depth resolution. In addition, it outputs an infrared VGA-resolution image and a 1080p color texture image at 30 fps. SR300 form-factor enables it to be integrated into small consumer products and as a front facing camera in laptops and Ultrabooks™. The SR300 depth camera is based on a coded-light technology where triangulation between projected patterns and images captured by a dedicated sensor is used to produce the depth map. Each projected line is coded by a special temporal optical code, that enables a dense depth map reconstruction from its reflection. The solid mechanical assembly of the camera allows it to stay calibrated throughout temperature and pressure changes, drops, and hits. In addition, active dynamic control maintains a calibrated depth output. An extended API LibRS released with the camera allows developers to integrate the camera in various applications. Algorithms for 3D scanning, facial analysis, hand gesture recognition, and tracking are within reach for applications using the SR300. In this paper, we describe the underlying technology, hardware, and algorithms of the SR300, as well as its calibration procedure, and outline some use cases. We believe that this paper will provide a full case study of a mass-produced depth sensing product and technology.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zabatani, Aviad and Surazhsky, Vitaly and Sperling, Erez and Moshe, Sagi Ben and Menashe, Ohad and Silver, David H. and Karni, Zachi and Bronstein, Alexander M. and Bronstein, Michael M. and Kimmel, Ron},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {/unread, 3D camera, Cameras, Image reconstruction, Intel, Optical imaging, Optical sensors, Pipelines, RealSense, Reflective binary codes, SR300, Three-dimensional displays, coded light, depth reconstruction},
	pages = {2333--2345},
}

@inproceedings{jeong_self-calibrating_2021,
	title = {Self-{Calibrating} {Neural} {Radiance} {Fields}},
	doi = {10.1109/ICCV48922.2021.00579},
	abstract = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also propose a new geometric loss function, viz., projected ray distance loss, to incorporate geometric consistency for complex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differentiable manner allows us to improve PSNR over baselines. Our module is an easy-to-use plugin that can be applied to NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECH-CVLab/SCNeRF},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Jeong, Yoonwoo and Ahn, Seokjun and Choy, Christopher and Anandkumar, Animashree and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Calibration, Cameras, Codes, Computational photography, Computer vision, Geometry, Nonlinear distortion, Standards, Stereo, Vision applications and systems},
	pages = {5826--5834},
}

@inproceedings{zhu_nice-slam_2022,
	title = {{NICE}-{SLAM}: {Neural} {Implicit} {Scalable} {Encoding} for {SLAM}},
	shorttitle = {{NICE}-{SLAM}},
	doi = {10.1109/CVPR52688.2022.01245},
	abstract = {Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over- smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Cameras, Computer vision, Geometry, Network architecture, RGBD sensors and analytics, Scalability, Simultaneous localization and mapping, Visualization},
	pages = {12776--12786},
}

@misc{shi_city-scale_2022,
	title = {City-scale {Incremental} {Neural} {Mapping} with {Three}-layer {Sampling} and {Panoptic} {Representation}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	url = {http://arxiv.org/abs/2209.14072},
	abstract = {Neural implicit representations are drawing a lot of attention from the robotics community recently, as they are expressive, continuous and compact. However, city-scale incremental implicit dense mapping based on sparse LiDAR input is still an under-explored challenge. To this end,we successfully build the first city-scale incremental neural mapping system with a panoptic representation that consists of both environment-level and instance-level modelling. Given a stream of sparse LiDAR point cloud, it maintains a dynamic generative model that maps 3D coordinates to signed distance field (SDF) values. To address the difficulty of representing geometric information at different levels in city-scale space, we propose a tailored three-layer sampling strategy to dynamically sample the global, local and near-surface domains. Meanwhile, to realize high fidelity mapping, category-specific prior is introduced to better model the geometric details, leading to a panoptic representation. We evaluate on the public SemanticKITTI dataset and demonstrate the significance of the newly proposed three-layer sampling strategy and panoptic representation, using both quantitative and qualitative results. Codes and data will be publicly available.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Shi, Yongliang and Yang, Runyi and Li, Pengfei and Wu, Zirui and Zhao, Hao and Zhou, Guyue},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14072 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading, Implicit Representation, RA-L, SDF},
}

@misc{mozer_discrete_2017,
	title = {Discrete {Event}, {Continuous} {Time} {RNNs}},
	url = {http://arxiv.org/abs/1710.04110},
	doi = {10.48550/arXiv.1710.04110},
	abstract = {We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Mozer, Michael C. and Kazakov, Denis and Lindsey, Robert V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.04110 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6},
}

@misc{schirmer_modeling_2022,
	title = {Modeling {Irregular} {Time} {Series} with {Continuous} {Recurrent} {Units}},
	url = {http://arxiv.org/abs/2111.11344},
	doi = {10.48550/arXiv.2111.11344},
	abstract = {Recurrent neural networks (RNNs) are a popular choice for modeling sequential data. Modern RNN architectures assume constant time-intervals between observations. However, in many datasets (e.g. medical records) observation times are irregular and can carry important information. To address this challenge, we propose continuous recurrent units (CRUs) -- a neural architecture that can naturally handle irregular intervals between observations. The CRU assumes a hidden state, which evolves according to a linear stochastic differential equation and is integrated into an encoder-decoder framework. The recursive computations of the CRU can be derived using the continuous-discrete Kalman filter and are in closed form. The resulting recurrent architecture has temporal continuity between hidden states and a gating mechanism that can optimally integrate noisy observations. We derive an efficient parameterization scheme for the CRU that leads to a fast implementation f-CRU. We empirically study the CRU on a number of challenging datasets and find that it can interpolate irregular time series better than methods based on neural ordinary differential equations.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Schirmer, Mona and Eltayeb, Mazin and Lessmann, Stefan and Rudolph, Maja},
	month = jul,
	year = {2022},
	note = {arXiv:2111.11344 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Read Later, Statistics - Machine Learning, Time Series Modeling},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {Dropout},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	keywords = {deep learning, model combination, neural networks, regularization, ⛔ No DOI found},
	pages = {1929--1958},
}

@misc{gal_bayesian_2016,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Bernoulli} {Approximate} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1506.02158},
	doi = {10.48550/arXiv.1506.02158},
	abstract = {Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jan,
	year = {2016},
	note = {arXiv:1506.02158 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kendall_modelling_2016,
	title = {Modelling uncertainty in deep learning for camera relocalization},
	doi = {10.1109/ICRA.2016.7487679},
	abstract = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6° accuracy for very large scale outdoor scenes and 0.5m and 10° accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kendall, Alex and Cipolla, Roberto},
	month = may,
	year = {2016},
	keywords = {Bayes methods, Cameras, Computational modeling, Measurement uncertainty, Neural networks, Simultaneous localization and mapping, Uncertainty},
	pages = {4762--4769},
}

@inproceedings{kendall_posenet_2015,
	title = {{PoseNet}: {A} {Convolutional} {Network} for {Real}-{Time} 6-{DOF} {Camera} {Relocalization}},
	shorttitle = {{PoseNet}},
	doi = {10.1109/ICCV.2015.336},
	abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	month = dec,
	year = {2015},
	note = {ISSN: 2380-7504},
	keywords = {2015, Cameras, ICCV, Neural networks, PoseNet, Quaternions, Read Later, Real-time systems, Relocalization, Robot vision systems, Simultaneous localization and mapping, Training},
	pages = {2938--2946},
}

@inproceedings{kendall_geometric_2017,
	title = {Geometric {Loss} {Functions} for {Camera} {Pose} {Regression} with {Deep} {Learning}},
	doi = {10.1109/CVPR.2017.694},
	abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNets performance across datasets ranging from indoor rooms to a small city.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kendall, Alex and Cipolla, Roberto},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {2017, CVPR, Cameras, Geometry, Machine learning, Measurement, Neural networks, Quaternions, Reading, Relocalization, Robustness},
	pages = {6555--6564},
}

@article{fawzi_discovering_2022,
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	volume = {610},
	copyright = {2022 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05172-4},
	doi = {10.1038/s41586-022-05172-4},
	abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
	language = {en},
	number = {7930},
	urldate = {2022-10-06},
	journal = {Nature},
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
	month = oct,
	year = {2022},
	note = {Number: 7930
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computer science, Read Later},
	pages = {47--53},
}

@inproceedings{sturm_benchmark_2012,
	title = {A benchmark for the evaluation of {RGB}-{D} {SLAM} systems},
	doi = {10.1109/IROS.2012.6385773},
	abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Sturm, Jürgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
	month = oct,
	year = {2012},
	note = {ISSN: 2153-0866},
	keywords = {Calibration, Cameras, Simultaneous localization and mapping, Trajectory, Visualization},
	pages = {573--580},
}
